{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmYGVziz50tu"
      },
      "source": [
        "- Remove non alphanumeric characters for simple training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install numpy\n",
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "FOwRggVcwtzP"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def get_device():\n",
        "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = q.size()[-1]\n",
        "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scaled = scaled.permute(1, 0, 2, 3) + mask\n",
        "        scaled = scaled.permute(1, 0, 2, 3)\n",
        "    attention = F.softmax(scaled, dim=-1)\n",
        "    values = torch.matmul(attention, v)\n",
        "    return values, attention\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_sequence_length):\n",
        "        super().__init__()\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self):\n",
        "        even_i = torch.arange(0, self.d_model, 2).float()\n",
        "        denominator = torch.pow(10000, even_i/self.d_model)\n",
        "        position = (torch.arange(self.max_sequence_length)\n",
        "                          .reshape(self.max_sequence_length, 1))\n",
        "        even_PE = torch.sin(position / denominator)\n",
        "        odd_PE = torch.cos(position / denominator)\n",
        "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
        "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
        "        return PE\n",
        "\n",
        "class SentenceEmbedding(nn.Module):\n",
        "    \"For a given sentence, create an embedding\"\n",
        "    def __init__(self, max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
        "        super().__init__()\n",
        "        self.vocab_size = len(language_to_index)\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
        "        self.language_to_index = language_to_index\n",
        "        self.position_encoder = PositionalEncoding(d_model, max_sequence_length)\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.START_TOKEN = START_TOKEN\n",
        "        self.END_TOKEN = END_TOKEN\n",
        "        self.PADDING_TOKEN = PADDING_TOKEN\n",
        "    \n",
        "    def batch_tokenize(self, batch, start_token, end_token):\n",
        "\n",
        "        def tokenize(sentence, start_token, end_token):\n",
        "            unknown_token_index = self.language_to_index.get('<UNK>')\n",
        "            sentence_word_indicies = [self.language_to_index[token] for token in list(sentence)]\n",
        "            if start_token:\n",
        "                sentence_word_indicies.insert(0, self.language_to_index[self.START_TOKEN])\n",
        "            if end_token:\n",
        "                sentence_word_indicies.append(self.language_to_index[self.END_TOKEN])\n",
        "            for _ in range(len(sentence_word_indicies), self.max_sequence_length):\n",
        "                sentence_word_indicies.append(self.language_to_index[self.PADDING_TOKEN])\n",
        "            return torch.tensor(sentence_word_indicies)\n",
        "\n",
        "        tokenized = []\n",
        "        for sentence_num in range(len(batch)):\n",
        "           tokenized.append( tokenize(batch[sentence_num], start_token, end_token) )\n",
        "        tokenized = torch.stack(tokenized)\n",
        "        return tokenized.to(get_device())\n",
        "    \n",
        "    def forward(self, x, start_token, end_token): # sentence\n",
        "        x = self.batch_tokenize(x, start_token, end_token)\n",
        "        x = self.embedding(x)\n",
        "        pos = self.position_encoder().to(get_device())\n",
        "        x = self.dropout(x + pos)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.qkv_layer = nn.Linear(d_model , 3 * d_model)\n",
        "        self.linear_layer = nn.Linear(d_model, d_model)\n",
        "    \n",
        "    def forward(self, x, mask):\n",
        "        batch_size, sequence_length, d_model = x.size()\n",
        "        qkv = self.qkv_layer(x)\n",
        "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n",
        "        qkv = qkv.permute(0, 2, 1, 3)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "        values, attention = scaled_dot_product(q, k, v, mask)\n",
        "        values = values.permute(0, 2, 1, 3).reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n",
        "        out = self.linear_layer(values)\n",
        "        return out\n",
        "\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, parameters_shape, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.parameters_shape=parameters_shape\n",
        "        self.eps=eps\n",
        "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
        "        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
        "        mean = inputs.mean(dim=dims, keepdim=True)\n",
        "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
        "        std = (var + self.eps).sqrt()\n",
        "        y = (inputs - mean) / std\n",
        "        out = self.gamma * y + self.beta\n",
        "        return out\n",
        "\n",
        "  \n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, hidden)\n",
        "        self.linear2 = nn.Linear(hidden, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x, self_attention_mask):\n",
        "        residual_x = x.clone()\n",
        "        x = self.attention(x, mask=self_attention_mask)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.norm1(x + residual_x)\n",
        "        residual_x = x.clone()\n",
        "        x = self.ffn(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.norm2(x + residual_x)\n",
        "        return x\n",
        "    \n",
        "class SequentialEncoder(nn.Sequential):\n",
        "    def forward(self, *inputs):\n",
        "        x, self_attention_mask  = inputs\n",
        "        for module in self._modules.values():\n",
        "            x = module(x, self_attention_mask)\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 d_model, \n",
        "                 ffn_hidden, \n",
        "                 num_heads, \n",
        "                 drop_prob, \n",
        "                 num_layers,\n",
        "                 max_sequence_length,\n",
        "                 language_to_index,\n",
        "                 START_TOKEN,\n",
        "                 END_TOKEN, \n",
        "                 PADDING_TOKEN):\n",
        "        super().__init__()\n",
        "        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.layers = SequentialEncoder(*[EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n",
        "                                      for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, self_attention_mask, start_token, end_token):\n",
        "        x = self.sentence_embedding(x, start_token, end_token)\n",
        "        x = self.layers(x, self_attention_mask)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiHeadCrossAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.kv_layer = nn.Linear(d_model , 2 * d_model)\n",
        "        self.q_layer = nn.Linear(d_model , d_model)\n",
        "        self.linear_layer = nn.Linear(d_model, d_model)\n",
        "    \n",
        "    def forward(self, x, y, mask):\n",
        "        batch_size, sequence_length, d_model = x.size() # in practice, this is the same for both languages...so we can technically combine with normal attention\n",
        "        kv = self.kv_layer(x)\n",
        "        q = self.q_layer(y)\n",
        "        kv = kv.reshape(batch_size, sequence_length, self.num_heads, 2 * self.head_dim)\n",
        "        q = q.reshape(batch_size, sequence_length, self.num_heads, self.head_dim)\n",
        "        kv = kv.permute(0, 2, 1, 3)\n",
        "        q = q.permute(0, 2, 1, 3)\n",
        "        k, v = kv.chunk(2, dim=-1)\n",
        "        values, attention = scaled_dot_product(q, k, v, mask) # We don't need the mask for cross attention, removing in outer function!\n",
        "        values = values.permute(0, 2, 1, 3).reshape(batch_size, sequence_length, d_model)\n",
        "        out = self.linear_layer(values)\n",
        "        return out\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.layer_norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "        self.encoder_decoder_attention = MultiHeadCrossAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.layer_norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.layer_norm3 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x, y, self_attention_mask, cross_attention_mask):\n",
        "        _y = y.clone()\n",
        "        y = self.self_attention(y, mask=self_attention_mask)\n",
        "        y = self.dropout1(y)\n",
        "        y = self.layer_norm1(y + _y)\n",
        "\n",
        "        _y = y.clone()\n",
        "        y = self.encoder_decoder_attention(x, y, mask=cross_attention_mask)\n",
        "        y = self.dropout2(y)\n",
        "        y = self.layer_norm2(y + _y)\n",
        "\n",
        "        _y = y.clone()\n",
        "        y = self.ffn(y)\n",
        "        y = self.dropout3(y)\n",
        "        y = self.layer_norm3(y + _y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class SequentialDecoder(nn.Sequential):\n",
        "    def forward(self, *inputs):\n",
        "        x, y, self_attention_mask, cross_attention_mask = inputs\n",
        "        for module in self._modules.values():\n",
        "            y = module(x, y, self_attention_mask, cross_attention_mask)\n",
        "        return y\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 d_model, \n",
        "                 ffn_hidden, \n",
        "                 num_heads, \n",
        "                 drop_prob, \n",
        "                 num_layers,\n",
        "                 max_sequence_length,\n",
        "                 language_to_index,\n",
        "                 START_TOKEN,\n",
        "                 END_TOKEN, \n",
        "                 PADDING_TOKEN):\n",
        "        super().__init__()\n",
        "        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.layers = SequentialDecoder(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, y, self_attention_mask, cross_attention_mask, start_token, end_token):\n",
        "        y = self.sentence_embedding(y, start_token, end_token)\n",
        "        y = self.layers(x, y, self_attention_mask, cross_attention_mask)\n",
        "        return y\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, \n",
        "                d_model, \n",
        "                ffn_hidden, \n",
        "                num_heads, \n",
        "                drop_prob, \n",
        "                num_layers,\n",
        "                max_sequence_length, \n",
        "                kn_vocab_size,\n",
        "                english_to_index,\n",
        "                kannada_to_index,\n",
        "                START_TOKEN, \n",
        "                END_TOKEN, \n",
        "                PADDING_TOKEN\n",
        "                ):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, english_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, kannada_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.linear = nn.Linear(d_model, kn_vocab_size)\n",
        "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "    def forward(self, \n",
        "                x, \n",
        "                y, \n",
        "                encoder_self_attention_mask=None, \n",
        "                decoder_self_attention_mask=None, \n",
        "                decoder_cross_attention_mask=None,\n",
        "                enc_start_token=False,\n",
        "                enc_end_token=False,\n",
        "                dec_start_token=False, # We should make this true\n",
        "                dec_end_token=False): # x, y are batch of sentences\n",
        "        x = self.encoder(x, encoder_self_attention_mask, start_token=enc_start_token, end_token=enc_end_token)\n",
        "        out = self.decoder(x, y, decoder_self_attention_mask, decoder_cross_attention_mask, start_token=dec_start_token, end_token=dec_end_token)\n",
        "        out = self.linear(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "6TApzOj5xCwR"
      },
      "outputs": [],
      "source": [
        "english_file = '/Users/chintabhanushri/Downloads/true-2.txt' # replace this path with appropriate one\n",
        "kannada_file = '/Users/chintabhanushri/Downloads/kannada.txt' # replace this path with appropriate one\n",
        "\n",
        "# Generated this by filtering Appendix code\n",
        "\n",
        "START_TOKEN = '<START>'\n",
        "PADDING_TOKEN = '<PADDING>'\n",
        "END_TOKEN = '<END>'\n",
        "\n",
        "kannada_vocabulary = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', \n",
        "                      '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '<', '=', '>', '?', 'ˌ', \n",
        "                      'ँ', 'ఆ', 'ఇ', 'ా', 'ి', 'ీ', 'ు', 'ూ', \n",
        "                      'ಅ', 'ಆ', 'ಇ', 'ಈ', 'ಉ', 'ಊ', 'ಋ', 'ೠ', 'ಌ', 'ಎ', 'ಏ', 'ಐ', 'ಒ', 'ಓ', 'ಔ', \n",
        "                      'ಕ', 'ಖ', 'ಗ', 'ಘ', 'ಙ', \n",
        "                      'ಚ', 'ಛ', 'ಜ', 'ಝ', 'ಞ', \n",
        "                      'ಟ', 'ಠ', 'ಡ', 'ಢ', 'ಣ', \n",
        "                      'ತ', 'ಥ', 'ದ', 'ಧ', 'ನ', \n",
        "                      'ಪ', 'ಫ', 'ಬ', 'ಭ', 'ಮ', \n",
        "                      'ಯ', 'ರ', 'ಱ', 'ಲ', 'ಳ', 'ವ', 'ಶ', 'ಷ', 'ಸ', 'ಹ', \n",
        "                      '಼', 'ಽ', 'ಾ', 'ಿ', 'ೀ', 'ು', 'ೂ', 'ೃ', 'ೄ', 'ೆ', 'ೇ', 'ೈ', 'ೊ', 'ೋ', 'ೌ', '್', 'ೕ', 'ೖ', 'ೞ', 'ೣ', 'ಂ', 'ಃ', \n",
        "                      '೦', '೧', '೨', '೩', '೪', '೫', '೬', '೭', '೮', '೯', PADDING_TOKEN, END_TOKEN]\n",
        "\n",
        "english_vocabulary = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', \n",
        "                        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "                        ':', '<', '=', '>', '?', '@',\n",
        "                        '[', '\\\\', ']', '^', '_', '`', \n",
        "                        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
        "                        'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', \n",
        "                        'y', 'z', \n",
        "                        '{', '|', '}', '~', PADDING_TOKEN, END_TOKEN]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "gA8ESmCrNoc7"
      },
      "outputs": [],
      "source": [
        "index_to_kannada = {k:v for k,v in enumerate(kannada_vocabulary)}\n",
        "kannada_to_index = {v:k for k,v in enumerate(kannada_vocabulary)}\n",
        "index_to_english = {k:v for k,v in enumerate(english_vocabulary)}\n",
        "english_to_index = {v:k for k,v in enumerate(english_vocabulary)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "9SYGjRdoxRg-"
      },
      "outputs": [],
      "source": [
        "with open(english_file, 'r') as file:\n",
        "    english_sentences = file.readlines()\n",
        "with open(kannada_file, 'r') as file:\n",
        "    kannada_sentences = file.readlines()\n",
        "\n",
        "# Limit Number of sentences\n",
        "TOTAL_SENTENCES = 200000\n",
        "english_sentences = english_sentences[:TOTAL_SENTENCES]\n",
        "kannada_sentences = kannada_sentences[:TOTAL_SENTENCES]\n",
        "english_sentences = [sentence.rstrip('\\n').lower() for sentence in english_sentences]\n",
        "kannada_sentences = [sentence.rstrip('\\n') for sentence in kannada_sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUB-BkgFxXfM",
        "outputId": "bcf7e19c-d1df-4b69-bdfa-eb74ac1a4338"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['hes a scientist.',\n",
              " \"'but we speak the truth aur ye sach hai ke gujarat mein vikas pagal hogaya hai,'' rahul gandhi further said in banaskantha\",\n",
              " '8 lakh crore have been looted.',\n",
              " 'i read a lot into this as well.',\n",
              " \"she was found dead with the phone's battery exploded close to her head the following morning.\",\n",
              " 'how did mankind come under satans rival sovereignty?',\n",
              " 'and then i became prime minister.',\n",
              " 'what about corruption?',\n",
              " 'no differences',\n",
              " '\"\"\"the shooting of the film is 90 percent done.\"']"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "english_sentences[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OT-aznAxc5U",
        "outputId": "716c4145-fdc2-4bb0-e883-c3dcad30c2da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ಇವರು ಸಂಶೋಧಕ ಸ್ವಭಾವದವರು.',\n",
              " '\"ಆದರೆ ಸತ್ಯ ಹೊರ ಬಂದೇ ಬರುತ್ತದೆ ಎಂದು ಹೇಳಿದ ರಾಹುಲ್ ಗಾಂಧಿ, \"\"ಸೂರತ್ ಜನರು ಚೀನಾದ ಜತೆ ಸ್ಪರ್ಧೆ ನಡೆಸುತ್ತಿದ್ದಾರೆ\"',\n",
              " 'ಕಳ್ಳತನವಾಗಿದ್ದ 8 ಲಕ್ಷ ರೂ.',\n",
              " 'ಇದರ ಬಗ್ಗೆ ನಾನೂ ಸಾಕಷ್ಟು ಓದಿದ್ದೇನೆ.',\n",
              " 'ಆಕೆಯ ತಲೆಯ ಹತ್ತಿರ ಇರಿಸಿಕೊಂಡಿದ್ದ ಫೋನ್\\u200cನ ಬ್ಯಾಟರಿ ಸ್ಫೋಟಗೊಂಡು ಆಕೆ ಮೃತಪಟ್ಟಿದ್ದಾಳೆ ಎನ್ನಲಾಗಿದೆ.',\n",
              " 'ಮಾನವಕುಲವು ಸೈತಾನನ ಆಳಿಕೆಯ ಕೆಳಗೆ ಬಂದದ್ದು ಹೇಗೆ?',\n",
              " 'ನಂತರ ಪ್ರಧಾನಿ ಕೂಡ ಆಗುತ್ತೇನೆ.',\n",
              " 'ಭ್ರಷ್ಟಾಚಾರ ಏಕಿದೆ?',\n",
              " '‘ಅನುಪಾತದಲ್ಲಿ ವ್ಯತ್ಯಾಸವಿಲ್ಲ’',\n",
              " 'ಆ ಚಿತ್ರದ ಶೇ 90ರಷ್ಟು ಚಿತ್ರೀಕರಣವೂ ಈಗಾಗಲೇ ಮುಗಿದು ಹೋಗಿದೆ.']"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "kannada_sentences[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8VAutsTxlaR",
        "outputId": "ff8fba72-020d-4f0c-b3c5-31c102b6fe9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "97th percentile length Kannada: 172.0\n",
            "97th percentile length English: 178.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "PERCENTILE = 97\n",
        "print( f\"{PERCENTILE}th percentile length Kannada: {np.percentile([len(x) for x in kannada_sentences], PERCENTILE)}\" )\n",
        "print( f\"{PERCENTILE}th percentile length English: {np.percentile([len(x) for x in english_sentences], PERCENTILE)}\" )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HG9ezqvaxl4b",
        "outputId": "d13be774-ca07-4333-856e-76186f71caae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of sentences: 200000\n",
            "Number of valid sentences: 164022\n"
          ]
        }
      ],
      "source": [
        "max_sequence_length = 200\n",
        "\n",
        "def is_valid_tokens(sentence, vocab):\n",
        "    for token in list(set(sentence)):\n",
        "        if token not in vocab:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def is_valid_length(sentence, max_sequence_length):\n",
        "    return len(list(sentence)) < (max_sequence_length - 1) # need to re-add the end token so leaving 1 space\n",
        "\n",
        "valid_sentence_indicies = []\n",
        "for index in range(len(kannada_sentences)):\n",
        "    kannada_sentence, english_sentence = kannada_sentences[index], english_sentences[index]\n",
        "    if is_valid_length(kannada_sentence, max_sequence_length) \\\n",
        "      and is_valid_length(english_sentence, max_sequence_length) \\\n",
        "      and is_valid_tokens(kannada_sentence, kannada_vocabulary):\n",
        "        valid_sentence_indicies.append(index)\n",
        "\n",
        "print(f\"Number of sentences: {len(kannada_sentences)}\")\n",
        "print(f\"Number of valid sentences: {len(valid_sentence_indicies)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "o80QDn4CxsV7"
      },
      "outputs": [],
      "source": [
        "kannada_sentences = [kannada_sentences[i] for i in valid_sentence_indicies]\n",
        "english_sentences = [english_sentences[i] for i in valid_sentence_indicies]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35xhLztQiLIQ",
        "outputId": "aa70ad04-2e45-4c78-c852-61e92c51a96a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ಇವರು ಸಂಶೋಧಕ ಸ್ವಭಾವದವರು.',\n",
              " '\"ಆದರೆ ಸತ್ಯ ಹೊರ ಬಂದೇ ಬರುತ್ತದೆ ಎಂದು ಹೇಳಿದ ರಾಹುಲ್ ಗಾಂಧಿ, \"\"ಸೂರತ್ ಜನರು ಚೀನಾದ ಜತೆ ಸ್ಪರ್ಧೆ ನಡೆಸುತ್ತಿದ್ದಾರೆ\"',\n",
              " 'ಕಳ್ಳತನವಾಗಿದ್ದ 8 ಲಕ್ಷ ರೂ.']"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "kannada_sentences[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "xqOFnclmyxAE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "d_model = 512\n",
        "batch_size = 30\n",
        "ffn_hidden = 2048\n",
        "num_heads = 8\n",
        "drop_prob = 0.1\n",
        "num_layers = 1\n",
        "max_sequence_length = 200\n",
        "kn_vocab_size = len(kannada_vocabulary)\n",
        "\n",
        "transformer = Transformer(d_model, \n",
        "                          ffn_hidden,\n",
        "                          num_heads, \n",
        "                          drop_prob, \n",
        "                          num_layers, \n",
        "                          max_sequence_length,\n",
        "                          kn_vocab_size,\n",
        "                          english_to_index,\n",
        "                          kannada_to_index,\n",
        "                          START_TOKEN, \n",
        "                          END_TOKEN, \n",
        "                          PADDING_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zc2hYQk9yxX0",
        "outputId": "c060f588-6a2e-4179-9475-5acafd641f1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): Encoder(\n",
              "    (sentence_embedding): SentenceEmbedding(\n",
              "      (embedding): Embedding(71, 512)\n",
              "      (position_encoder): PositionalEncoding()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (layers): SequentialEncoder(\n",
              "      (0): EncoderLayer(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm1): LayerNormalization()\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (ffn): PositionwiseFeedForward(\n",
              "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (norm2): LayerNormalization()\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (sentence_embedding): SentenceEmbedding(\n",
              "      (embedding): Embedding(125, 512)\n",
              "      (position_encoder): PositionalEncoding()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (layers): SequentialDecoder(\n",
              "      (0): DecoderLayer(\n",
              "        (self_attention): MultiHeadAttention(\n",
              "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (layer_norm1): LayerNormalization()\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (encoder_decoder_attention): MultiHeadCrossAttention(\n",
              "          (kv_layer): Linear(in_features=512, out_features=1024, bias=True)\n",
              "          (q_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (layer_norm2): LayerNormalization()\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (ffn): PositionwiseFeedForward(\n",
              "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (layer_norm3): LayerNormalization()\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (linear): Linear(in_features=512, out_features=125, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "asUJX-STy7fg"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "\n",
        "    def __init__(self, english_sentences, kannada_sentences):\n",
        "        self.english_sentences = english_sentences\n",
        "        self.kannada_sentences = kannada_sentences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.english_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.english_sentences[idx], self.kannada_sentences[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "-auNWjkdzDge"
      },
      "outputs": [],
      "source": [
        "dataset = TextDataset(english_sentences, kannada_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roH2A4m4zF4z",
        "outputId": "f4353aa8-2f37-43b6-be0f-12aab9a35145"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "164022"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGeHNlzozIGF",
        "outputId": "ec3596fe-feee-426c-dce8-373fb07080fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(\"'but we speak the truth aur ye sach hai ke gujarat mein vikas pagal hogaya hai,'' rahul gandhi further said in banaskantha\",\n",
              " '\"ಆದರೆ ಸತ್ಯ ಹೊರ ಬಂದೇ ಬರುತ್ತದೆ ಎಂದು ಹೇಳಿದ ರಾಹುಲ್ ಗಾಂಧಿ, \"\"ಸೂರತ್ ಜನರು ಚೀನಾದ ಜತೆ ಸ್ಪರ್ಧೆ ನಡೆಸುತ್ತಿದ್ದಾರೆ\"')"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "5YDttjQ0zMrv"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(dataset, batch_size)\n",
        "iterator = iter(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EnjHKB1zM8Y",
        "outputId": "1a825e56-6706-46ee-85b5-ed7f0ac657fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('hes a scientist.', \"'but we speak the truth aur ye sach hai ke gujarat mein vikas pagal hogaya hai,'' rahul gandhi further said in banaskantha\", '8 lakh crore have been looted.', 'i read a lot into this as well.', 'how did mankind come under satans rival sovereignty?', 'and then i became prime minister.', 'what about corruption?', '\"\"\"the shooting of the film is 90 percent done.\"', 'the special statute', '\"then the king said to ittai the gittite, \"\"why do you also go with us? return, and stay with the king. for you are a foreigner, and also an exile. return to your own place.\"', 'what happened at the un general assembly?', 'the meeting was attended by prime minister narendra modi, home minister amit shah and defence minister rajnath singh, among others.', 'it has been under discussion for a long time.', 'buses cannot get there.', 'why then this tradition was not thought of?', 'kashmiri youth join indian army', 'basic amenities elude this village', 'off-budget borrowings of the state increased from rs853 crore in 2011-12 to rs,173 crore in 2017-18', 'during the monsoon season, the rubbish is swept on to the road by rainwater, creating problems for the traffic.', 'however, the other two, ramesh jarkiholi and mahesh kumatahalli, had not given any reason, he said.', 'how to make pasta salad?', 'he accused the modi government of ruining the countrys economy.', 'add chopped carrots and potatoes.', 'first shot', 'after the incident, police reached the spot and admitted the injured to the hospital.', 'her father gave kunti to his childless cousin kuntibhoja.', 'how to eat', 'granted, the standard of living varies from country to country.', 'for example, medical.', 'thats a fine plan.'), ('ಇವರು ಸಂಶೋಧಕ ಸ್ವಭಾವದವರು.', '\"ಆದರೆ ಸತ್ಯ ಹೊರ ಬಂದೇ ಬರುತ್ತದೆ ಎಂದು ಹೇಳಿದ ರಾಹುಲ್ ಗಾಂಧಿ, \"\"ಸೂರತ್ ಜನರು ಚೀನಾದ ಜತೆ ಸ್ಪರ್ಧೆ ನಡೆಸುತ್ತಿದ್ದಾರೆ\"', 'ಕಳ್ಳತನವಾಗಿದ್ದ 8 ಲಕ್ಷ ರೂ.', 'ಇದರ ಬಗ್ಗೆ ನಾನೂ ಸಾಕಷ್ಟು ಓದಿದ್ದೇನೆ.', 'ಮಾನವಕುಲವು ಸೈತಾನನ ಆಳಿಕೆಯ ಕೆಳಗೆ ಬಂದದ್ದು ಹೇಗೆ?', 'ನಂತರ ಪ್ರಧಾನಿ ಕೂಡ ಆಗುತ್ತೇನೆ.', 'ಭ್ರಷ್ಟಾಚಾರ ಏಕಿದೆ?', 'ಆ ಚಿತ್ರದ ಶೇ 90ರಷ್ಟು ಚಿತ್ರೀಕರಣವೂ ಈಗಾಗಲೇ ಮುಗಿದು ಹೋಗಿದೆ.', 'ವಿಶೇಷ ಕಾನೂನು', 'ಆಗ ಅರಸನು ಗಿತ್ತೀಯನಾದ ಇತ್ತೈಯನ್ನು ನೋಡಿ--ನೀನು ನಮ್ಮ ಸಂಗಡ ಬರುವದು ಯಾಕೆ? ನಿನ್ನ ಸ್ಥಳಕ್ಕೆ ಹಿಂದಿರುಗಿ ಹೋಗಿ ಅರಸನ ಸಂಗಡ ಇರು. ಯಾಕಂದರೆ ನೀನು ಸೆರೆಹಿಡಿಯಲ್ಪಟ್ಟವನಾದ ಅನ್ಯದೇಶದವನು.', 'ವಿಶ್ವ ಗೋ ಸಮ್ಮೇಳನದ ಅಂಗಳದಲ್ಲಿ ಏನೇನು ನಡೆದಿದೆ?', 'ಪ್ರಧಾನ ಮಂತ್ರಿ ನರೇಂದ್ರ ಮೋದಿ, ರಕ್ಷಣಾ ಸಚಿವ ರಾಜನಾಥ್ ಸಿಂಗ್ ಮತ್ತು ಕೇಂದ್ರ ಗೃಹ ಸಚಿವ ಅಮಿತ್ ಷಾ ಅವರು ಮಸೂದೆಯ ಬಗ್ಗೆ ಸಾರ್ವಜನಿಕ ಚರ್ಚೆ ಗೆ ಬರುವಂತೆ ಸಂಘ ಸವಾಲು ಹಾಕಿದೆ.', 'ಎಂಬುದು ಬಹಳ ದೀರ್ಘ ಕಾಲದಿಂದಲೂ ಚರ್ಚಿತವಾಗುತ್ತಿರುವ ವಿಷಯ.', 'ಇಲ್ಲಿಗೆ ಬರಲು ಬಸ್ ಸೌಕರ್ಯವೂ ಇಲ್ಲ.', 'ಆ ಪರಂಪರೆ ಯಾಕೆ ಮುನ್ನೆಲೆಗೆ ಬರಲಿಲ್ಲ?', 'ಭಾರತೀಯ ಸೇನೆ ಸೇರಲು ಮುಗಿಬೀಳುತ್ತಿರುವ ಕಾಶ್ಮೀರಿ ಯುವಕರು !', 'ಕುಗ್ರಾಮವಾದ ಈ ಹಳ್ಳಿಯಲ್ಲಿ ಮೂಲಭೂತ ಸೌಕರ್ಯಗಳು', 'ರಾಜ್ಯದ ಬಜೆಟ್ ಯೇತರ ಸಾಲವು 2011-12ರಲ್ಲಿದ್ದ 1,853ಕೋಟಿ ರೂಪಾಯಿಯಿಂದ 2017-18ರಲ್ಲಿ 13,173 ಕ್ಕೆ ಏರಿಕಯಾಗಿದೆ.', 'ಮಳೆಗಾಲದಲ್ಲಿ ಕೊಳಚೆ ನೀರಿನೊಂದಿಗೆ ಮಳೆನೀರು ರಸ್ತೆಯಲ್ಲಿಯೇ ಮಡುಗಟ್ಟಿ ನಿಂತು ಸೊಳ್ಳೆಗಳ ಹೆಚ್ಚಳಕ್ಕೆ ಕಾರಣವಾಗುತ್ತಿದೆ.', 'ಯಾವುದೇ ಕಾರಣ ನೀಡದೆ ರಮೇಶ್ ಜಾರಕಿಹೊಳಿ ಮತ್ತು ಮಹೇಶ್ ಕಮಟಹಳ್ಳಿ ಗೈರು ಹಾಜರಾಗಿದ್ದಾರೆ ಎಂದು ಮಾಹಿತಿ ನೀಡಿದ್ದಾರೆ.', '\"ಹೇಗೆ \"\"ಗೂಳಿಕಾಳಗ\"\" ಒಂದು ಸಲಾಡ್ ತಯಾರು ಹೇಗೆ?\"', 'ಅದ್ರಲ್ಲೂ ಪ್ರಮುಖವಾಗಿ ದೇಶದ ಆರ್ಥಿಕತೆ ಪಾತಾಳಕ್ಕೆ ಕುಸಿದಿರೋದಕ್ಕೆ ಮೋದಿ ಸರ್ಕಾರವೇ ಸರ್ಕಾರ ಅಂತಾ ಟೀಕಿಸುತ್ತಿದ್ದಾರೆ.', 'ಅವರಿಗೆ ಕತ್ತರಿಸಿದ ಪಾರ್ಸ್ಲಿ ಮತ್ತು ತುಳಸಿ ಸೇರಿಸಿ.', 'ಮೊದಲ ಚಿತ್ರ ಶೂಟಿಂಗ್', 'ಅಪಘಾತದ ನಡೆದ ಕೂಡಲೇ ಮಾಹಿತಿ ಪಡೆದು ಸ್ಥಳಕ್ಕೆ ಆಗಮಿಸಿದ ಪೋಲಿಸರು ಗಾಯಗೊಂಡವರನ್ನು ಆಸ್ಪತ್ರೆ ದಾಖಲಿಸಿದ್ದಾರೆ.', 'ಆಕೆಯ ತಂದೆ ತನ್ನ ಮಕ್ಕಳಿಲ್ಲದ ಸೋದರಸಂಬಂಧಿ ಕುಂತಿಭೋಜನಿಗೆ ಕುಂತಿಯನ್ನು ಕೊಟ್ಟರು.', 'ಆಹಾರ ಬೇಯಿಸುವುದು ಹೇಗೆ', 'ಜೀವನಮಟ್ಟವು ದೇಶದಿಂದ ದೇಶಕ್ಕೆ ಬದಲಾಗುತ್ತದೆ ನಿಜ.', 'ಉದಾಹರಣೆಗೆ, ಖಗೋಳಶಾಸ್ತ್ರ.', 'ಇದೊಂದು ಉತ್ತಮ ಯೋಜನೆ.')]\n",
            "[(\"you don't know this.\", 'you are respected in society.', 'is this pic real?', '\"\"\"felicitations to all for the foundation laying of ram temple in ayodhya.\"', 'she looked stunningly beautiful in that', '{ -brand-name-nightly } blog', 'it was consistent.', 'case has been registered in malpe police station.', 'they still exist to this day.', 'breaking up isnt easy', 'students should abide by the rules.', 'we try to understand her well.', 'then the angel of yahweh commanded gad to tell david that david should go up, and raise an altar to yahweh in the threshing floor of ornan the jebusite.', 'for example, he forbids sexual immorality, idolatry, stealing, and drunkenness.', 'it is also a famous tourist destination', 'doctors and other persons concerned have been questioned.', 'but in politics.', 'to understand gods actions, we need to answer three questions: (1) who starts the war?', '14,000 crores.', 'what are those words?', 'shivakumars assumption of office as president of karnataka pradesh congress committee (kpcc).', 'now we were notified that we would receive the magazines in russian only.', 'they are found along coastlines around the world except antarcticas.', 'bill clinton went on to become president.', 'the culmination of the three stories towards the end. gives a new dimension to the film as a whole and concludes shuddhi.', 'while individuals may be allowed to die, god will never allow the extermination of his people as a whole.', '(the author is an educationist)', 'therefore i will not refrain my mouth. i will speak in the anguish of my spirit. i will complain in the bitterness of my soul.', 'are they hungry for knowledge?', 'the door did not open.'), ('ಈ ಬಗ್ಗೆ ನೀವಿನ್ನು ತಿಳಿದಿಲ್ಲ ಎ .', 'ಸಮಾಜದಲ್ಲಿ ಗೌರವವಿರುತ್ತದೆ.', 'ಈ ಫೋಟೋ ವಾಸ್ತವೋ, ಅಸಲಿಯೋ ?', \"'ಅಯೋಧ್ಯೆಯ ರಾಮ ದೇವಾಲಯದ ಅಡಿಪಾಯ ಹಾಕಿದ್ದಕ್ಕಾಗಿ ಎಲ್ಲರಿಗೂ ಶುಭಾಶಯಗಳು.\", 'ಅವಳು ಅಥವಾ ಆತ ನೋಡಲು ತುಂಬಾ ಸುಂದರ', 'ನೈಟ್ಲಿ ಬ್ಲಾಗ್', 'ಇದು ಸಂಕೇತವಾಗಿತ್ತು.', 'ಪ್ರಕರಣ ಮಳವಳ್ಳಿ ಪೊಲೀಸ್ ಠಾಣೆಯಲ್ಲಿ ದಾಖಲಾಗಿದೆ.', 'ಅಂತೆಯೇ ಅವರು ಇಂದಿಗೂ ಪ್ರಸ್ತುತವಾಗಿದ್ದಾರೆ.', 'ವೈರಾಗ್ಯ ಸುಲಭವಲ್ಲ', 'ವಿದ್ಯಾರ್ಥಿಗಳು ಕೂಡಾ ಶಿಸ್ತು ಪಾಲಿಸಬೇಕು.', '\"\"\" ಅವರೊಂದಿಗೆ ಅವನಿಗೆ ವಿವರವಾಗಿ ಅರ್ಥಮಾಡಿಕೊಳ್ಳಲು ಪ್ರಯತ್ನಿಸೋಣ.\"', 'ಆಗ ಕರ್ತನ ದೂತನು ಗಾದನಿಗೆ--ದಾವೀದನು ಹೋಗಿ ಯೆಬೂಸಿಯನಾದ ಒರ್ನಾನನ ಕಣದಲ್ಲಿ ಕರ್ತನಿಗೆ ಬಲಿಪೀಠವನ್ನು ಕಟ್ಟುವ ಹಾಗೆ ದಾವೀದನಿಗೆ ಹೇಳು ಅಂದನು.', 'ಬೈಬಲಿನಲ್ಲಿ ಯೆಹೋವನು ನಮಗೆ ಸ್ಪಷ್ಟ ಆಜ್ಞೆಗಳನ್ನು ಕೊಟ್ಟಿದ್ದಾನೆ.', 'ಇದೊಂದು ಬಹು ಜನಪ್ರಿಯವಾದ ವಿಹಾರ ತಾಣವಾಗಿದೆ', 'ಶಸ್ತ್ರ ಚಿಕಿತ್ಸೆ ಮಾಡಿದ ವೈದ್ಯರು ಮತ್ತು ಇತರ ಸಿಬ್ಬಂದಿಯನ್ನು ವಿಚಾರಣೆಗೆ ಒಳಪಡಿಸಿದ್ದಾರೆ.', 'ಆದರೆ ರಾಜಕೀಯದಲ್ಲಿರುತ್ತೇನೆ.', '( ನೆಹೆಮಿಾಯ 9: 17) ಇದನ್ನು ಅರ್ಥಮಾಡಿಕೊಳ್ಳಲು ಈ ಮೂರು ಪ್ರಶ್ನೆಗಳಿಗೆ ಉತ್ತರ ತಿಳಿಯಿರಿ: (1) ಯುದ್ಧ ಆರಂಭಿಸುವವರು ಯಾರು?', '14,000 ಒಟ್ಟುಗೂಡಿತು.', 'ಈ ಶಬ್ದಗಳು ಯಾವುವು?', 'ಕೊನೆಗೂ ಡಿಕೆ ಶಿವಕುಮಾರ್ ಅವರನ್ನ ಕರ್ನಾಟಕ ಪ್ರದೇಶ ಕಾಂಗ್ರೆಸ್ (ಕೆಪಿಸಿಸಿ) ಅಧ್ಯಕ್ಷರನ್ನಾಗಿ ಆಯ್ಮೆ ಮಾಡಿದೆ.', 'ಪತ್ರಿಕೆಯು ನಿಷೇಧಿಸಲ್ಪಟ್ಟಿತ್ತು, ಆದರೆ ಇತರ ಸ್ಥಳಗಳಿಂದ ಗುಟ್ಟಾಗಿ ನಾವು ಪತ್ರಿಕೆಗಳನ್ನು ಪಡೆಯುತ್ತಿದ್ದೆವು.', 'ಅಂಟಾರ್ಟಿಕಾ ಖಂಡದ ಹೊರತುಪಡಿಸಿ ಅವು ವಿಶ್ವದಾದ್ಯಂತ ಕಂಡುಬರುತ್ತವೆ.', 'ಇವರ ನಂತರ ಬಿಲ್ ಕ್ಲಿಂಟನ್ ಅಧ್ಯಕ್ಷ ಪಟ್ಟ ಅಲಂಕರಿಸಿದ್ದರು.', 'ಕೊನೆಯಲ್ಲಿ ಮೂರು ಕಥೆಗಳ ಪರಾಕಾಷ್ಠೆ. ಒಟ್ಟಾರೆಯಾಗಿ ಚಿತ್ರಕ್ಕೆ ಹೊಸ ಆಯಾಮವನ್ನು ನೀಡುತ್ತದೆ ಮತ್ತು ಸುಧಿಯನ್ನು ಮುಕ್ತಾಯಗೊಳಿಸುತ್ತದೆ.', 'ಕೆಲವು ವ್ಯಕ್ತಿಗಳು ಸಾಯುವಂತೆ ಅನುಮತಿಸಲ್ಪಡುವುದಾದರೂ, ತನ್ನ ಜನರೆಲ್ಲರೂ ಸಂಪೂರ್ಣವಾಗಿ ನಿರ್ಮೂಲರಾಗುವಂತೆ ದೇವರು ಎಂದಿಗೂ ಅನುಮತಿಸುವುದಿಲ್ಲ.', '(ಲೇಖಕರು ಶಿಕ್ಷಣತಜ್ಞರು)', 'ಆದದರಿಂದ ನಾನು ನನ್ನ ಬಾಯಿಯನ್ನು ಮುಚ್ಚು ವದಿಲ್ಲ. ನಾನು ಆತ್ಮ ವೇದನೆಯಿಂದ ಮಾತನಾಡು ವೆನು. ನನ್ನ ಮನೋವ್ಯಥೆಯಲ್ಲಿ ನಾನು ಗುಣುಗುಟ್ಟು ವೆನು.', 'ನಿಂಗೆ ಜ್ಞಾನದ ಹಸಿವು ನೀಗಸ್ತಾರೆ ?', 'ಅಂಗಳದ ಬಾಗಿಲು ಹಾಕಿರಲಿಲ್ಲ.')]\n",
            "[('i am non-vegetarian.', 'most of the government schools lack the teachers.', 'to the fans.', 'during the investigation, it was found that the 130 activists were in regular contact with the jmb leadership, he said.', 'it is 260 km away from mumbai city.', 'not everything happens as we expect.', 'what are you talking?', 'that young couple certainly started their marriage off on the best of foundations.', 'abbey waterfall flows from a height of 60 feet.', 'the treatment depends largely on the type and stage of the cancer.', 'i learned how to cook.', 'is coconut oil harmful?', 'may i answer?', 'what is love?', 'symptoms include rashes, diarrhea, vomiting, stomach cramps and bloating.', 'during 2015, they plan on introducing two new amg vehicles the c 63 s and gt s supercar', 'candidates belonging to st/sc, pwd category are exempted from paying any fee.', 'i love', 'you cant have everything always.', 'the death toll of coronavirus rose to 718 as 37 casualties were reported in 24 hours', 'his life was devoted to the service of the country.', 'he lost his father at an early age.', 'the vehicle is completely burnt.', 'but it was cancelled at the last moment.', 'many people are dead due to floods and extensive damage to property has occurred.', 'the scheme will be rolled out in the next couple of months.', 'what is not nice?', 'and he said, bring me a new cruse, and put salt therein. and they brought it to him.', 'the tvs entorq 125 will be up against the likes of honda activa, suzuki access and others', 'he handed over the documents for the 900 sq ft plot to nagar panchayat chairman zahir farooqui.'), ('ನಾನೇನು ಸಂಪೂರ್ಣ ಸಸ್ಯಾಹಾರಿ ಅಲ್ಲ.', 'ಬಹುತೇಕ ಸರ್ಕಾರಿ ಶಾಲೆಗಳಲ್ಲಿ ಮೂಲಭೂತ ಸೌಕರ್ಯಗಳೇ ಇಲ್ಲದಿರುವುದು ಕಂಡುಬಂದಿದೆ.', 'ಎಂಬ ಅಭಿಮಾನಿಗಳಿಗೆ ಕಾಡುತ್ತಿದೆ.', 'ಜೆಎಂಬಿ ನಾಯಕತ್ವದ ಜತೆ 130 ಕಾರ್ಯಕರ್ತರು ನಿಕಟ ಸಂಪರ್ಕದಲ್ಲಿರುವುದು ತನಿಖಾ ಹಂತದಲ್ಲಿ ಬೆಳಕಿಗೆ ಬಂದಿದೆ ಎಂದು ತಿಳಿಸಿದರು.', 'ಇದು ರಾಜ್ಯದ ರಾಜಧಾನಿಯಾದ ಮುಂಬೈನಿಂದ ಸುಮಾರು 260 ಕಿಮೀ ದೂರದಲ್ಲಿದೆ.', 'ಎಲ್ಲವೂ ನಾವು ಅಂದುಕೊಂಡಂತೆಯೇ ಆಗುವುದಿಲ್ಲ.', '\"\"\"ಏನೆಲ್ಲ ಮಾತಾಡತಾನೆ?\"', 'ಕೊನೆಯದಾಗಿ, ಈ ಘಟನೆಗಳು ಯೋಸೇಫ ಮರಿಯ ಇಬ್ಬರಿಗೂ ಪ್ರಾಮಾಣಿಕವೂ ಮುಕ್ತವೂ ಆದ ಸಂವಾದವು ಎಷ್ಟು ಮಹತ್ವ ಎಂಬ ವಿಷಯದಲ್ಲಿ ಹೆಚ್ಚನ್ನು ಕಲಿಸಿದ್ದಿರಬೇಕು.', '60 ಅಡಿ ಎತ್ತರದಿಂದ ನೀರು ಧರೆಗೆ ಧುಮ್ಮಿಕ್ಕುತ್ತದೆ.', 'ಚಿಕಿತ್ಸೆಯ ವಿಧಾನದ ಆಯ್ಕೆ ಹೆಚ್ಚಾಗಿ ರೋಗದ ಹಂತ ಮತ್ತು ಸ್ವರೂಪವನ್ನು ಅವಲಂಬಿಸಿರುತ್ತದೆ.', 'ಅಷ್ಟುಇಷ್ಟು ಅಡುಗೆ ಮಾಡುವುದನ್ನು ಕಲಿತುಬಿಟ್ಟೆ.', 'ಕಾರ್ನ್ ಎಣ್ಣೆ ಉಪಯುಕ್ತವಾಗಿದೆ?', '\"ಪ್ರಶ್ನೆ, \"\"ನಾನು ಉತ್ತರಿಸಬೇಕೇ?\"', 'ಪ್ರೀತಿ ಅಂದರೆ ಇದೇನಾ?', 'ರೋಗಲಕ್ಷಣಗಳು ಕ್ಯಾಂಪಿಂಗ್, ಭಾರಿ ಅವಧಿ, ಹೆಪ್ಪುಗಟ್ಟುವಿಕೆ, ಕೆಳ ಹೊಟ್ಟೆ ನೋವು, ಮತ್ತು ಉಬ್ಬುವುದು.', 'ಈ ಸಾಲಿಗೆ ಪ್ರಸಕ್ತ ವರ್ಷದಲ್ಲಿ ಎಎಂಜಿ ಸಿ 63 ಎಸ್ ಮತ್ತು ಜಿಟಿ ಎಸ್ ಸೂಪರ್ ಕಾರು ಸೇರ್ಪಡೆಯಾಗಲಿದೆ', 'ಎಸ್ಸಿ, ಎಸ್ಟಿ / ಪಿಡಬ್ಲ್ಯೂಡಿ ವಿಭಾಗಗಳಿಗೆ ಸೇರಿದ ಅಭ್ಯರ್ಥಿಗಳು ಅರ್ಜಿ ಪ್ರಕ್ರಿಯೆ ಶುಲ್ಕದಿಂದ ವಿನಾಯಿತಿ ಪಡೆದಿರುತ್ತಾರೆ.', 'ನಾನು ಪ್ರೀತಿಸಿದ್ದೇನೆ.', 'ಯಾವಾಗಲೂ ಎಲ್ಲಾ ಪದಾರ್ಥಗಳನ್ನು ಹೊಂದಿಲ್ಲ.', 'ದೇಶಾದ್ಯಂತ ಕರೋನಾ ಸೋಂಕಿತರ ಸಂಖ್ಯೆ 23 ಸಾವಿರಕ್ಕೂ ಹೆಚ್ಚಿದ್ದು, ಒಟ್ಟು ಈವರೆಗೆ 718 ಜನರು ಮೃತಪಟ್ಟಿದ್ದಾರೆ', 'ಅವರ ಜೀವನವನ್ನು ರಾಷ್ಟ್ರ ಸೇವೆಗೆ ಮಾತ್ರ ಮುಡಿಪಾಗಿಟ್ಟಿದ್ದರು ಎಂದು ತಿಳಿಸಿದರು.', 'ಬಾಲ್ಯದಲ್ಲೇ ತಂದೆಯನ್ನು ಕಳೆದುಕೊಂಡರು.', 'ಕಾರು ಸಂಪೂರ್ಣ ಬೆಂಕಿಗೆ ಆಹುತಿ ಆಗಿದೆ.', 'ಆದರೆ ಕೊನೇ ಕ್ಷಣದಲ್ಲಿ ತಮ್ಮ ನಿರ್ಧಾರದಿಂದ ಹಿಂದೆ ಸರಿದಿದ್ದಾರೆ.', 'ಪ್ರವಾಹದಿಂದ ಬಹಳಷ್ಟು ಆಸ್ತಿ ನಷ್ಟವಾಗಿದ್ದು, ಜನರು ಸಂಕಷ್ಟದಲ್ಲಿದ್ದಾರೆ.', 'ಮುಂದಿನ ಕೆಲವು ತಿಂಗಳಲ್ಲಿ ಈ ಯೋಜನೆಯನ್ನು ಪ್ರಕಟಿಸಲಾಗುವುದು.', 'ಚೆಂದ ಇಲ್ಲದಿದ್ದರೇನು?', 'ಅದಕ್ಕ ವನು--ನನ್ನ ಬಳಿಗೆ ಹೊಸ ಗಡಿಗೆಯನ್ನು ತಕ್ಕೊಂಡು ಬಂದು ಅದರಲ್ಲಿ ಉಪ್ಪು ಹಾಕಿರಿ ಅಂದನು. ಅವರು ಅದನ್ನು ಅವನ ಬಳಿಗೆ ತಕ್ಕೊಂಡು ಬಂದರು.', 'ಪ್ರಮುಖವಾಗಿಯೂ ಹೋಂಡಾ ಆಕ್ಟಿವಾ 125, ಸುಜುಕಿ ಆಕ್ಸೆಸ್ 125 ಹಾಗೂ ಮಹೀಂದ್ರ ಗಸ್ಟೊ 125 ಮಾದರಿಗಳಿಗೆ ಟಿವಿಎಸ್ ನೂತನ ಸ್ಕೂಟರ್ ಪ್ರತಿಸ್ಪರ್ಧಿಯಾಗಲಿದೆ', '900 ಚದರ ಅಡಿ ಜಾಗದ ಭೂ ದಾಖಲೆಗಳನ್ನು ಅವರು ನಗರ ಪಂಚಾಯತ್ ಅಧ್ಯಕ್ಷ ಜಹೀರ್ ಫಾರೂಕಿ ಅವರಿಗೆ ಹಸ್ತಾಂತರಿಸಿದರು.')]\n",
            "[('the engine is mated to a six-speed gearbox with a slip and assist clutch', 'this is what he has said.', 'but it is not taking off.', \"conflicting reports on amitabh bachchan's health confuse fans\", 'howsoever powerful.', 'the conversation thereafter went as follows:', 'what is the opposition doing?', 'best of luck to all team members.', 'but it is very expensive.', \"shah rukh khan's next film is titled sanki and will be directed by tamil director atlee.\", 'thats exactly what organizations are saying.', 'bike rider killed in motorcycle-tanker collision', 'students need not worry about it, he said.', 'there is no risk to anyone.', 'the matter had created uproar across the state.', 'parents are overjoyed to see the success of their children.', 'elections will be held soon.', 'to and fro vehicular traffic to the hill from uttanahalli road side has been completely banned.', 'do you leave?', 'define your goals and objectives.', \"what's up.\", 'how long will you take for completing the investigation?', 'there are two engine options.', 'the film in question has not yet received the certificate from censor board.', '\"even chhatrapati shivaji maharaj faced opposition from his own family,\"\" he said.\"', 'how to make a cake at home?', 'there was no harm to any passenger due to the fire.', 'they too moved their forces.', 'development has taken a hit in the state.', 'and this can be done by:'), ('ಈ ಎಂಜಿನ್ ಅನ್ನು 6- ಸ್ಪೀಡ್ ಗೇರ್ಬಾಕ್ಸ್ಗೆ ಅಸಿಸ್ಟ್ ಮತ್ತು ಸ್ಲಿಪ್ಪರ್ ಕ್ಲಚ್ನೊಂದಿಗೆ ಜೋಡಿಸಲಾಗಿದೆ', 'ಈ ಮೂಲಕ ತಾವೇನೆಂಬುದನ್ನು ತಿಳಿಸಿದ್ದಾರೆ.', 'ಆದರೆ, ಅದನ್ನು ಬಿಡುಗಡೆ ಮಾಡುತ್ತಿಲ್ಲ.', 'ಅಭಿಮಾನಿಗಳಲ್ಲಿ ಗೊಂದಲ ಸೃಷ್ಟಿಸಿದ ಅಮಿತಾಬ್ ಬಚ್ಚನ್ ಅನಾರೋಗ್ಯ ಸುದ್ದಿ', 'ಷ್ಟೇ ಬಲಶಾಲಿ ಆಗಿರಲಿ.', 'ಆನಂತರ ನಡೆದ ಸಂಭಾಷಣೆಯ ಸಾರಾಂಶ ಹೀಗಿದೆ.', 'ವಿರೋಧ ಪಕ್ಷದವರ ಕೆಲಸ ಏನಿದೆ?', 'ತಂಡದ ಸದಸ್ಯರಿಗೆಲ್ಲಾ ಅದೃಷ್ಟ ತಂದುಕೊಡುತ್ತವೆ.', 'ಆದರೆ ಆರ್ಥಿಕ ದೃಷ್ಟಿಯಿಂದ ಬಹಳ ದುಬಾರಿಯಾಗಿರುತ್ತದೆ.', 'ಶಾರುಖ್ ಖಾನ್ ಅವರ ಮುಂದಿನ ಚಲನಚಿತ್ರವನ್ನು ತಮಿಳು ನಿರ್ದೇಶಕ ಅಟ್ಲೀ ನಿರ್ದೇಶನ ಮಾಡಲಿದ್ದಾರೆ.', 'ಹಾಗೆಂದು ಆ ಸಂಸ್ಥೆಯೇ ಹೇಳಿಕೊಳ್ಳುತ್ತಿದೆ.', 'ಕಬ್ಬು ಸಾಗಿಸುತ್ತಿದ್ದ ಲಾರಿ ಹಾಗೂ ಬೈಕ್ ನಡುವೆ ಡಿಕ್ಕಿ : ಸವಾರ ಸಾವು', 'ಇದರ ಬಗ್ಗೆ ಯಾವುದೇ ಆತಂಕ ಬೇಡ ಎಂದು ವಿದ್ಯಾರ್ಥಿಗಳಿಗೆ ತಿಳಿಸಿದರು.', 'ಯಾರಿಗೂ ಅಂತಹ ಅಪಾಯಗಳಾಗಿಲ್ಲ ಎಂದರು.', 'ಈ ಸುದ್ದಿ ರಾಜ್ಯಾದ್ಯಂತ ಚರ್ಚೆಗೆ ಕಾರಣವಾಗಿತ್ತು.', 'ಪ್ರತಿ ಹೆತ್ತವರಿಗೂ ಅವರವರ ಮಕ್ಕಳ ಸಾಧನೆ ನೋಡೋ ಖುಷಿ ಇದ್ದೇ ಇರುತ್ತೆ.', 'ಚುನಾವಣೆ ಇನ್ನೇನು ಸಧ್ಯದಲ್ಲೇ ನಡೆಯಲಿದೆ.', 'ಉತ್ತನಹಳ್ಳಿ ರಸ್ತೆಯ ಕಡೆಯಿಂದ ಚಾಮುಂಡಿ ಬೆಟ್ಟಕ್ಕೆ ಬರುವ ಮತ್ತು ಹೋಗುವ ವಾಹನಗಳಿಗೆ ಸಂಚಾರವನ್ನು ಸಂಪೂರ್ಣವಾಗಿ ನಿರ್ಬಂಧಿಸಲಾಗಿದೆ.', 'ಬಿಡಲಿಕ್ಕೆ ಆಗುತ್ತಾ?', 'ನಿಮ್ಮ ಗುರಿ ಮತ್ತು ಉದ್ದೇಶಗಳನ್ನು ಗುರುತಿಸಿ.', 'ಏನು ಉರುಳೇ.', 'ತನಿಖೆಯನ್ನು ಪೂರ್ಣಗೊಳಿಸಲು ನಿಮಗೆ ಇನ್ನೂ ಎಷ್ಟು ಸಮಯ ಬೇಕು?', 'ಎರಡು ಎಂಜಿನ್ ಆಯ್ಕೆಗಳಿವೆ.', 'ಸೆನ್ಸಾರ್ ಮಂಡಳಿಯಿಂದ ಈ ಸಿನಿಮಾಗೆ ಇನ್ನೂ ಪ್ರಮಾಣಪತ್ರ ನೀಡಿಲ್ಲ.', 'ಛತ್ರಪತಿ ಶಿವಾಜಿ ಕೂಡ ತಮ್ಮದೇ ಕುಟುಂಬದಿಂದಲೇ ವಿರೋಧವನ್ನು ಎದುರಿಸಿದ್ದರು.', 'ಹೇಗೆ ಮನೆಯಲ್ಲಿ ಒಂದು ಕಾರ್ಟೂನ್ ರಚಿಸಲು?', 'ಬೆಂಕಿ ಅವಘಡದಲ್ಲಿ ಪ್ರಯಾಣಿಕರ ಪ್ರಾಣಕ್ಕೆ ಹಾನಿಯಾಗಿಲ್ಲ.', 'ತಮ್ಮ ಅಧಿಕಾರವನ್ನು ಚಲಾಯಿಸಿದ್ದರು ಕೂಡಾ.', 'ರಾಜ್ಯದಲ್ಲಿ ಅಭಿವೃದ್ಧಿ ಪರ್ವ ಬಂದಿದೆ.', 'ಮತ್ತು ನೀವು ಇದನ್ನು ಪರಿಹರಿಸಬಹುದು:')]\n",
            "[('a proposal has been made.', 'but the government is unmoved.', 'the programme was attended by students and teachers of the school.', 'they taught me much about the true holy father in heaven, jehovah god.', 'what we can do', 'the bjp is trying to gain a strong foothold in the state.', 'farmers are in distress all over the country.', 'but euphoria, relief, and achievement likewise provoke emotional tears in this case, tears of joy.', 'and to pass by you into macedonia, and to come again out of macedonia unto you, and of you to be brought on my way toward judaea.', 'in case of an attack, we need to respond swiftly to minimise the damage.', 'for unknown reasons.', 'community spread', 'which board?', 'there are 14 girls and 12 junior players.', 'preparing for the role', 'recipe: banana blossom salad', 'do not isolate yourself from your faithful christian brothers and sisters.', 'patient with leukaemia disease.', 'the total length of the highway road is 250 km.', 'the governors post is a constitutional post.', 'development and growth', 'it was even found among the ones he had chosen as apostles!', 'in their hands lies the future of india.', 'government high school', 'they stay in their own world.', 'you cant be regretful.', 'location: ranga rao road, near shankar mutt, shankarapura, near basavanagudi, bangalore', \"that's all humbug.\", 'they were all there.', 'he was keen to learn english also.'), ('ಪ್ರಸ್ತಾವನೆ ಸಲ್ಲಿಸಿತ್ತು.', 'ಆದರೆ ಈ ಸರ್ಕಾರ ಸದ್ಯ ಅತಂತ್ರದಲ್ಲಿದೆ.', 'ಈ ಕಾರ್ಯಕ್ರಮದಲ್ಲಿ ಶಿಕ್ಷಕ ವೃಂದ ಹಾಗೂ ವಿದ್ಯಾರ್ಥಿಗಳು ಭಾಗಿಯಾಗಿದ್ದರು.', 'ಅವರು ನನಗೆ ಸ್ವರ್ಗದಲ್ಲಿರುವ ನಿಜವಾದ ಪವಿತ್ರ ತಂದೆ ಯೆಹೋವ ದೇವರ ಬಗ್ಗೆ ಬಹಳಷ್ಟನ್ನು ಕಲಿಸಿದರು.', 'ನಾವೇನು ಮಾಡಬಹುದು ?', 'ರಾಜ್ಯದಲ್ಲಿ ಬಿಜೆಪಿ ಅಧಿಕಾರ ಪಡೆದುಕೊಳ್ಳಲು ಶಥಾಯ ಗಥಾಯ ಪ್ರಯತ್ನ ಮಾಡುತ್ತಿದೆ', 'ದೇಶಾದ್ಯಂತ ರೈತರು ಸಂಕಷ್ಟದಲ್ಲಿದ್ದಾರೆ.', 'ಭಾವನಾತ್ಮಕ ಕಣ್ಣೀರು ಉಕ್ಕಿ ಬರಲು ಅನೇಕ ಕಾರಣಗಳಿವೆ.', 'ತರುವಾಯ ನಿಮ್ಮ ಮಾರ್ಗವಾಗಿ ಮಕೆದೋನ್ಯಕ್ಕೆ ಹೋಗಿ ತಿರಿಗಿ ಮಕೆದೋನ್ಯದಿಂದ ನಿಮ್ಮ ಬಳಿಗೆ ಬಂದು ನಿಮ್ಮಿಂದ ಯೂದಾಯಕ್ಕೆ ಸಾಗಕಳುಹಿಸಲ್ಪಡುವಂತೆಯೂ ಯೋಚಿಸಿದ್ದೆನು.', 'ದಾಳಿಯ ಸಂದರ್ಭದಲ್ಲಿ ಹಾನಿಯನ್ನು ಕನಿಷ್ಠಗೊಳಿಸಲು ನಾವು ಚುರುಕಾಗಿ ಕಾರ್ಯಾಚರಿಸಬೇಕಾಗುತ್ತದೆ.', 'ಸ್ಪಷ್ಟೀಕರಿಸದ ಕಾರಣಗಳಿಗಾಗಿ', 'ಸಮುದಾಯ ಹರಡುವಿಕೆ', 'ಯಾರಿಗೆ ಯಾವ ಮಂಡಳಿ?', 'ಇವುಗಳಲ್ಲಿ 14 ಗಂಡು, 12 ಹೆಣ್ಣು ಚಿರತೆ ಸೇರಿವೆ.', 'ಪಾತ್ರಕ್ಕಾಗಿ ತಯಾರಿ', '\"ಪಾಕವಿಧಾನ: ಹೂಕೋಸು \"\"ಆಲೂಗಡ್ಡೆ\"\" ಸಲಾಡ್\"', 'ನಂಬಿಗಸ್ತ ಸಹೋದರ ಸಹೋದರಿಯರಿಂದ ನಿಮ್ಮನ್ನು ಪ್ರತ್ಯೇಕಿಸಿಕೊಳ್ಳಬೇಡಿ.', 'ತೀವ್ರತರವಾದ ಲ್ಯುಕೇಮಿಯಾ ಬಳಲುತ್ತಿರುವ ರೋಗಿಗಳು.', 'ರಸ್ತೆಯಲ್ಲಿ ಒಟ್ಟು ಅಗಲ - 250 ಮೀಟರ್.', 'ರಾಜ್ಯಪಾಲರದು ಸಂವಿಧಾನಾತ್ಮಕ ಹುದ್ದೆ.', 'ಅಭಿವೃದ್ಧಿ ಮತ್ತು ಅನಾರೋಗ್ಯ', 'ಯೇಸು ಸಾಯುವ ದಿನದ ವರೆಗೂ ಅವರು ಹೆಬ್ಬಯಕೆಯನ್ನು ತೋರ್ಪಡಿಸಿದರು.', 'ಇವರ ಆಟದ ಮೇಲೆ ಭಾರತದ ಭವಿಷ್ಯ ನಿಂತಿದೆ.', 'ಸಮಸ್ಯೆಗಳ ಆಗರ ಸರ್ಕಾರಿ ಪ್ರೌಢಶಾಲೆ', 'ಇವರು ತಮ್ಮದೇ ಆಗಿರುವ ಲೋಕದಲ್ಲಿ ವಿಹರಿಸುತ್ತಾ ಇರುವರು.', 'ನಿಮಗೆ ಕ್ಷಮಿಸುವ ಮನಸ್ಸಿಲ್ಲದಿರಬಹುದು.', 'ಸ್ಥಳ : ಜಕ್ಕಸಂದ್ರ, ಸರ್ಜಾಪುರ ರಸ್ತೆ, ಬೆಂಗಳೂರು', 'ಅದೆಲ್ಲ ಗಿಮಿಕ್ ಅಷ್ಟೆ.', 'ಅವರೆಲ್ಲರೂ ಇದ್ದರು.', 'ಅವರಿಗೆ ಇಂಗ್ಲಿಷ್ ಕಲಿಯುವುದಕ್ಕೆ ತುಂಬಾ ಆಸೆ ಇತ್ತು.')]\n"
          ]
        }
      ],
      "source": [
        "for batch_num, batch in enumerate(iterator):\n",
        "    print(batch)\n",
        "    if batch_num > 3:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "XnanjzqtzQi8"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "criterian = nn.CrossEntropyLoss(ignore_index=kannada_to_index[PADDING_TOKEN],\n",
        "                                reduction='none')\n",
        "\n",
        "# When computing the loss, we are ignoring cases when the label is the padding token\n",
        "for params in transformer.parameters():\n",
        "    if params.dim() > 1:\n",
        "        nn.init.xavier_uniform_(params)\n",
        "\n",
        "optim = torch.optim.Adam(transformer.parameters(), lr=1e-4)\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "_saWU5QmVem2"
      },
      "outputs": [],
      "source": [
        "NEG_INFTY = -1e9\n",
        "\n",
        "def create_masks(eng_batch, kn_batch):\n",
        "    num_sentences = len(eng_batch)\n",
        "    look_ahead_mask = torch.full([max_sequence_length, max_sequence_length] , True)\n",
        "    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n",
        "    encoder_padding_mask = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
        "    decoder_padding_mask_self_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
        "    decoder_padding_mask_cross_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
        "\n",
        "    for idx in range(num_sentences):\n",
        "      eng_sentence_length, kn_sentence_length = len(eng_batch[idx]), len(kn_batch[idx])\n",
        "      eng_chars_to_padding_mask = np.arange(eng_sentence_length + 1, max_sequence_length)\n",
        "      kn_chars_to_padding_mask = np.arange(kn_sentence_length + 1, max_sequence_length)\n",
        "      encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True\n",
        "      encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True\n",
        "      decoder_padding_mask_self_attention[idx, :, kn_chars_to_padding_mask] = True\n",
        "      decoder_padding_mask_self_attention[idx, kn_chars_to_padding_mask, :] = True\n",
        "      decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True\n",
        "      decoder_padding_mask_cross_attention[idx, kn_chars_to_padding_mask, :] = True\n",
        "\n",
        "    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)\n",
        "    decoder_self_attention_mask =  torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n",
        "    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n",
        "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdgtTSKvwN9_"
      },
      "source": [
        "Modify mask such that the padding tokens cannot look ahead.\n",
        "In Encoder, tokens before it should be -1e9 while tokens after it should be -inf.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLcXI4wkMLck"
      },
      "source": [
        "Note the target mask starts with 2 rows of non masked items: https://github.com/SamLynnEvans/Transformer/blob/master/Beam.py#L55\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju59VDGLuOqf",
        "outputId": "0ad34e31-521a-4ca2-f444-26b5374946f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Iteration 0 : 5.886385440826416\n",
            "English: hes a scientist.\n",
            "Kannada Translation: ಇವರು ಸಂಶೋಧಕ ಸ್ವಭಾವದವರು.\n",
            "Kannada Prediction: ಝ3ಝಡ#ೋಆಝಣಝಝಣಣ33ಖಣಣಐಣೋಝಐಲಲಣೋೋೋೋೋಲೋಲಝಏೋಈ+ೋಈಋಋಖಖಏಙೃೋ4ಖಏಏಖಲಖಏಈಖೋೋೋೋಈಷ6ೋೋಣಾಏ3ೋಖಈಖಏಈ6ಈೋಏಌೋೋಏಏೋಈೋೋೋೋಈಏಏೋೋೋೋೋಈೋೋಣಖ6ಏಈಏೋಏಏೋೋೋ5ಈ5ೋಈೋಐ5ಃಣೋಣೋೋಃಣಣಖಣಈೋಌೋೋಖಣೋಖೋಖಌೋಣೋಣೋೋೋೋೋೋೋೋೋಌೋಖಓಌಖಚಣಖಣಏ೬ೠಣ6ೂಣ++ಈೋಈಣಣಈಣಣಖಣಹಖಈಣಣుಣಣಈుಃ\n",
            "Evaluation translation (should we go to the mall?) : ('ಈ<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 100 : 3.5249667167663574\n",
            "English: she ate it.\n",
            "Kannada Translation: ಅವಳು ಅವನಿಗೆ ಊಟ ಹಾಕಿದಳೂ.\n",
            "Kannada Prediction: ಅದ್್ ್  ್    ್್ಿ \n",
            "Evaluation translation (should we go to the mall?) : ('ಅರ್                    ು ುು<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 200 : 3.2528533935546875\n",
            "English: caste and religion were unknown.\n",
            "Kannada Translation: ಜಾತಿ, ಬೇಧ ಎಂಬುದೇ ಗೊತ್ತಿರಲಿಲ್ಲ.\n",
            "Kannada Prediction: ಆನರಿ   ಿ ್ ್ ಿ ು ಲ್ಿ್ ್ ಿಿ ್ ಿ\n",
            "Evaluation translation (should we go to the mall?) : ('ಇದು   ನ ನಿ ನ ನಿ ನು ನಿ ನಿ ನಿ ನಿ ಲಿ ನಿದಿದಿದಿದಿದಿದಿದಿದಿದಿದಿದ.<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 300 : 2.98008394241333\n",
            "English: seeing this, ruler was elated and told his son that the strength of the rabbit is due to the valour of the region's citizenry.\n",
            "Kannada Translation: ಇದನ್ನು ನೋಡಿ, ಆಡಳಿತಗಾರನು ಉತ್ಸಾಹದಿಂದ ಮತ್ತು ಮೊಲದ ಬಲವು ಪ್ರದೇಶದ ನಾಗರಿಕರ ಶೌರ್ಯದ ಕಾರಣ ಎಂದು ತನ್ನ ಮಗನಿಗೆ ತಿಳಿಸಿದನು.\n",
            "Kannada Prediction: ಹದ್್ರ  ಕ್ ಿ  ಹ ಿ್ ್್ರು್ ಸ ್ ್    ದ್ಸಾ್ ್ ಸು ್ುಅು್ು ಕು ುು ್್ಮ್ ೆಾ ುುಕು ು  ್ಸ್ರ  ಸ ದು ಸ್ುದುಅಾೆಾ ಿ ಸ್ ್ದ್ದುು \n",
            "Evaluation translation (should we go to the mall?) : ('ಇದು ಮ ಮಾರು ಮಾರ್ತ್ತ್ತ್ತ್ಲ್ದು ಮು.<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 400 : 2.9094772338867188\n",
            "English: i also had such a feeling.\n",
            "Kannada Translation: ನನಗಂತೂ ಅಂಥ ಅನುಭೂತಿಯೇ ಆಗಿದ್ದು.\n",
            "Kannada Prediction: ಈಾುುದಿ ಮದದಿಸಲು ಿ ು ೆಲಮಲುದುಲು \n",
            "Evaluation translation (should we go to the mall?) : ('ಇದು ಮ ಮಾರು ಮಾಗಿ ಮಾಗಿದು ಮಾರು ಮು.<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 500 : 2.9593262672424316\n",
            "English: what if its too late?\n",
            "Kannada Translation: ದೀರ್ಘಕಾಲ ಇದ್ದರೆ ನಾಟ್ ಏನಾಗುತ್ತದೆ?\n",
            "Kannada Prediction: ಅರದದಿ  ಿರ್ಪ ುನ ು ಸ್ರಿಲಅದುದೆ.್ತ್ೆ.\n",
            "Evaluation translation (should we go to the mall?) : ('ಅದರ್ನ ಅವಿ ಅದಿ ಅದ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ದ್ದೆ.<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 600 : 2.702836275100708\n",
            "English: i am happy that our principals and teachers are enthusiastically participating in this campaign to implement the national education policy.\n",
            "Kannada Translation: ರಾಷ್ಟ್ರೀಯ ಶಿಕ್ಷಣ ನೀತಿಯನ್ನು ಜಾರಿಗೆ ತರುವ ಈ ಅಭಿಯಾನದಲ್ಲಿ ನಮ್ಮ ಪ್ರಾಂಶುಪಾಲರು ಮತ್ತು ಶಿಕ್ಷಕರು ಉತ್ಸಾಹದಿಂದ ಭಾಗವಹಿಸುತ್ತಿರುವುದು ನನಗೆ ಸಂತೋಷವಾಗಿದೆ.\n",
            "Kannada Prediction: ನ್ವ್ರ್  ರ ಮ್ದ್ರ್ ನ  ್  ್ಲ್ ಸ್ರ್ ಿ ಕ್್ ರಬ ಮದಿ  ಗ್ು್ರಿ ಮ್ಿದ ನ್ತ್ಗದಿ ್ಗಿ್ ಮು್ ್ ಮಿ ್ರುಾಿ ಮದಿರ್ಗ ೆ ದ ಸ ರಿಾಿ ಿ ್ದ್ ್ ಿ   ನ್ಿಿ ನಿದ್ದಿುಗಿ ಿ.\n",
            "Evaluation translation (should we go to the mall?) : ('ಆದರು ನಿದು ಸ್ರು ಮಾರು ನು ನು ನು.<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 700 : 2.779871940612793\n",
            "English: this will cause heartburn.\n",
            "Kannada Translation: ಇದು ಎದೆಯುರಿಗೆ ಕಾರಣವಾಗುತ್ತದೆ.\n",
            "Kannada Prediction: ಅದರ ನಂ      ಿ ಸ್ಗಿ ಿಗಳ ್ಲ್ು.\n",
            "Evaluation translation (should we go to the mall?) : ('ಅದರು ಅದ್ರು ಅವಿದು ಅವಾಗಳಿದು ಅಡಿದೆ.<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 800 : 2.6938602924346924\n",
            "English: the government is sinking into debt.\n",
            "Kannada Translation: ಸಾಲದ ಬಲೆಗೆ ಸರ್ಕಾರ ಸಿಕ್ಕಿಕೊಂಡಿದೆ.\n",
            "Kannada Prediction: ಆಿರ್್ಮ್್ ಿ ಮ್್ತ್ಗುಮಿಗ್ತ್ ೆಂದಿಗೆ.\n",
            "Evaluation translation (should we go to the mall?) : ('ಅದರು ಮಾರು ಮಾರ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ.<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 900 : 2.6917147636413574\n",
            "English: rajeshwari devi and others participated in this lively programme.\n",
            "Kannada Translation: ರಾಜೇಶ್ವರಿ ದೇವಿ ಮೊದಲಾದವರು ಉಪಸ್ಥಿತರಿದ್ದರು.\n",
            "Kannada Prediction: ಇೀರ್ ್  ಿ ಸ್ಕರದಸಾಂ್್ಗ  ಿ ಮದ್್ತ್ ್್ಸ್ಯು  \n",
            "Evaluation translation (should we go to the mall?) : ('ಅವರ್ಯ ಅವಾರು ಅವಾರು ಅವಾಗಳು ಮಾರುತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ದೆ.<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 1000 : 2.631793737411499\n",
            "English: the virus can be passed on through contact with contaminated surfaces.\n",
            "Kannada Translation: ಕಲುಷಿತ ಮೇಲ್ಮೈಗಳ ಮೂಲಕ ವೈರಸ್ ಹರಡಬಹುದು.\n",
            "Kannada Prediction: ಅೊ್ ್ ಿಸಾವ್ಲ  ಳುಮಾ ್ೆಸಾನಿ್ತಸಿಿಿಾೆ ೆ \n",
            "Evaluation translation (should we go to the mall?) : ('ಅವರ್ಯ ಅವಿದ ಸ್ತ್ತ್ತ್ಲ್ಲ್ಲ್ಲ.<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 1100 : 2.68070387840271\n",
            "English: \"\"\"i don't know too much more.\"\n",
            "Kannada Translation: \"\"\"ನನಗೆ ಹೆಚ್ಚೇನು ತಿಳಿದಿಲ್ಲ.\"\n",
            "Kannada Prediction: ಆ\"\" ುಿೆ ಕಿ ಿರಿ ು ಮ್ತಿಸೆ ಿಲ್\n",
            "Evaluation translation (should we go to the mall?) : ('ಇದರೆ ಮಾರಿದ ಮಾರಿದು ಮಾರಿದೆ.<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 1200 : 2.6818392276763916\n",
            "English: fake journalist arrested\n",
            "Kannada Translation: ಗಾಂಜಾ ಮಾರುತ್ತಿದ್ದ ನಕಲಿ ಪತ್ರಕರ್ತ ಸೆರೆ\n",
            "Kannada Prediction: ಅೆರದಿರಪಾರು ್ ್ ರತುಮ್್್ ಬ್್ರೆ್ುತ್ನ್ರು.                                                                                                                                                ದ                  \n",
            "Evaluation translation (should we go to the mall?) : ('ಅವರ್ಯ ಅವರಿ ಅವಿರು ಅವಿಸ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ದೆ.<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 1300 : 2.634042978286743\n",
            "English: here are two such images.\n",
            "Kannada Translation: ಅಲ್ಲದೆ ಇಲ್ಲಿ ಎರಡು ಅಷ್ಟಧಾತು ಚಿತ್ರಗಳಿವೆ.\n",
            "Kannada Prediction: ಇದ್ನ್್ ಕಂ್ನ್ ಕಂಿಿ ಮಡ್ತ್್ಗ್ ಕಿರ್ತುಿುದು.\n",
            "Evaluation translation (should we go to the mall?) : ('ಇದರ್ಯ ಕ್ರಿಯ ಕ್ರಿಯ ಕ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ದೆ.<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 1400 : 2.5321199893951416\n",
            "English: ps chirmiri\n",
            "Kannada Translation: ಪಿಎಸ್ ಚಾರ್ಮಿರಿ\n",
            "Kannada Prediction: ಅ್ಕಂ್ಯಮಾರಿತಾ ಿದ                       ತ                                                                              ದ                                                    ದ ದ      ದದದದ ದ    ದ          \n",
            "Evaluation translation (should we go to the mall?) : ('ಅವರೆ ಅದು ಕಾರಿಯಾಗಿದೆ.<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 1500 : 2.5633275508880615\n",
            "English: there are no answers to questions like these.\n",
            "Kannada Translation: ಇತ್ಯಾದಿ ಪ್ರಶ್ನೆಗಳಿಗೆ ಉತ್ತರಗಳು ಲಭಿಸುವುದಿಲ್ಲ.\n",
            "Kannada Prediction: ಇದ್ಯ ಗ  ಕ್ರಿ್ ು ಳು ಳ ಸದ್ಯಿುಳು ಸ್ಾ ಿ ು.ೆದ.ಲ್\n",
            "Evaluation translation (should we go to the mall?) : ('ಇದರೆ ಸ್ರು ಸ್ತ್ತು ಸ್ತ್ತು ಸಿತ್ತ್ತಿದೆ.<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 1600 : 2.6009089946746826\n",
            "English: the burden of the desert of the sea. as whirlwinds in the south pass through. so it cometh from the desert, from a terrible land.\n",
            "Kannada Translation: ಸಮುದ್ರದ ಅಡವಿಯ ವಿಷಯವಾದ ದೈವೋಕ್ತಿ ದಕ್ಷಿಣ ಸೀಮೆಯಲ್ಲಿ ಬೀಸುವ ಬಿರುಗಾಳಿ ದಾಟಿಹೋಗುವಂತೆ ಅದು ಮರುಭೂಮಿ ಕಡೆಯಿಂದ ಭಯಂಕರವಾದ ದೇಶದಿಂದ ಬರುತ್ತದೆ.\n",
            "Kannada Prediction: ಇಾ್ಂ  ಿ ವವ್ಿ  ಪಿರ್ಾನಗ ಪ್ರಿತ್ತ್ ಮ್್ತಾ  ಮ್ರಾ  ್ಲ್ ಕೇರಿ ನಸಾಲ್ ಿಗು ಅ್ರ್ಸಿಗಿ  ದ್ ಪಂರ ಸತಿ ಾ ಾಸಪ್ಿ ಾಸದುಪಾಾದ್ುಾರೆಮಿಳ್ಿಸದುಅಿಿ ್ತ್್.\n",
            "Evaluation translation (should we go to the mall?) : ('ಇದರೆ ಅವಿಕ್ತಿ ಅವಿಸ್ತಿಸಿಸಿಸಿಸಿಸಿಸ್ತ್ತಿದೆ.<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 1700 : 2.4795072078704834\n",
            "English: how had that happened?\n",
            "Kannada Translation: ಹೇಗೆ - ಇದು ಹೇಗೆ ಸಂಭವಿಸಿತು?\n",
            "Kannada Prediction: ಇಾವೆ ನರನದು ಮಾವಿ ಸ್ದಾಿದಿದ್ \n",
            "Evaluation translation (should we go to the mall?) : ('ಆದರೆ ನ್ರು ನಾರಿಯ ನ್ರಿಯಾಗಿದೆ.<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 1800 : 2.4348056316375732\n",
            "English: this has become very popular among teens\n",
            "Kannada Translation: ಯುವಜನತೆಗೆ ಈ ಸ್ಥಳವು ಅತ್ಯಂತ ಆಕರ್ಷಣೀಯವಾದ ಸ್ಥಳವಾಗಿ ಜನಪ್ರಿಯವಾಗಿದೆ\n",
            "Kannada Prediction: ಅಾವಾ್್್ ಳ ಅ ಅಂರಿ ಾ ಅವ್ರಾದ್ಸರ್ಿಯಿಿ  ುಗರನ್ರಿುಾಗಿದಅಿಿ್ರುಸಾುರಿದೆ\n",
            "Evaluation translation (should we go to the mall?) : ('ಅವರ್ಯಾರ ಅವಿಯಾಗಿ ಅವಿಯಾಗಿದ್ದೆ<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 1900 : 2.539259195327759\n",
            "English: the findings showed that taking a nap during the day was associated with an average 5 mm hg drop in blood pressure.\n",
            "Kannada Translation: ಆ ದಿನಗಳಲ್ಲಿ ಎನ್ಎಪಿ ತೆಗೆದುಕೊಳ್ಳುವಿಕೆಯು ರಕ್ತದೊತ್ತಡದಲ್ಲಿ ಸರಾಸರಿ 5 ಎಂಎಂ ಎಚ್ಜಿ ಡ್ರಾಪ್ಗೆ ಸಂಬಂಧಿಸಿದೆ ಎಂದು ಸಂಶೋಧನೆಗಳು ತೋರಿಸಿಕೊಟ್ಟವು.\n",
            "Kannada Prediction: ಇದಸರರ್ಳಿ್ಲಿ ಸಲಿಯಂ್ ಸಾಲೆ   ೊಂುನು   ್   ಸಾ್ರ್ ಂ್ನ್ೆ್್ಲಿ ಸ್್ರ್ು ಪ0ನಂದಂದಸಂ್ರಿದನಿರಿರ್ತೆ ನ್ದಿದಿದಿದ್.ಸಂದು ಸ್ದಿಗಿ್ ೆು ಸ್ರಿದಿದ್ಂ್ ್ಾ \n",
            "Evaluation translation (should we go to the mall?) : ('ಇದರೆ ಅವಿಯ ಸ್ರಿಯ ಸ್ರಿಸಿಸಿಸಿಸಿದೆ?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 2000 : 2.523153305053711\n",
            "English: principal of the college dr\n",
            "Kannada Translation: ಕಾಲೇಜಿನ ಪ್ರಿನ್ಸಿಪಾಲ ಡಾ.\n",
            "Kannada Prediction: ಇಾರ್ ್ ್ಸ್ರಾಕುನಿ ್ಗಿಸೆಡ\n",
            "Evaluation translation (should we go to the mall?) : ('ಇದು ಸ್ರು ಸಾವು ಸಾರಿಸಿದ್ದೆ?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 2100 : 2.4337406158447266\n",
            "English: apply it on inflamed and rashy areas.\n",
            "Kannada Translation: ಮೊಡವೆ ಹಾಗೂ ಮೊಡವೆಯ ಕಲೆಗಳು ಇರುವ ಜಾಗಕ್ಕೆ ಇದನ್ನು ಹಚ್ಚಿಕೊಳ್ಳಿ.\n",
            "Kannada Prediction: ಅಾಲ್ು ಸಾಜಿ ಸಾಲ್ಾ  ಪ್್ ಳು ಮದ್ ಿಮಿರಿ್ತೆ ಮಲ್್ನು ಕಾಿರಿದ್ಂಿತಿದ\n",
            "Evaluation translation (should we go to the mall?) : ('ಇದು ಅದು ಅವರಿಗೆ ಮಾಡಿದ್ದೆ.<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 2200 : 2.4992856979370117\n",
            "English: some common terms\n",
            "Kannada Translation: ಕೆಲವು ಸಾಮಾನ್ಯ ಪದಗಳನ್ನು\n",
            "Kannada Prediction: ಅಾೕ್  ಮಾರಾರ್ನ ಸ್್ಳು್ನು                                                                                                   ದ                                                  ದ                           \n",
            "Evaluation translation (should we go to the mall?) : ('ಇದು ಅವರು ಅವರು ಅವರು?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 2300 : 2.4588217735290527\n",
            "English: the used water should be recycled.\n",
            "Kannada Translation: ಆ ಮೂಲಕ ಹರಿದು ಹೋಗುವ ನೀರು ಸದ್ಬಳಕೆ ಮಾಡಿಕೊಳ್ಳಬೇಕು.\n",
            "Kannada Prediction: ಇದಸಾಂ್್ಅಾಾರಲ ಮಾಗ್ ಾಇಾರು ನಾ್ಯೇಿ್.ಕಾಗಿದ್ಳ್ದುೇಕೆ.\n",
            "Evaluation translation (should we go to the mall?) : ('ಇದು ಮಾವು ಮಾವಾಗಿದೆ?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 2400 : 2.483766555786133\n",
            "English: thwarted love?\n",
            "Kannada Translation: ಬ್ಲೈಂಡ್ ಲವ್?\n",
            "Kannada Prediction: ಅೆರಿ ದಿ ಅಾುಯ\n",
            "Evaluation translation (should we go to the mall?) : ('ಆದರೆ ಅವರಿ ಅವರಿಯಾಗಿ ಅವರಿದೆ?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 2500 : 2.359156608581543\n",
            "English: not possible\n",
            "Kannada Translation: ಸಾಧ್ಯವೇ ಇಲ್ಲ\n",
            "Kannada Prediction: ಕ್ವಿತಾೆ ಸದ್ಲಿ                                                                                                                                                ದ                                ದ         \n",
            "Evaluation translation (should we go to the mall?) : ('ಇದು ಸ್ರತ್ತು ಸಾಡಿದ್ದು?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 2600 : 2.3035154342651367\n",
            "English: beware of the water: 17 killed by lightning in bihar, flood-like situation in several parts of the state\n",
            "Kannada Translation: ಬಿಹಾರದಲ್ಲಿ ಸಿಡಿಲು ಬಡಿದು 17 ಮಂದಿ ಸಾವು. ರಾಜ್ಯದ ಹಲವೆಡೆ ಪ್ರವಾಹ ಪರಿಸ್ಥಿತಿ\n",
            "Kannada Prediction: ಆಿದಾನಿ ್ಲಿ ಮಂನಿಗ್ ಮಂಿಸ್ ಸ00ಸಾದ  ಸಂರಿ \n",
            "Evaluation translation (should we go to the mall?) : ('ಆದರೆ ಅದು ಸಾವು ಸಾವುದು?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 2700 : 2.3165667057037354\n",
            "English: the actor in her next movie would be seen alongside rajkummar rao.\n",
            "Kannada Translation: ತಮ್ಮ ಮುಂದಿನ ಚಿತ್ರದಲ್ಲಿ ನಟ ಧ್ಯಾನ್ ಗೆ ಜೊತೆಯಾಗಲಿದ್ದಾರೆ ರಮ್ಯಾ.\n",
            "Kannada Prediction: ಈಿ್ತ ಮತಂದಲ ್ಸಿಂ್ತ  ್ಲಿ ಸಡ್ಪಾಯಕಗೆನಮಾಚಸಾತ್ ಲಗಿ್ ್ದರರೆ.ಪಿಾಯಾಗ\n",
            "Evaluation translation (should we go to the mall?) : ('ಆದರೆ ಅವರ ಸಾರಿಯ ಮಾಡಿದು?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 2800 : 2.3491570949554443\n",
            "English: you sleep.\n",
            "Kannada Translation: ನೀವು ನಿದ್ದೆ ಮಾಡುತ್ತಿದ್ದೀರಿ.\n",
            "Kannada Prediction: ಅಾನು ಅಾನುದು.ಹಾಡಿತ್ತಿದೆದೆರೆದ\n",
            "Evaluation translation (should we go to the mall?) : ('ಇದು ಬಗ್ತು ನಿನ್ನು ನಿನ್ನು ಮಾಡಿದ್ದು?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 2900 : 2.191582441329956\n",
            "English: however, jehovah assured moses: i shall prove to be with you.\n",
            "Kannada Translation: ತಾನು ಇದಕ್ಕೆ ಅರ್ಹನಾಗಿದ್ದೇನೆಂದು ಅವನಿಗೆ ಅನಿಸಲಿಲ್ಲ, ಮತ್ತು ಅವನ ಸ್ವಂತ ಸಾಮರ್ಥ್ಯದಲ್ಲಿ ಅವನು ಅದನ್ನು ಮಾಡಲು ಅರ್ಹನಾಗಿರಲಿಲ್ಲ.\n",
            "Kannada Prediction: ಅಮರ್ ಮಲು್ಕೆ ನವ್ತಾು ಿ ೆದ  ು ದು ಮವರ್ನಳ ನವ್ಯಿುದ್ಲಿ ಮಾ್ರಿ ಮವರ್ಮಂಯರದ್ನಂರಿಿತಿತ ೆ್ಲಿ ಅವರು ಮವು್ನು ಮಾಡಿ್ತಹವಿಯಿುಗೆದು್ ್ಲಿ\n",
            "Evaluation translation (should we go to the mall?) : ('ಅದು ಅವರು ಅವರು ಮಾಡಿದ್ದು?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 3000 : 2.293158531188965\n",
            "English: then there was pryce hughes, * who later became the presiding minister of the london branch office of jehovahs witnesses.\n",
            "Kannada Translation: ಅವರು ನಮ್ಮ ಕುಟುಂಬದೊಂದಿಗೆ ಸಂಪರ್ಕವನ್ನಿಟ್ಟುಕೊಂಡರು.\n",
            "Kannada Prediction: ಇವರು ಮಾ್ಮಾಮುಂ್ ದಾ ಂದ  ೆ ಮಂದ್ು ೆಾ್ನುನ್ ು ್ಂದಿುವ\n",
            "Evaluation translation (should we go to the mall?) : ('ಆದರೆ ಅವರು ಮಾಡಿದ ಮಾಡಿದ್ದೇನೆ ಮಾಡಿದೆ?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 3100 : 2.273808717727661\n",
            "English: what happened at the meeting?\n",
            "Kannada Translation: ಸಭೆಯಲ್ಲಿ ನಡೆದಿದ್ದೇನು.\n",
            "Kannada Prediction: ಅಂ್ೕ್್ಲಿ ಅಿು ್ ್ದೇ ್ \n",
            "Evaluation translation (should we go to the mall?) : ('ಅವರು ಅವರು ಸಾವುದು ಸಾಡಿದ್ದಾರೆ?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 3200 : 2.2549612522125244\n",
            "English: thousands of devotees attended the function.\n",
            "Kannada Translation: ಸಾವಿರಾರು ಭಕ್ತರು ಹಾಜರಿದ್ದು ಪೂಜೆ ಸಲ್ಲಿಸಿದರು.\n",
            "Kannada Prediction: ಅಂವು ಿ ಿ ಸಾ್ಷುಿ ನಿರ್್ಸ ದಾ ಸ್ರ್ಯಬ್್ಲ.ಸಿದ್ು.\n",
            "Evaluation translation (should we go to the mall?) : ('ಆದರೆ ಅವರು ಹೇಳಿಗೆ ಹೇಳಿದ್ದಾರೆ?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 3300 : 2.291490077972412\n",
            "English: its just as easy to use.\n",
            "Kannada Translation: ಅದನ್ನು ಬಳಸಲು ತುಂಬಾ ಸುಲಭ.\n",
            "Kannada Prediction: ಇವುುನು ಅಿಿ್್ ಹಿಂದರರಹಾಂ್ಿ\n",
            "Evaluation translation (should we go to the mall?) : ('ಆದರೆ ಅವರು ನಿಮಾವುದು?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 3400 : 2.251682996749878\n",
            "English: paris is undoubtedly the fashion capital of the world.\n",
            "Kannada Translation: ಪ್ಯಾರಿಸ್ ವಿಶ್ವ ಫ್ಯಾಷನ್ ರಾಜಧಾನಿಯಾಗಿದೆ.\n",
            "Kannada Prediction: ಆ್ರಾರಿ ್ಥಸಿರ್ಚರಕಾಯಾರ್್ ಸಾರ್್ರೆ ಲಗಿ ೆ \n",
            "Evaluation translation (should we go to the mall?) : ('ಆದರೆ ಅವರು ಮಾವುದ ಮಾಡುತ್ತಿರುತ್ತಿದೆ?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 3500 : 2.307594060897827\n",
            "English: sanjeev kumar balyan - minister of state in the ministry of mal husbandry, dairying and fisheries.\n",
            "Kannada Translation: ಸಂಜೀವ್ ಕುಮಾರ್ ಬಲ್ಯಾನ್- ಜಾನುವಾರು ಮೇವು, ಹೈನುಗಾರಿಕೆ ಮತ್ತು ಮೀನುಗಾರಿಕೆ ಸಹಾಯಕ ಸಚಿವ.\n",
            "Kannada Prediction: ಕ್ದ್ರು ವಾರಾರಿ ಮಿ್ಲಾ    ನಾರ್ ಾಗ್ ಸಾರಾ  ಸಾದ್ ಳಗು ್ ಸು್ತು ಕತಡ್ ೆಗ್ದ  ಸಾೋರಿ್ಸಾಿಸಾ\n",
            "Evaluation translation (should we go to the mall?) : ('ಆದರೆ ಅವರು ನೀಡಿದ್ದಾರೆ ಹೇಳಿದು?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 3600 : 2.193603515625\n",
            "English: dont overeat\n",
            "Kannada Translation: ಅತೀಯಾಗಿ ಸೇವಿಸಬೇಡಿ\n",
            "Kannada Prediction: ಅದ್ರ ರಿ ಬಾಕಾದಿೇಕುದ                     ,                                                            ಗ                  ದ           ದ                                                                    \n",
            "Evaluation translation (should we go to the mall?) : ('ಅವರು ಅವರಿಗೆ ಸಾವುದು ಸಾವುದು?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 3700 : 2.189966917037964\n",
            "English: do i really listen when you express yourself, or do i impulsively respond before you are finished speaking?\n",
            "Kannada Translation: ನನ್ನ ಮಾತುಗಳು ನಿನಗೆ ಭಾವಶೂನ್ಯವಾಗಿ ಅಥವಾ ಕೋಪಗೊಂಡಿರುವಂತೆ ಧ್ವನಿಸುತ್ತವೊ?\n",
            "Kannada Prediction: ಆಮ್ನುನನವ್ ೆನ ಸಿರ್ಳ ಮಾರು್ ್ನ ಾಗಿ ಸವ್ುರಸ್ರ್್ಂಡುದುವುದ್ ಸನರರುಸಿವ್ತು ಂ\n",
            "Evaluation translation (should we go to the mall?) : ('ಅವರು ಅವರು ಮಾಡುವುದು ಮಾಡುತ್ತಿದೆ?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 3800 : 2.1690754890441895\n",
            "English: there's no difference between the two.\n",
            "Kannada Translation: ಇಬ್ಬರಿಗೂ ಂುುಾವ ವ್ಯತ್ಯಾಸವೂ ಇಲ್ಲ.\n",
            "Kannada Prediction: ಆದ್ಬರಿ ೆ ಮದರ ರುಸಿಯಾಿತವಗಿು ಬಲ್ದ.\n",
            "Evaluation translation (should we go to the mall?) : ('ಆದರೆ ಅವರು ಮಾಡುವುದು ಮಾಡುತ್ತಿದೆ?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 3900 : 2.251694440841675\n",
            "English: reforms in atomic energy sector\n",
            "Kannada Translation: ಪರಮಾಣು ಇಂಧನ ವಲಯದಲ್ಲಿ ಸುಧಾರಣೆಗಳು\n",
            "Kannada Prediction: ಕಾ್ಾಜ್ ಮಲಗಿಿಮಿ್ ಲ್ಲಿ ಮ್ಕ್ರ್ೆ\n",
            "Evaluation translation (should we go to the mall?) : ('ಆದರೆ ಅವರ್ಯಾಗಿ ಮಾಡಿದು ಮಾಡಿದು ಮಾಡಿದು?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 4000 : 2.2423675060272217\n",
            "English: you got questions?\n",
            "Kannada Translation: ಪ್ರಶ್ನೆಗಳಿವೆಯೆ?\n",
            "Kannada Prediction: ಅ್ರಾ್ಟು ೆನಲೆ ಾೕ\n",
            "Evaluation translation (should we go to the mall?) : ('ಅವರು ಅವರು ಮಾಡುವುದು ಮಾಡುತ್ತಿದೆ?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 4100 : 2.1941637992858887\n",
            "English: directed by zoya akhtar.\n",
            "Kannada Translation: ಝೋಯಾ ಅಖ್ತರ್ ನಿರ್ದೇಶನದಲ್ಲಿ ಮೂಡಿಬಂದಿದೆ.\n",
            "Kannada Prediction: ಇಾವಾನಸದ್ಯಿ ತಕಾರ್ಮು  ್್್ಲಿ ಮಾಡಿದೇದುದೆ.\n",
            "Evaluation translation (should we go to the mall?) : ('ಆದರೆ ನೀವು ಮಾಡುವುದು ಹೇಳಿದು?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 4200 : 2.1624815464019775\n",
            "English: \"\"\"leaders of both parties of coalition will hold a discussion about the loss of jd(s)-congress coalition candidates.\"\n",
            "Kannada Translation: ಹಾಗೆಯೇ ಜೆಡಿಎಸ್-ಕಾಂಗ್ರೆಸ್ ಮೈತ್ರಿ ಅಭ್ಯರ್ಥಿಗಳ ಸೋಲಿನ ಬಗ್ಗೆ ಎರಡು ಪಕ್ಷಗಳ ನಾಯಕರು ಚರ್ಚೆ ನಡೆಸುತ್ತೇವೆ.\n",
            "Kannada Prediction: ಇಾಗಾ ಲ ಸಾನಿದಂ್  ್ರದ್ರ  ್ ಮತದ್ತಿಯಮವಿಯಾಿ ಿಸೆುಮಿಪ್ ್ಮಂ್ ೆ ಸಲ್ುವಸ್್ಷಿಳುಮಿಡಾ್ು ಕಿ್ಚಿಯಮಿಿಯಿವ್ತ್ ೆ.\n",
            "Evaluation translation (should we go to the mall?) : ('ಆದರೆ ನೀವು ಮಾಡುವುದು?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 4300 : 2.147348403930664\n",
            "English: the cast of the movie includes ramesh bhat, ramakrishna, sharat lohitashwa, sanket kashi, ashwath neenasam, ravichetan, nagini bharana, veena sundar, aruna balraj, mamata rahut and others.\n",
            "Kannada Translation: ರಮೇಶ್ ಅರವಿಂದ್ ಮೋನಾ ಪರ್ವೇಶ್, ಸನಾತನಿ, ರಾಜುತಾಳಿಕೋಟೆ, ಎಂ. ಎಸ್. ಉಮೇಶ್, ಅಚ್ಯುತಕುಮಾರ್, ರಾಜೇಂದ್ರಕಾರಂತ್ ಮುಂತಾದವರು ಈ ಚಿತ್ರದ ತಾರಾಬಳಗದಲ್ಲಿದ್ದಾರೆ.\n",
            "Kannada Prediction: ಅಾ್ಂದ ಮವ್ರಕದ ಯಮತದ್ನಮ್್ರಾಶ   ಮಂ್ನ್್  ಮಾಜ್ ್ನ  ್ರ್  ಮಂದ ಮಂ್  ಮತಾಲ್  ಮಭ್ಯಾ ್್ ಾರ   ಹಾರನ ತ ರ ೆರ ಗ್ರಮಾಖದ್ನ ರು ಮ ಮಿಶ್ರ  ಮಿರ್ಗಲಿಳೆ್ಲಿ ್ದರರೆ.\n",
            "Evaluation translation (should we go to the mall?) : ('ನಾನು ಹೆಚ್ಚು ಹೆಚ್ಚು ಹೇಗೆ ಹೇಗೆ ಹೇಳಿದು?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 4400 : 2.1837193965911865\n",
            "English: write poetry?\n",
            "Kannada Translation: ಕವನ ಬರೆಯುವುದು?\n",
            "Kannada Prediction: ನೆಿ್ಮೇುಯನ ುದು?\n",
            "Evaluation translation (should we go to the mall?) : ('ಅವರು ಅವರು ಮಾಡಿದ ಮಾಡಿದ್ದೇನು?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 4500 : 2.0494847297668457\n",
            "English: two persons were seriously injured in the incident.\n",
            "Kannada Translation: ಘಟನೆಯಲ್ಲಿ ಇಬ್ಬರಿಗೆ ಗಂಭೀರ ಗಾಯಗಳಾಗಿದೆ\n",
            "Kannada Prediction: ಈಲನ್  ್ಲಿ ನದ್ಬರಿ ೆ ಸಾದಾಯ್ಸಾರಕಿುಗಿದ್.ಗ  ಗಕ      ದ            ದ ಗದದಗದಗ ದಗ ಗ ದಪ    ದ    ಗ ದ  ದದದದಗ   ಗದದದದ     ಗ ದ ದ ದ ದದ  ದ   ದ ದ ದದದದದದ  ಗ ದದದದದ    ದದದ  ಗ ದಗ   ಗಗದದ   ದ ಷ ದದದದ   ಗಗ  ದದ  ಗದಗದದ ದ     ಗ  \n",
            "Evaluation translation (should we go to the mall?) : ('ಅವರು ಅವರು ಮಾಡುತ್ತಿದೆ?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 4600 : 2.141190767288208\n",
            "English: slogans were raised against the government.\n",
            "Kannada Translation: ಸರ್ಕಾರದ ವಿರುದ್ಧ ಘೋಷಣೆ ಕೂಗಿತು.\n",
            "Kannada Prediction: ಈ್ಿಕಾರಿ ಸಿಶ್  ದ ಸಾಗ್ೆಯಸಾಡೆದ್ \n",
            "Evaluation translation (should we go to the mall?) : ('ನಾನು ಮಾಡಿದ ಹೇಗೆ ಹೇಗೆ?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 4700 : 2.1196253299713135\n",
            "English: love taking pictures?\n",
            "Kannada Translation: ನೀವು ಭಾವಚಿತ್ರಗಳನ್ನು ತೆಗೆದುಕೊಳ್ಳಲು ಇಷ್ಟಪಡುತ್ತೀರಾ?\n",
            "Kannada Prediction: ನಾವು ನಾರುಾನ್ರ ೆು್ನು ಮೆಗೆ ು?ೊಳ್ಳಿ್ ಮದ್ಟಿಿಿತ್ತದರಿ?\n",
            "Evaluation translation (should we go to the mall?) : ('ನಾನು ನೀವು ನೀಡಿದು ನೀಡಿದು?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 4800 : 2.065659761428833\n",
            "English: plastic tumblers will be replaced with paper tumblers.\n",
            "Kannada Translation: ಕಾಗದದ ನೋಟುಗಳ ಜಾಗದಲ್ಲಿ ಬರಲಿವೆ ಪ್ಲಾಸ್ಟಿಕ್ ನೋಟುಗಳು.\n",
            "Kannada Prediction: ಈಾಂಿ್ೆಸಿಡ್ ಳನಮಿರೂಲ್ಲಿ ಸಿು್ಸೆ ಹ್ರಾಸ್ ಿ ್ಕನಡಡಿ ಳಿ \n",
            "Evaluation translation (should we go to the mall?) : ('ಅವರು ಅವರು ಮಾಡುವುದು ಹೇಳು?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 4900 : 2.1425368785858154\n",
            "English: however, she has already made her mark in bollywood by singing two songs for himesh reshammiyas upcoming bollywood film happy hardy and heer.\n",
            "Kannada Translation: ರಾನು ಮಂಡಲ್ ಹಾಡನ್ನು ಮೆಚ್ಚಿಕೊಂಡ ಬಾಲಿವುಡ್ ಗಾಯಕ ಹಿಮೇಶ್ ರೇಶಮಿಯಾ ಅವರ ಮುಂದಿನ ಚಿತ್ರ ಹ್ಯಾಪಿ ಹಾರ್ಡಿ ಅಂಡ್ ಹೀರ್ ಚಿತ್ರದಲ್ಲಿ ಎರಡು ಗೀತೆಗಳನ್ನು ಹಾಡಿಸಿದ್ದರು.\n",
            "Kannada Prediction: ನಾಜು ಮತದ ುಲಮಾಗಿ್ನು ಮತಚ್ಚಿನ್ಳದಿಸೆರ್ ುದುಡನಾರಕ್ಮಾಂಾಲ್ಯನಾವಗಾ ನಗಮವರುನಾಖದು ್ಸಿರ್ತ ನೊಯಾಗಿಸಸಾಗ್ಯಿಸಸವದಿ ನೇಗ್ಯಮಿರ್ರ  ್ಲಿ ಮಂ್ೆತಮಾಡ್ ಳನ್ನು ಸೇಡಿದಿದ್ದಾು.\n",
            "Evaluation translation (should we go to the mall?) : ('ನಾನು ನೀವು ನಿಮ್ಮ ಹೇಳಿದ್ದು ಹೇಳಿದ್ದಾರೆ?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 5000 : 2.2291808128356934\n",
            "English: as a result the market is flat.\n",
            "Kannada Translation: ಇದರ ಪರಿಣಾಮ ಮಾರುಕಟ್ಟೆ ಇನ್ನಿಲ್ಲದಂತೆ ನೆಲ ಕಚ್ಚಿದೆ.\n",
            "Kannada Prediction: ಅದು ಕ್ಿತ  ಾಸಾಡ್ ೊ್ಟಿ ಸನ್ನುಸ್ಲ.ೆತೆ ನಿಗ್ಸೆಿಚುತ್.\n",
            "Evaluation translation (should we go to the mall?) : ('ನಾನು ಹೇಗೆ ಹೇಗೆ ಹೇಗೆ ಹೇಗೆ ಹೇಗೆ?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 5100 : 1.964555025100708\n",
            "English: parents should educate their children.\n",
            "Kannada Translation: ಪಾಲಕರು ಮಕ್ಕಳು ಕನ್ನಡ ಕಲಿಯಲು ಆಸ್ತೆ ವಹಿಸಬೇಕು.\n",
            "Kannada Prediction: ಸ್ಲ್್  ಮತ್ಕಳು ಸಾೆನು ಸಾ್ಸಲ್ ಮಗ್ಕಿ ಮಿಿಚಿೇಕು.\n",
            "Evaluation translation (should we go to the mall?) : ('ನಿಮ್ಮ ಮಾಡುತ್ತಿದೆ ಮಾಡುತ್ತಿದೆ?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 5200 : 2.13004207611084\n",
            "English: rain recedes\n",
            "Kannada Translation: ಮಳೆ ಕಡಿಮೆಯಾಗುತ್ತಿದೆ\n",
            "Kannada Prediction: ಸತಿಸಸಾೆತ್ದ ಗಿವ್ಯಿರೆ\n",
            "Evaluation translation (should we go to the mall?) : ('ನಿಮ್ಮ ಮಾಡಿದ ಮಾಡಿದ್ದಾರೆ?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 5300 : 1.9301313161849976\n",
            "English: but this time its not happening.\n",
            "Kannada Translation: ಆದರೆ ಈ ಬಾರಿ ಇದಕ್ಕೆ ಅವಕಾಶ ನೀಡುವುದಿಲ್ಲ.\n",
            "Kannada Prediction: ಆದರೆ ಅ ಸದರಿತಅದು್ಕೆ ಅವರ್ರಕಸಾಡಿತುದಿಲ್ಲ.\n",
            "Evaluation translation (should we go to the mall?) : ('ಅವರು ಮಾಡಿದ್ದರೆ ಮಾಡಿದ್ದಾರೆ?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 5400 : 2.1449456214904785\n",
            "English: a case has been registered on a complaint of the woman.\n",
            "Kannada Translation: ಮಹಿಳೆ ನೀಡಿದ ದೂರನ್ನು ದಾಖಲಿಸಿಕೊಳ್ಳಲಾಗಿದೆ.\n",
            "Kannada Prediction: ನುಾಳಿಯಪಾಡಿ  ಮೇರು್ನು ಅೇಖಲ್ ಿದೊಂ್ನು್ಗಿದೆ \n",
            "Evaluation translation (should we go to the mall?) : ('ನಿಮ್ಮ ಮಾಡುವುದು ಮಾಡುತ್ತಿದ್ದಾರೆ?<END>',)\n",
            "-------------------------------------------\n",
            "Epoch 1\n",
            "Iteration 0 : 2.1877076625823975\n",
            "English: hes a scientist.\n",
            "Kannada Translation: ಇವರು ಸಂಶೋಧಕ ಸ್ವಭಾವದವರು.\n",
            "Kannada Prediction: ಅದರಿ ಕಾದ್ಷಿ್ಕ್ವಾಾಗಿ ರು.\n",
            "Evaluation translation (should we go to the mall?) : ('ನಾವು ಅವರು ಹೇಳುವುದು ಹೇಳುತ್ತಿದೆ?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 100 : 2.0301036834716797\n",
            "English: she ate it.\n",
            "Kannada Translation: ಅವಳು ಅವನಿಗೆ ಊಟ ಹಾಕಿದಳೂ.\n",
            "Kannada Prediction: ನದರು ನದರ್ಸೆ ಮಲ್ಸಾನಿ ೆು.\n",
            "Evaluation translation (should we go to the mall?) : ('ಅದಕ್ಕೆ ನಿಮ್ಮ ಮಾಡುವುದು ಹೇಳಿದ್ದಾರೆ?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 200 : 2.0526368618011475\n",
            "English: caste and religion were unknown.\n",
            "Kannada Translation: ಜಾತಿ, ಬೇಧ ಎಂಬುದೇ ಗೊತ್ತಿರಲಿಲ್ಲ.\n",
            "Kannada Prediction: ಇನವ್  ಸಿರಾಮಸದಂದು ಕಾರ್ತೆ ು್ಲ್ಲ.\n",
            "Evaluation translation (should we go to the mall?) : ('ಇದು ಹೇಳಿದ್ದರೆ ಹೇಳಿದ್ದಾರೆ?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 300 : 2.1442887783050537\n",
            "English: seeing this, ruler was elated and told his son that the strength of the rabbit is due to the valour of the region's citizenry.\n",
            "Kannada Translation: ಇದನ್ನು ನೋಡಿ, ಆಡಳಿತಗಾರನು ಉತ್ಸಾಹದಿಂದ ಮತ್ತು ಮೊಲದ ಬಲವು ಪ್ರದೇಶದ ನಾಗರಿಕರ ಶೌರ್ಯದ ಕಾರಣ ಎಂದು ತನ್ನ ಮಗನಿಗೆ ತಿಳಿಸಿದನು.\n",
            "Kannada Prediction: ಇದರ್ನು ಸಿಡುದ ಸರಿಿದ್ೆಗಿ್ ಸತ್ತ್ರಿಿಯದಿಸಾ್ತು ನ್ತವುಸಿೆು ನ್ರಮ್ ವಿಸಿವಿಿದೊಿಸಾಲ್ಯಕಲಅಾರಣವಅಂದು ಸಿ್ನುಮಾೆ್ಸೆ ಸೊಳಿಸುದರ್ \n",
            "Evaluation translation (should we go to the mall?) : ('ಅವರು ಅವರು ಮಾಡುವುದು ಹೇಳಿದ್ದು ಹೇಳಿದ್ದು ಹೇಳಿದ್ದಾರೆ?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 400 : 2.108459711074829\n",
            "English: i also had such a feeling.\n",
            "Kannada Translation: ನನಗಂತೂ ಅಂಥ ಅನುಭೂತಿಯೇ ಆಗಿದ್ದು.\n",
            "Kannada Prediction: ನಾಗೆತೆ ನವತ ನವ್ ವ ೆ ಿ ನಗಿಲೆದೇ.\n",
            "Evaluation translation (should we go to the mall?) : ('ನಾನು ಮಾಡಿದ್ದೇ ಮಾಡುತ್ತಿದೆ?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 500 : 2.186833381652832\n",
            "English: what if its too late?\n",
            "Kannada Translation: ದೀರ್ಘಕಾಲ ಇದ್ದರೆ ನಾಟ್ ಏನಾಗುತ್ತದೆ?\n",
            "Kannada Prediction: ನೇನವಿಯಾ್ಗ ಹನುದೇಿ ಏಿನುಟಹನು?ುತ್ತಿೆ?\n",
            "Evaluation translation (should we go to the mall?) : ('ನಾನು ಮಾಡಿದ ಹೆಚ್ಚು ಹೆಚ್ಚು ಹೆಚ್ಚು ಹೇಳಿದ್ದೇನೆ?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 600 : 1.8733805418014526\n",
            "English: i am happy that our principals and teachers are enthusiastically participating in this campaign to implement the national education policy.\n",
            "Kannada Translation: ರಾಷ್ಟ್ರೀಯ ಶಿಕ್ಷಣ ನೀತಿಯನ್ನು ಜಾರಿಗೆ ತರುವ ಈ ಅಭಿಯಾನದಲ್ಲಿ ನಮ್ಮ ಪ್ರಾಂಶುಪಾಲರು ಮತ್ತು ಶಿಕ್ಷಕರು ಉತ್ಸಾಹದಿಂದ ಭಾಗವಹಿಸುತ್ತಿರುವುದು ನನಗೆ ಸಂತೋಷವಾಗಿದೆ.\n",
            "Kannada Prediction: ಈಾಜ್ಯ್ ಿಯ ಸಾಕ್ಷಣ ಸಾರಿಯ ್ನು ಪನಲ್ಕಳ ಸಂ್ಖಾಸ ಮವಿವ  ್ ್ಲಿ ಪಿ್ಮ ಸ್ರಧರಗ್ ಿರ್್ ಪತ್ತು ಮಾಕ್ಷ ್್ ಮತ್ತ್ರ   ದ ಸಾರ್ಾಿಸಿತ್ತಿದು  ದು ಪಿ್ಳ ಸಂಪ್ಪ್ಾಗಿದೆ.\n",
            "Evaluation translation (should we go to the mall?) : ('ಇದು ನೀವರು ಮಾಡುವುದು ಹೇಗೆ?<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 700 : 2.004469394683838\n",
            "English: this will cause heartburn.\n",
            "Kannada Translation: ಇದು ಎದೆಯುರಿಗೆ ಕಾರಣವಾಗುತ್ತದೆ.\n",
            "Kannada Prediction: ಇದು ಸಂು   ುಗೆ ಸೊರಣ ಾಗಿತ್ತದೆ.\n",
            "Evaluation translation (should we go to the mall?) : ('ಅವರು ಮಾಡಿದ ಮಾಡುವುದು ಮಾಡುತ್ತದೆ?<END>',)\n",
            "-------------------------------------------\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[43], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask \u001b[38;5;241m=\u001b[39m create_masks(eng_batch, kn_batch)\n\u001b[1;32m     13\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 14\u001b[0m kn_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43meng_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mkn_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mencoder_self_attention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdecoder_self_attention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdecoder_cross_attention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menc_start_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menc_end_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdec_start_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdec_end_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m labels \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39msentence_embedding\u001b[38;5;241m.\u001b[39mbatch_tokenize(kn_batch, start_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, end_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterian(\n\u001b[1;32m     25\u001b[0m     kn_predictions\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, kn_vocab_size)\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     26\u001b[0m     labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     27\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[23], line 296\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, y, encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask, enc_start_token, enc_end_token, dec_start_token, dec_end_token)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m    287\u001b[0m             x, \n\u001b[1;32m    288\u001b[0m             y, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    294\u001b[0m             dec_start_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;66;03m# We should make this true\u001b[39;00m\n\u001b[1;32m    295\u001b[0m             dec_end_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m): \u001b[38;5;66;03m# x, y are batch of sentences\u001b[39;00m\n\u001b[0;32m--> 296\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_self_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menc_start_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menc_end_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(x, y, decoder_self_attention_mask, decoder_cross_attention_mask, start_token\u001b[38;5;241m=\u001b[39mdec_start_token, end_token\u001b[38;5;241m=\u001b[39mdec_end_token)\n\u001b[1;32m    298\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(out)\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[23], line 174\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x, self_attention_mask, start_token, end_token)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, self_attention_mask, start_token, end_token):\n\u001b[1;32m    173\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentence_embedding(x, start_token, end_token)\n\u001b[0;32m--> 174\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_attention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[23], line 152\u001b[0m, in \u001b[0;36mSequentialEncoder.forward\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m    150\u001b[0m x, self_attention_mask  \u001b[38;5;241m=\u001b[39m inputs\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m--> 152\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_attention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[23], line 143\u001b[0m, in \u001b[0;36mEncoderLayer.forward\u001b[0;34m(self, x, self_attention_mask)\u001b[0m\n\u001b[1;32m    141\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m residual_x)\n\u001b[1;32m    142\u001b[0m residual_x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m--> 143\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mffn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(x)\n\u001b[1;32m    145\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m residual_x)\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[23], line 122\u001b[0m, in \u001b[0;36mPositionwiseFeedForward.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    120\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1(x)\n\u001b[1;32m    121\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m--> 122\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(x)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/modules/dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/torch/nn/functional.py:1268\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "transformer.train()\n",
        "transformer.to(device)\n",
        "total_loss = 0\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch}\")\n",
        "    iterator = iter(train_loader)\n",
        "    for batch_num, batch in enumerate(iterator):\n",
        "        transformer.train()\n",
        "        eng_batch, kn_batch = batch\n",
        "        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch, kn_batch)\n",
        "        optim.zero_grad()\n",
        "        kn_predictions = transformer(eng_batch,\n",
        "                                     kn_batch,\n",
        "                                     encoder_self_attention_mask.to(device), \n",
        "                                     decoder_self_attention_mask.to(device), \n",
        "                                     decoder_cross_attention_mask.to(device),\n",
        "                                     enc_start_token=False,\n",
        "                                     enc_end_token=False,\n",
        "                                     dec_start_token=True,\n",
        "                                     dec_end_token=True)\n",
        "        labels = transformer.decoder.sentence_embedding.batch_tokenize(kn_batch, start_token=False, end_token=True)\n",
        "        loss = criterian(\n",
        "            kn_predictions.view(-1, kn_vocab_size).to(device),\n",
        "            labels.view(-1).to(device)\n",
        "        ).to(device)\n",
        "        valid_indicies = torch.where(labels.view(-1) == kannada_to_index[PADDING_TOKEN], False, True)\n",
        "        loss = loss.sum() / valid_indicies.sum()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        #train_losses.append(loss.item())\n",
        "        if batch_num % 100 == 0:\n",
        "            print(f\"Iteration {batch_num} : {loss.item()}\")\n",
        "            print(f\"English: {eng_batch[0]}\")\n",
        "            print(f\"Kannada Translation: {kn_batch[0]}\")\n",
        "            kn_sentence_predicted = torch.argmax(kn_predictions[0], axis=1)\n",
        "            predicted_sentence = \"\"\n",
        "            for idx in kn_sentence_predicted:\n",
        "              if idx == kannada_to_index[END_TOKEN]:\n",
        "                break\n",
        "              predicted_sentence += index_to_kannada[idx.item()]\n",
        "            print(f\"Kannada Prediction: {predicted_sentence}\")\n",
        "\n",
        "\n",
        "            transformer.eval()\n",
        "            kn_sentence = (\"\",)\n",
        "            eng_sentence = (\"should we go to the mall?\",)\n",
        "            for word_counter in range(max_sequence_length):\n",
        "                encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask= create_masks(eng_sentence, kn_sentence)\n",
        "                predictions = transformer(eng_sentence,\n",
        "                                          kn_sentence,\n",
        "                                          encoder_self_attention_mask.to(device), \n",
        "                                          decoder_self_attention_mask.to(device), \n",
        "                                          decoder_cross_attention_mask.to(device),\n",
        "                                          enc_start_token=False,\n",
        "                                          enc_end_token=False,\n",
        "                                          dec_start_token=True,\n",
        "                                          dec_end_token=False)\n",
        "                next_token_prob_distribution = predictions[0][word_counter] # not actual probs\n",
        "                next_token_index = torch.argmax(next_token_prob_distribution).item()\n",
        "                next_token = index_to_kannada[next_token_index]\n",
        "                kn_sentence = (kn_sentence[0] + next_token, )\n",
        "                if next_token == END_TOKEN:\n",
        "                  break\n",
        "            \n",
        "            print(f\"Evaluation translation (should we go to the mall?) : {kn_sentence}\")\n",
        "            print(\"-------------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nosVPGVijId"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOQe-juylBiJ"
      },
      "outputs": [],
      "source": [
        "transformer.eval()\n",
        "def translate(eng_sentence):\n",
        "  eng_sentence = (eng_sentence,)\n",
        "  kn_sentence = (\"\",)\n",
        "  for word_counter in range(max_sequence_length):\n",
        "    encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask= create_masks(eng_sentence, kn_sentence)\n",
        "    predictions = transformer(eng_sentence,\n",
        "                              kn_sentence,\n",
        "                              encoder_self_attention_mask.to(device), \n",
        "                              decoder_self_attention_mask.to(device), \n",
        "                              decoder_cross_attention_mask.to(device),\n",
        "                              enc_start_token=False,\n",
        "                              enc_end_token=False,\n",
        "                              dec_start_token=True,\n",
        "                              dec_end_token=False)\n",
        "    next_token_prob_distribution = predictions[0][word_counter]\n",
        "    next_token_index = torch.argmax(next_token_prob_distribution).item()\n",
        "    next_token = index_to_kannada[next_token_index]\n",
        "    kn_sentence = (kn_sentence[0] + next_token, )\n",
        "    if next_token == END_TOKEN:\n",
        "      break\n",
        "  return kn_sentence[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDVH_YsxlK6q",
        "outputId": "83c47f99-53c0-4c2d-c26a-aaa426f50563"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"what should we do when the day starts?\")\n",
        "print(translation)\n",
        "#ದಿನ ಪ್ರಾರಂಭವಾದಾಗ ನಾವು ಏನು ಮಾಡಬೇಕು?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9yfawBnul0W",
        "outputId": "d9e6e6b7-683b-45f9-f013-c53c31038306"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"how is this the truth?\")\n",
        "print(translation)\n",
        "#ಇದು ಹೇಗೆ ಸತ್ಯ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpdYBk5-urcQ",
        "outputId": "ca7249c5-efda-4f41-f052-ecef9691be82"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"the world is a large place with different people\")\n",
        "print(translation)\n",
        "#ಪ್ರಪಂಚವು ವಿಭಿನ್ನ ಜನರೊಂದಿಗೆ ದೊಡ್ಡ ಸ್ಥಳವಾಗಿದೆ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ni9e2UYUuxi3",
        "outputId": "b93968e6-3f12-4794-b277-3a3821af221e"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"my name is ajay\")\n",
        "print(translation)\n",
        "#ನನ್ನ ಹೆಸರು ಅಜಯ್"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJuJKHqFldW3",
        "outputId": "71aa2c6c-ec77-4b02-d39b-bd724012fb53"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"i cannot stand this smell\")\n",
        "print(translation)\n",
        "#ನಾನು ಈ ವಾಸನೆಯನ್ನು ಸಹಿಸುವುದಿಲ್ಲ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxHC4Lirlfu8",
        "outputId": "5a3ca401-abac-41af-d9db-99fb7a57fe00"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"noodles are the best\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLVOSI0Oli16",
        "outputId": "9f9445a1-2802-4688-900c-d7ecf2bd5c35"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"why care about this?\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MStWCoAt0Ixp"
      },
      "source": [
        "This translated pretty well : \"What is the reason. Why\" without punctuation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AB6TEJfGlkRT",
        "outputId": "adb465d5-b0ed-4be4-9251-62c079f49491"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"this is the best thing ever\")\n",
        "print(translation)\n",
        "# ಇದು ಎಂದೆಂದಿಗೂ ಉತ್ತಮವಾಗಿದೆ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxsUjSybxYkh"
      },
      "source": [
        "The translation : \"This is very unusual\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQwDbuWBlmmA",
        "outputId": "7ae0e2c0-02c0-4c74-bc47-26b67da55a06"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"i am here\")\n",
        "print(translation)\n",
        "# ನಾನು ಇಲ್ಲಿದ್ದೇನೆ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmyZ2-I6x0Yf"
      },
      "source": [
        "Translation: \"I have heard\". \n",
        "This is why word based translator may perform better than character translator. This is actually very good at optimizing the objective of the current transformer even though the translation is off."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ifeV4bGluIj",
        "outputId": "6bce922d-d0db-432c-e6c6-97d482f1dea6"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"click this\")\n",
        "print(translation)\n",
        "# ಇದನ್ನು ಕ್ಲಿಕ್ ಮಾಡಿ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RB5DUBEl1kD",
        "outputId": "9109e504-8b9e-45dc-e13c-e878b3741e4e"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"where is the mall?\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdeJ9CvMn5LM",
        "outputId": "044b5dac-29a9-4b60-e66a-4e10739a9756"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"what should we do?\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoikFnov1rj-"
      },
      "source": [
        "This is correct; but it absolutely fumbles on the next one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GFeyzrg1fIZ",
        "outputId": "2b4dfb04-def2-4725-9e76-5476bf85e2ef"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"today, what should we do\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0upygLS-sXcO",
        "outputId": "53a62435-ab16-4d4c-8a9f-0bf07af2a501"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"why did they activate?\")\n",
        "print(translation)\n",
        "# ಅವರು ಏಕೆ ಸಕ್ರಿಯಗೊಳಿಸಿದರು?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSrsqEGmtcl2",
        "outputId": "1b8b8faa-5370-426a-f2f2-fe852696bf7a"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"why did they do this?\")\n",
        "print(translation)\n",
        "# ಅವರು ಇದನ್ನು ಏಕೆ ಮಾಡಿದರು?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7ISM5rd3BLJ"
      },
      "source": [
        "That turned out well!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjTcH2HFtyld",
        "outputId": "9dea5498-f139-4cbd-ee8c-6108e1b92fc4"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"i am well.\")\n",
        "print(translation)\n",
        "# ನಾನು ಆರಾಮವಾಗಿದ್ದೇನೆ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP0YX2g74eP7"
      },
      "source": [
        "Translation: \"I will give you something\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pGdN13kt5Br",
        "outputId": "240256c5-f594-41b0-8218-2c70a22a156f"
      },
      "outputs": [],
      "source": [
        "translation = translate(\"whats the word on the street?\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFYZ6pOe4o-X"
      },
      "source": [
        "Kind of close semantically. Translation is something like: \"What is this about\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzrOcNUk1-e5"
      },
      "source": [
        "## Insights\n",
        "\n",
        "- When training, we can treat every alphabet as a single unit instead of splitting it into it's corresponding parts to preserve meaning. For example, ಮಾ should be 1 unit when comuting a loss. It should not be decomposed into ಮ + ఆ\n",
        "- Using word-based or BPE based tokenizations may help mitigate (1). Also, we will get valid word (or BPE) units if we do so. \n",
        "- Make sure the training set has a large variety of sentences that are not just about one topic like \"work\" and \"government\"\n",
        "- Increase the number of encoder / decoder units for better translations. It was set to the minimum of 1 of each unit here.\n",
        "- Create a translator with a language you understand ideally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd6_k0Uu5V7f"
      },
      "source": [
        "Overall, this model definately learned something. And you can use other languages instead of this kannada language and might see better luck"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
