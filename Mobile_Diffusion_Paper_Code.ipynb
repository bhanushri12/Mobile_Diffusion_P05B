{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### The flow of the model is as follows->\n",
        "1. We have trained the VAE with the celebface datatset and obtained the weights of Encoder and Decoder\n",
        "2. Now we have a taken the Stable Diffusion code and Optimised it according to the below MD-Lite Architecture.\n",
        "3.We are using the Clip Model to embedd the Prompts of the Images.\n",
        "4. Flow-> Image-> VAE's trained Encoder-> Z sample-> Forward Diffuison-> Training the UNET model for Denoising -> The obtained Z sample-> VAE's Trained Decoder."
      ],
      "metadata": {
        "id": "3xMerUTcrQDt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###SelfAttention previous\n",
        "![Screenshot 2024-05-09 at 2.45.31 PM.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "GcT5kISL642u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zf7jIysAKoW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The Below is the Variational Auto Encoder\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dvbT4igoMY_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing necessary libraries\n",
        "import torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "metadata": {
        "id": "rtwYST3U2XVe"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# DEVICE = \"cpu\""
      ],
      "metadata": {
        "id": "z8116YWu3BZC"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, n_heads, d_embed, in_proj_bias=True, out_proj_bias=True):\n",
        "        super().__init__()\n",
        "        # This combines the Wq, Wk and Wv matrices into one matrix\n",
        "        self.in_proj = nn.Linear(d_embed, 3 * d_embed, bias=in_proj_bias)\n",
        "        # This one represents the Wo matrix\n",
        "        self.out_proj = nn.Linear(d_embed, d_embed, bias=out_proj_bias)\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head = d_embed // n_heads\n",
        "\n",
        "    def forward(self, x, causal_mask=False):\n",
        "        # x: # (Batch_Size, Seq_Len, Dim)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim)\n",
        "        input_shape = x.shape\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim)\n",
        "        batch_size, sequence_length, d_embed = input_shape\n",
        "\n",
        "        # (Batch_Size, Seq_Len, H, Dim / H)\n",
        "        interim_shape = (batch_size, sequence_length, self.n_heads, self.d_head)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim * 3) -> 3 tensor of shape (Batch_Size, Seq_Len, Dim)\n",
        "        q, k, v = self.in_proj(x).chunk(3, dim=-1)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, H, Dim / H) -> (Batch_Size, H, Seq_Len, Dim / H)\n",
        "        q = q.view(interim_shape).transpose(1, 2)\n",
        "        k = k.view(interim_shape).transpose(1, 2)\n",
        "        v = v.view(interim_shape).transpose(1, 2)\n",
        "\n",
        "        # (Batch_Size, H, Seq_Len, Dim / H) @ (Batch_Size, H, Dim / H, Seq_Len) -> (Batch_Size, H, Seq_Len, Seq_Len)\n",
        "        weight = q @ k.transpose(-1, -2)\n",
        "\n",
        "        if causal_mask:\n",
        "            # Mask where the upper triangle (above the principal diagonal) is 1\n",
        "            mask = torch.ones_like(weight, dtype=torch.bool).triu(1)\n",
        "            # Fill the upper triangle with -inf\n",
        "            weight.masked_fill_(mask, -torch.inf)\n",
        "\n",
        "        # Divide by d_k (Dim / H).\n",
        "        # (Batch_Size, H, Seq_Len, Seq_Len) -> (Batch_Size, H, Seq_Len, Seq_Len)\n",
        "        weight /= math.sqrt(self.d_head)\n",
        "\n",
        "        # (Batch_Size, H, Seq_Len, Seq_Len) -> (Batch_Size, H, Seq_Len, Seq_Len)\n",
        "        weight = F.softmax(weight, dim=-1)\n",
        "\n",
        "        # (Batch_Size, H, Seq_Len, Seq_Len) @ (Batch_Size, H, Seq_Len, Dim / H) -> (Batch_Size, H, Seq_Len, Dim / H)\n",
        "        output = weight @ v\n",
        "\n",
        "        # (Batch_Size, H, Seq_Len, Dim / H) -> (Batch_Size, Seq_Len, H, Dim / H)\n",
        "        output = output.transpose(1, 2)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, H, Dim / H) -> (Batch_Size, Seq_Len, Dim)\n",
        "        output = output.reshape(input_shape)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n",
        "        output = self.out_proj(output)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim)\n",
        "        return output\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, n_heads, d_embed, d_cross, in_proj_bias=True, out_proj_bias=True):\n",
        "        super().__init__()\n",
        "        self.q_proj   = nn.Linear(d_embed, d_embed, bias=in_proj_bias)\n",
        "        self.k_proj   = nn.Linear(d_cross, d_embed, bias=in_proj_bias)\n",
        "        self.v_proj   = nn.Linear(d_cross, d_embed, bias=in_proj_bias)\n",
        "        self.out_proj = nn.Linear(d_embed, d_embed, bias=out_proj_bias)\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head = d_embed // n_heads\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # x (latent): # (Batch_Size, Seq_Len_Q, Dim_Q)\n",
        "        # y (context): # (Batch_Size, Seq_Len_KV, Dim_KV) = (Batch_Size, 77, 768)\n",
        "\n",
        "        input_shape = x.shape\n",
        "        batch_size, sequence_length, d_embed = input_shape\n",
        "        # Divide each embedding of Q into multiple heads such that d_heads * n_heads = Dim_Q\n",
        "        interim_shape = (batch_size, -1, self.n_heads, self.d_head)\n",
        "\n",
        "        # (Batch_Size, Seq_Len_Q, Dim_Q) -> (Batch_Size, Seq_Len_Q, Dim_Q)\n",
        "        q = self.q_proj(x)\n",
        "        # (Batch_Size, Seq_Len_KV, Dim_KV) -> (Batch_Size, Seq_Len_KV, Dim_Q)\n",
        "        k = self.k_proj(y)\n",
        "        # (Batch_Size, Seq_Len_KV, Dim_KV) -> (Batch_Size, Seq_Len_KV, Dim_Q)\n",
        "        v = self.v_proj(y)\n",
        "\n",
        "        # (Batch_Size, Seq_Len_Q, Dim_Q) -> (Batch_Size, Seq_Len_Q, H, Dim_Q / H) -> (Batch_Size, H, Seq_Len_Q, Dim_Q / H)\n",
        "        q = q.view(interim_shape).transpose(1, 2)\n",
        "        # (Batch_Size, Seq_Len_KV, Dim_Q) -> (Batch_Size, Seq_Len_KV, H, Dim_Q / H) -> (Batch_Size, H, Seq_Len_KV, Dim_Q / H)\n",
        "        k = k.view(interim_shape).transpose(1, 2)\n",
        "        # (Batch_Size, Seq_Len_KV, Dim_Q) -> (Batch_Size, Seq_Len_KV, H, Dim_Q / H) -> (Batch_Size, H, Seq_Len_KV, Dim_Q / H)\n",
        "        v = v.view(interim_shape).transpose(1, 2)\n",
        "\n",
        "        # (Batch_Size, H, Seq_Len_Q, Dim_Q / H) @ (Batch_Size, H, Dim_Q / H, Seq_Len_KV) -> (Batch_Size, H, Seq_Len_Q, Seq_Len_KV)\n",
        "        weight = q @ k.transpose(-1, -2)\n",
        "\n",
        "        # (Batch_Size, H, Seq_Len_Q, Seq_Len_KV)\n",
        "        weight /= math.sqrt(self.d_head)\n",
        "\n",
        "        # (Batch_Size, H, Seq_Len_Q, Seq_Len_KV)\n",
        "        weight = F.softmax(weight, dim=-1)\n",
        "\n",
        "        # (Batch_Size, H, Seq_Len_Q, Seq_Len_KV) @ (Batch_Size, H, Seq_Len_KV, Dim_Q / H) -> (Batch_Size, H, Seq_Len_Q, Dim_Q / H)\n",
        "        output = weight @ v\n",
        "\n",
        "        # (Batch_Size, H, Seq_Len_Q, Dim_Q / H) -> (Batch_Size, Seq_Len_Q, H, Dim_Q / H)\n",
        "        output = output.transpose(1, 2).contiguous()\n",
        "\n",
        "        # (Batch_Size, Seq_Len_Q, H, Dim_Q / H) -> (Batch_Size, Seq_Len_Q, Dim_Q)\n",
        "        output = output.view(input_shape)\n",
        "\n",
        "        # (Batch_Size, Seq_Len_Q, Dim_Q) -> (Batch_Size, Seq_Len_Q, Dim_Q)\n",
        "        output = self.out_proj(output)\n",
        "\n",
        "        # (Batch_Size, Seq_Len_Q, Dim_Q)\n",
        "        return output"
      ],
      "metadata": {
        "id": "WR7qQo1Gzw1I"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE_AttentionBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.groupnorm = nn.GroupNorm(32, channels)\n",
        "        self.attention = SelfAttention(1, channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (Batch_Size, Features, Height, Width)\n",
        "\n",
        "        residue = x\n",
        "\n",
        "        # (Batch_Size, Features, Height, Width) -> (Batch_Size, Features, Height, Width)\n",
        "        x = self.groupnorm(x)\n",
        "\n",
        "        n, c, h, w = x.shape\n",
        "\n",
        "        # (Batch_Size, Features, Height, Width) -> (Batch_Size, Features, Height * Width)\n",
        "        x = x.view((n, c, h * w))\n",
        "\n",
        "        # (Batch_Size, Features, Height * Width) -> (Batch_Size, Height * Width, Features). Each pixel becomes a feature of size \"Features\", the sequence length is \"Height * Width\".\n",
        "        x = x.transpose(-1, -2)\n",
        "\n",
        "        # Perform self-attention WITHOUT mask\n",
        "        # (Batch_Size, Height * Width, Features) -> (Batch_Size, Height * Width, Features)\n",
        "        x = self.attention(x)\n",
        "\n",
        "        # (Batch_Size, Height * Width, Features) -> (Batch_Size, Features, Height * Width)\n",
        "        x = x.transpose(-1, -2)\n",
        "\n",
        "        # (Batch_Size, Features, Height * Width) -> (Batch_Size, Features, Height, Width)\n",
        "        x = x.view((n, c, h, w))\n",
        "\n",
        "        # (Batch_Size, Features, Height, Width) + (Batch_Size, Features, Height, Width) -> (Batch_Size, Features, Height, Width)\n",
        "        x += residue\n",
        "\n",
        "        # (Batch_Size, Features, Height, Width)\n",
        "        return x"
      ],
      "metadata": {
        "id": "3-rF7pVez2RW"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "class VAE_ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.groupnorm_1 = nn.GroupNorm(32, in_channels)\n",
        "        self.conv_1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "\n",
        "        self.groupnorm_2 = nn.GroupNorm(32, out_channels)\n",
        "        self.conv_2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "\n",
        "        if in_channels == out_channels:\n",
        "            self.residual_layer = nn.Identity()\n",
        "        else:\n",
        "            self.residual_layer = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (Batch_Size, In_Channels, Height, Width)\n",
        "\n",
        "        residue = x\n",
        "\n",
        "        # (Batch_Size, In_Channels, Height, Width) -> (Batch_Size, In_Channels, Height, Width)\n",
        "        x = self.groupnorm_1(x)\n",
        "\n",
        "        # (Batch_Size, In_Channels, Height, Width) -> (Batch_Size, In_Channels, Height, Width)\n",
        "        x = F.silu(x)\n",
        "\n",
        "        # (Batch_Size, In_Channels, Height, Width) -> (Batch_Size, Out_Channels, Height, Width)\n",
        "        x = self.conv_1(x)\n",
        "\n",
        "        # (Batch_Size, Out_Channels, Height, Width) -> (Batch_Size, Out_Channels, Height, Width)\n",
        "        x = self.groupnorm_2(x)\n",
        "\n",
        "        # (Batch_Size, Out_Channels, Height, Width) -> (Batch_Size, Out_Channels, Height, Width)\n",
        "        x = F.silu(x)\n",
        "\n",
        "        # (Batch_Size, Out_Channels, Height, Width) -> (Batch_Size, Out_Channels, Height, Width)\n",
        "        x = self.conv_2(x)\n",
        "\n",
        "        # (Batch_Size, Out_Channels, Height, Width) -> (Batch_Size, Out_Channels, Height, Width)\n",
        "        return x + self.residual_layer(residue)\n"
      ],
      "metadata": {
        "id": "5umzel6oz4IE"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "# from decoder import VAE_AttentionBlock, VAE_ResidualBlock\n",
        "\n",
        "class VAE_Encoder(nn.Sequential):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            # (Batch_Size, Channel, Height, Width) -> (Batch_Size, 128, Height, Width)\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "\n",
        "             # (Batch_Size, 128, Height, Width) -> (Batch_Size, 128, Height, Width)\n",
        "            VAE_ResidualBlock(64, 64),\n",
        "\n",
        "            # (Batch_Size, 128, Height, Width) -> (Batch_Size, 128, Height, Width)\n",
        "            VAE_ResidualBlock(64, 64),\n",
        "\n",
        "            # (Batch_Size, 128, Height, Width) -> (Batch_Size, 128, Height / 2, Width / 2)\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=0),\n",
        "\n",
        "            # (Batch_Size, 128, Height / 2, Width / 2) -> (Batch_Size, 256, Height / 2, Width / 2)\n",
        "            VAE_ResidualBlock(64, 128),\n",
        "\n",
        "            # (Batch_Size, 256, Height / 2, Width / 2) -> (Batch_Size, 256, Height / 2, Width / 2)\n",
        "            VAE_ResidualBlock(128, 128),\n",
        "\n",
        "            # (Batch_Size, 256, Height / 2, Width / 2) -> (Batch_Size, 256, Height / 4, Width / 4)\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=0),\n",
        "\n",
        "            # (Batch_Size, 256, Height / 4, Width / 4) -> (Batch_Size, 512, Height / 4, Width / 4)\n",
        "            VAE_ResidualBlock(128, 256),\n",
        "\n",
        "            # (Batch_Size, 512, Height / 4, Width / 4) -> (Batch_Size, 512, Height / 4, Width / 4)\n",
        "            VAE_ResidualBlock(256, 256),\n",
        "\n",
        "            # (Batch_Size, 512, Height / 4, Width / 4) -> (Batch_Size, 512, Height / 8, Width / 8)\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=0),\n",
        "\n",
        "            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n",
        "            VAE_ResidualBlock(256, 256),\n",
        "\n",
        "            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n",
        "            VAE_ResidualBlock(256, 256),\n",
        "\n",
        "            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n",
        "#             VAE_ResidualBlock(512, 512),\n",
        "\n",
        "            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n",
        "            VAE_AttentionBlock(256),\n",
        "\n",
        "            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n",
        "            VAE_ResidualBlock(256, 256),\n",
        "\n",
        "            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n",
        "            nn.GroupNorm(32, 256),\n",
        "\n",
        "            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n",
        "            nn.SiLU(),\n",
        "\n",
        "            # Because the padding=1, it means the width and height will increase by 2\n",
        "            # Out_Height = In_Height + Padding_Top + Padding_Bottom\n",
        "            # Out_Width = In_Width + Padding_Left + Padding_Right\n",
        "            # Since padding = 1 means Padding_Top = Padding_Bottom = Padding_Left = Padding_Right = 1,\n",
        "            # Since the Out_Width = In_Width + 2 (same for Out_Height), it will compensate for the Kernel size of 3\n",
        "            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 8, Height / 8, Width / 8).\n",
        "            nn.Conv2d(256, 8, kernel_size=3, padding=1),\n",
        "\n",
        "            # (Batch_Size, 8, Height / 8, Width / 8) -> (Batch_Size, 8, Height / 8, Width / 8)\n",
        "            nn.Conv2d(8, 8, kernel_size=1, padding=0),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (Batch_Size, Channel, Height, Width)\n",
        "        # noise: (Batch_Size, 4, Height / 8, Width / 8)\n",
        "\n",
        "        for module in self:\n",
        "\n",
        "            if getattr(module, 'stride', None) == (2, 2):  # Padding at downsampling should be asymmetric (see #8)\n",
        "                # Pad: (Padding_Left, Padding_Right, Padding_Top, Padding_Bottom).\n",
        "                # Pad with zeros on the right and bottom.\n",
        "                # (Batch_Size, Channel, Height, Width) -> (Batch_Size, Channel, Height + Padding_Top + Padding_Bottom, Width + Padding_Left + Padding_Right) = (Batch_Size, Channel, Height + 1, Width + 1)\n",
        "                x = F.pad(x, (0, 1, 0, 1))\n",
        "\n",
        "            x = module(x)\n",
        "        # (Batch_Size, 8, Height / 8, Width / 8) -> two tensors of shape (Batch_Size, 4, Height / 8, Width / 8)\n",
        "        mean, log_variance = torch.chunk(x, 2, dim=1)\n",
        "        # Clamp the log variance between -30 and 20, so that the variance is between (circa) 1e-14 and 1e8.\n",
        "        # (Batch_Size, 4, Height / 8, Width / 8) -> (Batch_Size, 4, Height / 8, Width / 8)\n",
        "        log_variance = torch.clamp(log_variance, -30, 20)\n",
        "        # (Batch_Size, 4, Height / 8, Width / 8) -> (Batch_Size, 4, Height / 8, Width / 8)\n",
        "        variance = log_variance.exp()\n",
        "        # (Batch_Size, 4, Height / 8, Width / 8) -> (Batch_Size, 4, Height / 8, Width / 8)\n",
        "        stdev = variance.sqrt().to(DEVICE)\n",
        "        mean = mean.to(DEVICE)\n",
        "        # Transform N(0, 1) -> N(mean, stdev)\n",
        "        # (Batch_Size, 4, Height / 8, Width / 8) -> (Batch_Size, 4, Height / 8, Width / 8)\n",
        "        noise = torch.randn(mean.shape).to(DEVICE)\n",
        "        x = mean + stdev * noise\n",
        "\n",
        "        # Scale by a constant\n",
        "        # Constant taken from: https://github.com/CompVis/stable-diffusion/blob/21f890f9da3cfbeaba8e2ac3c425ee9e998d5229/configs/stable-diffusion/v1-inference.yaml#L17C1-L17C1\n",
        "        x *= 0.18215\n",
        "\n",
        "        return x, mean, stdev\n"
      ],
      "metadata": {
        "id": "tCF8BSqWz6Vv"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE_Decoder(nn.Sequential):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            # (Batch_Size, 4, Height / 8, Width / 8) -> (Batch_Size, 4, Height / 8, Width / 8)\n",
        "            nn.Conv2d(4, 4, kernel_size=1, padding=0),\n",
        "\n",
        "            # (Batch_Size, 4, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n",
        "            nn.Conv2d(4, 256, kernel_size=3, padding=1),\n",
        "\n",
        "            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n",
        "            VAE_ResidualBlock(256, 256),\n",
        "\n",
        "            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n",
        "            VAE_AttentionBlock(256),\n",
        "\n",
        "            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n",
        "#             VAE_ResidualBlock(512, 512),\n",
        "\n",
        "            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n",
        "            VAE_ResidualBlock(256, 256),\n",
        "\n",
        "            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n",
        "            VAE_ResidualBlock(256, 256),\n",
        "\n",
        "            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n",
        "#             VAE_ResidualBlock(512, 512),\n",
        "\n",
        "            # Repeats the rows and columns of the data by scale_factor (like when you resize an image by doubling its size).\n",
        "            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 4, Width / 4)\n",
        "            nn.Upsample(scale_factor=2),\n",
        "\n",
        "            # (Batch_Size, 512, Height / 4, Width / 4) -> (Batch_Size, 512, Height / 4, Width / 4)\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "\n",
        "            # (Batch_Size, 512, Height / 4, Width / 4) -> (Batch_Size, 512, Height / 4, Width / 4)\n",
        "            VAE_ResidualBlock(256, 256),\n",
        "\n",
        "            # (Batch_Size, 512, Height / 4, Width / 4) -> (Batch_Size, 512, Height / 4, Width / 4)\n",
        "            VAE_ResidualBlock(256, 256),\n",
        "\n",
        "            # (Batch_Size, 512, Height / 4, Width / 4) -> (Batch_Size, 512, Height / 4, Width / 4)\n",
        "#             VAE_ResidualBlock(512, 512),\n",
        "\n",
        "            # (Batch_Size, 512, Height / 4, Width / 4) -> (Batch_Size, 512, Height / 2, Width / 2)\n",
        "            nn.Upsample(scale_factor=2),\n",
        "\n",
        "            # (Batch_Size, 512, Height / 2, Width / 2) -> (Batch_Size, 512, Height / 2, Width / 2)\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "\n",
        "            # (Batch_Size, 512, Height / 2, Width / 2) -> (Batch_Size, 256, Height / 2, Width / 2)\n",
        "            VAE_ResidualBlock(256, 128),\n",
        "\n",
        "            # (Batch_Size, 256, Height / 2, Width / 2) -> (Batch_Size, 256, Height / 2, Width / 2)\n",
        "            VAE_ResidualBlock(128, 128),\n",
        "\n",
        "            # (Batch_Size, 256, Height / 2, Width / 2) -> (Batch_Size, 256, Height / 2, Width / 2)\n",
        "            VAE_ResidualBlock(128, 128),\n",
        "\n",
        "            # (Batch_Size, 256, Height / 2, Width / 2) -> (Batch_Size, 256, Height, Width)\n",
        "            nn.Upsample(scale_factor=2),\n",
        "\n",
        "            # (Batch_Size, 256, Height, Width) -> (Batch_Size, 256, Height, Width)\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "\n",
        "            # (Batch_Size, 256, Height, Width) -> (Batch_Size, 128, Height, Width)\n",
        "            VAE_ResidualBlock(128, 64),\n",
        "\n",
        "            # (Batch_Size, 128, Height, Width) -> (Batch_Size, 128, Height, Width)\n",
        "            VAE_ResidualBlock(64, 64),\n",
        "\n",
        "            # (Batch_Size, 128, Height, Width) -> (Batch_Size, 128, Height, Width)\n",
        "            VAE_ResidualBlock(64, 64),\n",
        "\n",
        "            # (Batch_Size, 128, Height, Width) -> (Batch_Size, 128, Height, Width)\n",
        "            nn.GroupNorm(32, 64),\n",
        "\n",
        "            # (Batch_Size, 128, Height, Width) -> (Batch_Size, 128, Height, Width)\n",
        "            nn.SiLU(),\n",
        "\n",
        "            # (Batch_Size, 128, Height, Width) -> (Batch_Size, 3, Height, Width)\n",
        "            nn.Conv2d(64, 3, kernel_size=3, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (Batch_Size, 4, Height / 8, Width / 8)\n",
        "\n",
        "        # Remove the scaling added by the Encoder.\n",
        "        x /= 0.18215\n",
        "\n",
        "        for module in self:\n",
        "            x = module(x)\n",
        "\n",
        "        # (Batch_Size, 3, Height, Width)\n",
        "        return x"
      ],
      "metadata": {
        "id": "x6CikRpRz8or"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "def get_image(img_path):\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize(128),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.0, 0.0, 0.0], [1.0, 1.0, 1.0])\n",
        "            ]\n",
        "        )\n",
        "        image = transform(image)\n",
        "        return image"
      ],
      "metadata": {
        "id": "1wQLHhFQ_3eY"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Checking the Z_sample by using an image"
      ],
      "metadata": {
        "id": "4kAFlFgJsT4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming VAE_Decoder is your class, first instantiate it\n",
        "encoder = VAE_Encoder()\n",
        "\n",
        "# Load the state dictionary from the file\n",
        "state_dict = torch.load('/content/vae_encoder_0.003.pth')\n",
        "# ,map_location=torch.device('cpu')\n",
        "\n",
        "# Now load the state dictionary into your model instance\n",
        "encoder.load_state_dict(state_dict)\n",
        "\n",
        "# Now your decoder model has the trained weights loaded and is ready to be used\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dy4j9Bk7z9V7",
        "outputId": "09fb2429-cfff-4579-ca18-a958f330cd21"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "Ri9Zn7zS2rsQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample a latent vector z from a standard normal distribution\n",
        "z_sample = get_image(\"/content/4.jpg\")\n",
        "# encoder(z_sample)\n",
        "# z_sample = torch.randn(1,3,128,128)  # batch_size can be 1 or more depending on your need\n",
        "# # Assuming 'decoder' is an instance of VAE_Decoder\n",
        "# x_recon,_,_ = encoder(z_sample)\n",
        "# print(x_recon)\n",
        "import torch\n",
        "\n",
        "# Assuming z_sample is a PyTorch tensor with shape (3, 128, 128)\n",
        "z_sample = z_sample.unsqueeze(0)  # Adds a new dimension at the 0th index\n",
        "\n",
        "# Now, z_sample has a shape of (1, 3, 128, 128)\n",
        "print(f\"New shape with batch size: {z_sample.shape}\")\n",
        "\n",
        "# print(z_sample.shape)\n",
        "x_recon,_,_ = encoder(z_sample)\n",
        "print(x_recon.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TYXrvY80Zmf",
        "outputId": "07ee0912-5f57-40c9-eba2-763bb771970e"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New shape with batch size: torch.Size([1, 3, 128, 128])\n",
            "torch.Size([1, 4, 16, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = VAE_Decoder()\n",
        "\n",
        "# Load the state dictionary from the file\n",
        "state_dict_1 = torch.load('/content/vae_decoder_0.003.pth')\n",
        "\n",
        "# Now load the state dictionary into your model instance\n",
        "decoder.load_state_dict(state_dict_1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ug4UlWzMJEiw",
        "outputId": "96d5654e-c458-47e5-d7db-8b278a868d05"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # np.count_nonzero(x_recon<0), np.count_nonzero(x_recon>1)\n",
        "# # Assuming x_recon is a PyTorch tensor on the GPU\n",
        "# x_recon_cpu = x_recon.cpu()  # Move tensor to CPU\n",
        "# x_recon_numpy = x_recon_cpu.detach().numpy()  # Convert to NumPy array\n",
        "\n",
        "# # Now you can perform your NumPy operations\n",
        "# count_negative = np.count_nonzero(x_recon_numpy < -1)\n",
        "# count_above_one = np.count_nonzero(x_recon_numpy > 1)\n",
        "\n",
        "# print(f\"Count of negative values: {count_negative}\")\n",
        "# print(f\"Count of values above one: {count_above_one}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WM5Qntpx2aIx",
        "outputId": "bef2bcdd-d37c-4539-fe2e-b4d5a648583e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count of negative values: 0\n",
            "Count of values above one: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Clip model for Embedding the Captions\n",
        "\n"
      ],
      "metadata": {
        "id": "9wco4QKP9ESB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from transformers import CLIPTokenizer"
      ],
      "metadata": {
        "id": "iVafnpsp9Fp8"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, n_heads, d_embed, in_proj_bias=True, out_proj_bias=True):\n",
        "        super().__init__()\n",
        "        # This combines the Wq, Wk and Wv matrices into one matrix\n",
        "        self.in_proj = nn.Linear(d_embed, 3 * d_embed, bias=in_proj_bias)\n",
        "        # This one represents the Wo matrix\n",
        "        self.out_proj = nn.Linear(d_embed, d_embed, bias=out_proj_bias)\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head = d_embed // n_heads\n",
        "\n",
        "    def forward(self, x, causal_mask=False):\n",
        "        # x: # (Batch_Size, Seq_Len, Dim)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim)\n",
        "        input_shape = x.shape\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim)\n",
        "        batch_size, sequence_length, d_embed = input_shape\n",
        "\n",
        "        # (Batch_Size, Seq_Len, H, Dim / H)\n",
        "        interim_shape = (batch_size, sequence_length, self.n_heads, self.d_head)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim * 3) -> 3 tensor of shape (Batch_Size, Seq_Len, Dim)\n",
        "        q, k, v = self.in_proj(x).chunk(3, dim=-1)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, H, Dim / H) -> (Batch_Size, H, Seq_Len, Dim / H)\n",
        "        q = q.view(interim_shape).transpose(1, 2)\n",
        "        k = k.view(interim_shape).transpose(1, 2)\n",
        "        v = v.view(interim_shape).transpose(1, 2)\n",
        "\n",
        "        # (Batch_Size, H, Seq_Len, Dim / H) @ (Batch_Size, H, Dim / H, Seq_Len) -> (Batch_Size, H, Seq_Len, Seq_Len)\n",
        "        weight = q @ k.transpose(-1, -2)\n",
        "\n",
        "        if causal_mask:\n",
        "            # Mask where the upper triangle (above the principal diagonal) is 1\n",
        "            mask = torch.ones_like(weight, dtype=torch.bool).triu(1)\n",
        "            # Fill the upper triangle with -inf\n",
        "            weight.masked_fill_(mask, -torch.inf)\n",
        "\n",
        "        # Divide by d_k (Dim / H).\n",
        "        # (Batch_Size, H, Seq_Len, Seq_Len) -> (Batch_Size, H, Seq_Len, Seq_Len)\n",
        "        weight /= math.sqrt(self.d_head)\n",
        "\n",
        "        # (Batch_Size, H, Seq_Len, Seq_Len) -> (Batch_Size, H, Seq_Len, Seq_Len)\n",
        "        weight = F.softmax(weight, dim=-1)\n",
        "\n",
        "        # (Batch_Size, H, Seq_Len, Seq_Len) @ (Batch_Size, H, Seq_Len, Dim / H) -> (Batch_Size, H, Seq_Len, Dim / H)\n",
        "        output = weight @ v\n",
        "\n",
        "        # (Batch_Size, H, Seq_Len, Dim / H) -> (Batch_Size, Seq_Len, H, Dim / H)\n",
        "        output = output.transpose(1, 2)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, H, Dim / H) -> (Batch_Size, Seq_Len, Dim)\n",
        "        output = output.reshape(input_shape)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n",
        "        output = self.out_proj(output)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "UmiGQrqP9HML"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class CLIPEmbedding(nn.Module):\n",
        "    def __init__(self, n_vocab: int, n_embd: int, n_token: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding = nn.Embedding(n_vocab, n_embd)\n",
        "        # A learnable weight matrix encodes the position information for each token\n",
        "        self.position_embedding = nn.Parameter(torch.zeros((n_token, n_embd)))\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        # (Batch_Size, Seq_Len) -> (Batch_Size, Seq_Len, Dim)\n",
        "        x = self.token_embedding(tokens)\n",
        "\n",
        "        # (Batch_Size, Seq_Len) -> (Batch_Size, Seq_Len, Dim)\n",
        "        x += self.position_embedding\n",
        "\n",
        "        return x\n",
        "\n",
        "class CLIPLayer(nn.Module):\n",
        "    def __init__(self, n_head: int, n_embd: int):\n",
        "        super().__init__()\n",
        "\n",
        "        # Pre-attention norm\n",
        "        self.layernorm_1 = nn.LayerNorm(n_embd)\n",
        "        # Self attention\n",
        "        self.attention = SelfAttention(n_head, n_embd)\n",
        "        # Pre-FNN norm\n",
        "        self.layernorm_2 = nn.LayerNorm(n_embd)\n",
        "        # Feedforward layer\n",
        "        self.linear_1 = nn.Linear(n_embd, 4 * n_embd)\n",
        "        self.linear_2 = nn.Linear(4 * n_embd, n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (Batch_Size, Seq_Len, Dim)\n",
        "        residue = x\n",
        "\n",
        "        ### SELF ATTENTION ###\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n",
        "        x = self.layernorm_1(x)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n",
        "        x = self.attention(x, causal_mask=True)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim) + (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n",
        "        x += residue\n",
        "\n",
        "        ### FEEDFORWARD LAYER ###\n",
        "        # Apply a feedforward layer where the hidden dimension is 4 times the embedding dimension.\n",
        "\n",
        "        residue = x\n",
        "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n",
        "        x = self.layernorm_2(x)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, 4 * Dim)\n",
        "        x = self.linear_1(x)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, 4 * Dim) -> (Batch_Size, Seq_Len, 4 * Dim)\n",
        "        x = x * torch.sigmoid(1.702 * x)   # QuickGELU activation function\n",
        "\n",
        "        # (Batch_Size, Seq_Len, 4 * Dim) -> (Batch_Size, Seq_Len, Dim)\n",
        "        x = self.linear_2(x)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim) + (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n",
        "        x += residue\n",
        "\n",
        "        return x\n",
        "\n",
        "class CLIP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = CLIPEmbedding(49408, 768, 77)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            CLIPLayer(12, 768) for i in range(12)\n",
        "        ])\n",
        "\n",
        "        self.layernorm = nn.LayerNorm(768)\n",
        "\n",
        "    def forward(self, tokens: torch.LongTensor) -> torch.FloatTensor:\n",
        "        tokens = tokens.type(torch.long)\n",
        "\n",
        "        # (Batch_Size, Seq_Len) -> (Batch_Size, Seq_Len, Dim)\n",
        "        state = self.embedding(tokens)\n",
        "\n",
        "        # Apply encoder layers similar to the Transformer's encoder.\n",
        "        for layer in self.layers:\n",
        "            # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n",
        "            state = layer(state)\n",
        "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n",
        "        output = self.layernorm(state)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "Lr3Jqnt_9I6k"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import CLIPTokenizer\n",
        "# from CLIP import CLIP  # Make sure you have a proper import for your CLIP model\n",
        "\n",
        "class TextEmbeddingProcessor(nn.Module):\n",
        "    def __init__(self, device):\n",
        "        super(TextEmbeddingProcessor, self).__init__()\n",
        "        self.device = device  # Store the device as an instance variable\n",
        "        self.clip = CLIP()  # Assuming CLIP() is a defined model compatible with these inputs\n",
        "        self.clip.to(self.device)  # Move the CLIP model to the specified device\n",
        "        self.tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "        self.text_emb_layer = nn.Linear(59136, 4 * 16 * 16).to(self.device)  # Move the linear layer to the device\n",
        "\n",
        "    def forward(self, prompt, uncond_prompt=\"\", do_cfg=True, cfg_scale=8):\n",
        "        if do_cfg:\n",
        "            # Conditional tokens\n",
        "            cond_tokens = self.tokenizer(prompt, padding=\"max_length\", truncation=True, max_length=77, return_tensors=\"pt\").input_ids.to(self.device)\n",
        "            cond_context = self.clip(cond_tokens)\n",
        "\n",
        "            # # Unconditional tokens\n",
        "            # uncond_tokens = self.tokenizer(uncond_prompt, padding=\"max_length\", truncation=True, max_length=77, return_tensors=\"pt\").input_ids.to(self.device)\n",
        "            # uncond_context = self.clip(uncond_tokens)\n",
        "\n",
        "            # Concatenate contexts\n",
        "            # context = torch.cat([cond_context, cond_context], dim=0)\n",
        "            context = cond_context\n",
        "        else:\n",
        "            # Handle case where CFG is not used\n",
        "            tokens = self.tokenizer(prompt, padding=\"max_length\", truncation=True, max_length=77, return_tensors=\"pt\").input_ids.to(self.device)\n",
        "            context = self.clip(tokens)\n",
        "\n",
        "        # Reshape and transform context\n",
        "        x, y, z = context.shape\n",
        "        text_emb_flat = context.view(x, y * z)  # Flatten context\n",
        "        text_emb_transformed = self.text_emb_layer(text_emb_flat)\n",
        "        text_emb_reshaped = text_emb_transformed.view(-1, 4, 16, 16)  # Reshape to target dimensions; ensure batch size handling with -1\n",
        "        text_emb_reshaped = torch.clamp(text_emb_reshaped, -1, 1)  # Clamp values to range [-1, 1]\n",
        "\n",
        "        return text_emb_reshaped\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqP9Rz1uECGc",
        "outputId": "39ee336a-db40-4bf2-a739-50548d036227"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 4, 16, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage example:\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "processor = TextEmbeddingProcessor(device=device)\n",
        "prompt = \"A teenage boy wearing a blue shirt.\"\n",
        "output = processor(prompt)\n",
        "print(output.shape)  # Should print (2, 4, 16, 16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMDooZKKMrmO",
        "outputId": "6a9658f3-b877-4f4f-c665-fc7e9e2952ef"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 4, 16, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###This is the class used to reshape the embeddings so that they fit the concatenation with the Z_sample which will then be denoised\n"
      ],
      "metadata": {
        "id": "YFuv7-vnsft7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReshapeEmbedding(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(59136, 4*16*16)\n",
        "\n",
        "    def forward(self, text_emb):\n",
        "        x, y, z = text_emb.shape\n",
        "        text_emb_flat = text_emb.view(x,y*z )  # Reshape to include a spatial dimension as if it's a grayscale image\n",
        "        text_emb_transformed = self.fc(text_emb_flat)\n",
        "        text_emb_reshaped = text_emb_transformed.view(2, 4, 16, 16)\n",
        "        text_emb_reshaped = torch.clamp(text_emb_reshaped, -1, 1)\n",
        "        return text_emb_reshaped\n",
        "\n",
        "# # Create the module and apply\n",
        "# reshape_module = ReshapeEmbedding()\n",
        "# text_emb = torch.randn(2, 77, 768)\n",
        "# text_emb_reshaped = reshape_module(text_emb)\n",
        "# text_emb_reshaped.shape, np.count_nonzero(text_emb_reshaped.cpu().detach().numpy()<-1), np.count_nonzero(text_emb_reshaped.cpu().detach().numpy()>1)"
      ],
      "metadata": {
        "id": "0_gfF8Oo7C-o"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_emb_reshaped.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChNPnBet-AA0",
        "outputId": "8be05514-bdb9-40ff-f174-adf5b6084acf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 4, 16, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YURXDQ9G-G2g",
        "outputId": "579a31ec-e33e-4650-ac33-9961a3a92faf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting diffusers\n",
            "  Downloading diffusers-0.27.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/2.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/2.0 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/2.0 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m1.7/2.0 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers) (7.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.2 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.20.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers) (1.25.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.4.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers) (9.4.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.2->diffusers) (2023.6.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.2->diffusers) (4.66.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.2->diffusers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.2->diffusers) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.2->diffusers) (24.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers) (3.18.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2024.2.2)\n",
            "Installing collected packages: diffusers\n",
            "Successfully installed diffusers-0.27.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###This is also a model which can be used for denoising that is -> UNET2Dmodel"
      ],
      "metadata": {
        "id": "kW3feDaDstNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from diffusers import DDPMScheduler, UNet2DModel\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "from torchvision import datasets, transforms\n",
        "import torch.utils.data as data_utils\n",
        "import json"
      ],
      "metadata": {
        "id": "fV2cZ5biHqrl"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.Resize([128,128]),\n",
        "                                 transforms.ToTensor()])\n",
        "train_dataset = datasets.ImageFolder('/kaggle/input/animals-class/raw-img/', transform=transform)\n",
        "\n",
        "# indices = torch.arange(100)\n",
        "# train_dataset = data_utils.Subset(train_dataset, indices)\n",
        "\n",
        "print(\"Length of dataset\", len(train_dataset))\n",
        "\n",
        "print(\"Dataset Loaded\")\n",
        "\n",
        "# with open(\"label_animal.json\", \"w\") as outfile:\n",
        "#     json.dump(train_dataset.class_to_idx, outfile)\n"
      ],
      "metadata": {
        "id": "vTCF_eJgHxOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ClassConditionedUnet(nn.Module):\n",
        "  def __init__(self,device = None, num_classes=10, class_emb_size=4):\n",
        "    super().__init__()\n",
        "\n",
        "    # The embedding layer will map the class label to a vector of size class_emb_size\n",
        "    self.class_emb = TextEmbeddingProcessor(device)\n",
        "\n",
        "\n",
        "\n",
        "    # Self.model is an unconditional UNet with extra input channels to accept the conditioning information (the class embedding)\n",
        "    self.model = UNet2DModel(\n",
        "        sample_size=256,           # the target image resolution\n",
        "        in_channels=4 + class_emb_size, # Additional input channels for class cond.\n",
        "        out_channels=4,           # the number of output channels\n",
        "        layers_per_block=2,       # how many ResNet layers to use per UNet block\n",
        "        block_out_channels=(32, 64, 64),\n",
        "        down_block_types=(\n",
        "            \"DownBlock2D\",        # a regular ResNet downsampling block\n",
        "            \"AttnDownBlock2D\",    # a ResNet downsampling block with spatial self-attention\n",
        "            \"AttnDownBlock2D\",\n",
        "        ),\n",
        "        up_block_types=(\n",
        "            \"AttnUpBlock2D\",\n",
        "            \"AttnUpBlock2D\",      # a ResNet upsampling block with spatial self-attention\n",
        "            \"UpBlock2D\",          # a regular ResNet upsampling block\n",
        "          ),\n",
        "    )\n",
        "\n",
        "  # Our forward method now takes the class labels as an additional argument\n",
        "  def forward(self, x, t, class_labels):\n",
        "    # Shape of x:\n",
        "    bs, ch, w, h = x.shape\n",
        "    print(x.shape)\n",
        "\n",
        "    # class conditioning in right shape to add as additional input channels\n",
        "    class_cond = self.class_emb(class_labels) # Map to embedding dimension\n",
        "    print(class_cond.size())\n",
        "    # class_cond = class_cond.view(bs, class_cond.shape[1], 1, 1).expand(bs, class_cond.shape[1], w, h)\n",
        "    # x is shape (bs, 1, 28, 28) and class_cond is now (bs, 4, 28, 28)\n",
        "\n",
        "    # Net input is now x and class cond concatenated together along dimension 1\n",
        "    net_input = torch.cat((x, class_cond), 1)# (bs, 5, 28, 28)\n",
        "    print(net_input.size())\n",
        "\n",
        "    # Feed this to the UNet alongside the timestep and return the prediction\n",
        "    return self.model(net_input, t).sample # (bs, 1, 28, 28)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoA-oZ1tBY5h",
        "outputId": "cd930bf6-298c-4f97-a45c-37eaf82467c4"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a scheduler\n",
        "noise_scheduler = DDPMScheduler(num_train_timesteps=10, beta_schedule='squaredcos_cap_v2')\n",
        "\n",
        "# Training loop (10 Epochs):\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Epochs\n",
        "n_epochs = 3\n",
        "\n",
        "# Our network\n",
        "net = ClassConditionedUnet().to(device)\n",
        "\n",
        "# Our loss function\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# The optimizer\n",
        "opt = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
        "\n",
        "# Keeping a record of the losses for later viewing\n",
        "losses = []\n",
        "\n",
        "# The training loop\n",
        "for epoch in range(n_epochs):\n",
        "    for x, y in tqdm(train_dataloader):\n",
        "\n",
        "        # Get some data and prepare the corrupted version\n",
        "        x = x.to(device) * 2 - 1 # Data on the GPU (mapped to (-1, 1))\n",
        "        y = y.to(device)\n",
        "        noise = torch.randn_like(x)\n",
        "        timesteps = torch.randint(0, 9, (x.shape[0],)).long().to(device)\n",
        "        noisy_x = noise_scheduler.add_noise(x, noise, timesteps)\n",
        "\n",
        "        # Get the model prediction\n",
        "        pred = net(noisy_x, timesteps, y) # Note that we pass in the labels y\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = loss_fn(pred, noise) # How close is the output to the noise\n",
        "\n",
        "        # Backprop and update the params:\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        # Store the loss for later\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    # Print out the average of the last 100 loss values to get an idea of progress:\n",
        "    avg_loss = sum(losses[-100:])/100\n",
        "    print(f'Finished epoch {epoch}. Average of the last 100 loss values: {avg_loss:05f}')\n",
        "\n",
        "\n",
        "torch.save(net.state_dict(), \"model_animal.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "IYsq0k2pHdkF",
        "outputId": "d26dabef-b25c-42d1-b71f-a2e8c36f94a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-7054068d88fe>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Training loop (10 Epochs):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The MDLite model as described in the Mobile Diffusion Paper -> [Mobile Diffusion\n",
        "](https://research.google/blog/mobilediffusion-rapid-text-to-image-generation-on-device/)"
      ],
      "metadata": {
        "id": "hyB4uRdxs3g1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "\n",
        "\n",
        "class UnetSelfAttention(nn.Module):\n",
        "    def __init__(self, n_heads, d_embed, in_proj_bias=True, out_proj_bias=True):\n",
        "        super().__init__()\n",
        "        # This combines the Wq, Wk and Wv matrices into one matrix\n",
        "        self.in_proj = nn.Linear(d_embed, 2 * d_embed, bias=in_proj_bias)\n",
        "        # This one represents the Wo matrix\n",
        "        self.out_proj = nn.Linear(d_embed, d_embed, bias=out_proj_bias)\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head = d_embed // n_heads\n",
        "\n",
        "    def forward(self, x, causal_mask=False):\n",
        "        # x: # (Batch_Size, Seq_Len, Dim)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim)\n",
        "        input_shape = x.shape\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim)\n",
        "        batch_size, sequence_length, d_embed = input_shape\n",
        "\n",
        "        # (Batch_Size, Seq_Len, H, Dim / H)\n",
        "        interim_shape = (batch_size, sequence_length, self.n_heads, self.d_head)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim * 3) -> 3 tensor of shape (Batch_Size, Seq_Len, Dim)\n",
        "        q, k = self.in_proj(x).chunk(2, dim=-1)\n",
        "        v = k\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, H, Dim / H) -> (Batch_Size, H, Seq_Len, Dim / H)\n",
        "        q = q.view(interim_shape).transpose(1, 2)\n",
        "        k = k.view(interim_shape).transpose(1, 2)\n",
        "        v = v.view(interim_shape).transpose(1, 2)\n",
        "\n",
        "        # (Batch_Size, H, Seq_Len, Dim / H) @ (Batch_Size, H, Dim / H, Seq_Len) -> (Batch_Size, H, Seq_Len, Seq_Len)\n",
        "        weight = q @ k.transpose(-1, -2)\n",
        "\n",
        "        if causal_mask:\n",
        "            # Mask where the upper triangle (above the principal diagonal) is 1\n",
        "            mask = torch.ones_like(weight, dtype=torch.bool).triu(1)\n",
        "            # Fill the upper triangle with -inf\n",
        "            weight.masked_fill_(mask, -torch.inf)\n",
        "\n",
        "        # Divide by d_k (Dim / H).\n",
        "        # (Batch_Size, H, Seq_Len, Seq_Len) -> (Batch_Size, H, Seq_Len, Seq_Len)\n",
        "        weight /= math.sqrt(self.d_head)\n",
        "\n",
        "        # (Batch_Size, H, Seq_Len, Seq_Len) -> (Batch_Size, H, Seq_Len, Seq_Len)\n",
        "        relu = nn.ReLU()\n",
        "        weight = relu(weight)\n",
        "\n",
        "        # (Batch_Size, H, Seq_Len, Seq_Len) @ (Batch_Size, H, Seq_Len, Dim / H) -> (Batch_Size, H, Seq_Len, Dim / H)\n",
        "        output = weight @ v\n",
        "\n",
        "        # (Batch_Size, H, Seq_Len, Dim / H) -> (Batch_Size, Seq_Len, H, Dim / H)\n",
        "        output = output.transpose(1, 2)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, H, Dim / H) -> (Batch_Size, Seq_Len, Dim)\n",
        "        output = output.reshape(input_shape)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n",
        "        output = self.out_proj(output)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim)\n",
        "        return output\n",
        "\n",
        "class UnetCrossAttention(nn.Module):\n",
        "    def __init__(self, n_heads, d_embed, d_cross, in_proj_bias=True, out_proj_bias=True):\n",
        "        super().__init__()\n",
        "        self.q_proj   = nn.Linear(d_embed, d_embed, bias=in_proj_bias)\n",
        "        self.k_proj   = nn.Linear(d_cross, d_embed, bias=in_proj_bias)\n",
        "        self.v_proj   = nn.Linear(d_cross, d_embed, bias=in_proj_bias)\n",
        "        self.out_proj = nn.Linear(d_embed, d_embed, bias=out_proj_bias)\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head = d_embed // n_heads\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # x (latent): # (Batch_Size, Seq_Len_Q, Dim_Q)\n",
        "        # y (context): # (Batch_Size, Seq_Len_KV, Dim_KV) = (Batch_Size, 77, 768)\n",
        "\n",
        "        input_shape = x.shape\n",
        "        batch_size, sequence_length, d_embed = input_shape\n",
        "        # Divide each embedding of Q into multiple heads such that d_heads * n_heads = Dim_Q\n",
        "        interim_shape = (batch_size, -1, self.n_heads, self.d_head)\n",
        "\n",
        "        # (Batch_Size, Seq_Len_Q, Dim_Q) -> (Batch_Size, Seq_Len_Q, Dim_Q)\n",
        "        print(\"x in cross attention\",x.shape)\n",
        "        q = self.q_proj(x)\n",
        "        # (Batch_Size, Seq_Len_KV, Dim_KV) -> (Batch_Size, Seq_Len_KV, Dim_Q)\n",
        "        k = self.k_proj(y)\n",
        "        # (Batch_Size, Seq_Len_KV, Dim_KV) -> (Batch_Size, Seq_Len_KV, Dim_Q)\n",
        "        v = self.v_proj(y)\n",
        "        # v = k\n",
        "        print(\"printing q,k,v\",q.shape, k.shape, v.shape)\n",
        "\n",
        "        # (Batch_Size, Seq_Len_Q, Dim_Q) -> (Batch_Size, Seq_Len_Q, H, Dim_Q / H) -> (Batch_Size, H, Seq_Len_Q, Dim_Q / H)\n",
        "        q = q.view(interim_shape).transpose(1, 2)\n",
        "        # (Batch_Size, Seq_Len_KV, Dim_Q) -> (Batch_Size, Seq_Len_KV, H, Dim_Q / H) -> (Batch_Size, H, Seq_Len_KV, Dim_Q / H)\n",
        "        k = k.view(interim_shape).transpose(1, 2)\n",
        "        # (Batch_Size, Seq_Len_KV, Dim_Q) -> (Batch_Size, Seq_Len_KV, H, Dim_Q / H) -> (Batch_Size, H, Seq_Len_KV, Dim_Q / H)\n",
        "        v = v.view(interim_shape).transpose(1, 2)\n",
        "\n",
        "        # (Batch_Size, H, Seq_Len_Q, Dim_Q / H) @ (Batch_Size, H, Dim_Q / H, Seq_Len_KV) -> (Batch_Size, H, Seq_Len_Q, Seq_Len_KV)\n",
        "        weight = q @ k.transpose(-1, -2)\n",
        "\n",
        "        # (Batch_Size, H, Seq_Len_Q, Seq_Len_KV)\n",
        "        weight /= math.sqrt(self.d_head)\n",
        "\n",
        "        # (Batch_Size, H, Seq_Len_Q, Seq_Len_KV)\n",
        "        relu = nn.ReLU()\n",
        "        weight = relu(weight)\n",
        "\n",
        "        # (Batch_Size, H, Seq_Len_Q, Seq_Len_KV) @ (Batch_Size, H, Seq_Len_KV, Dim_Q / H) -> (Batch_Size, H, Seq_Len_Q, Dim_Q / H)\n",
        "        output = weight @ v\n",
        "\n",
        "        # (Batch_Size, H, Seq_Len_Q, Dim_Q / H) -> (Batch_Size, Seq_Len_Q, H, Dim_Q / H)\n",
        "        output = output.transpose(1, 2).contiguous()\n",
        "\n",
        "        # (Batch_Size, Seq_Len_Q, H, Dim_Q / H) -> (Batch_Size, Seq_Len_Q, Dim_Q)\n",
        "        output = output.view(input_shape)\n",
        "\n",
        "        # (Batch_Size, Seq_Len_Q, Dim_Q) -> (Batch_Size, Seq_Len_Q, Dim_Q)\n",
        "        output = self.out_proj(output)\n",
        "\n",
        "        # (Batch_Size, Seq_Len_Q, Dim_Q)\n",
        "        return output\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "# from attention import SelfAttention, CrossAttention\n",
        "\n",
        "class TimeEmbedding(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(n_embd, 4 * n_embd)\n",
        "        self.linear_2 = nn.Linear(4 * n_embd, 4 * n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (1, 320)\n",
        "\n",
        "        # (1, 320) -> (1, 1280)\n",
        "        x = self.linear_1(x)\n",
        "\n",
        "        # (1, 1280) -> (1, 1280)\n",
        "        x = F.silu(x)\n",
        "\n",
        "        # (1, 1280) -> (1, 1280)\n",
        "        x = self.linear_2(x)\n",
        "\n",
        "        return x\n",
        "class SeparableConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, padding=1):\n",
        "        super().__init__()\n",
        "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, padding=padding, groups=in_channels)\n",
        "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (Batch_Size, In_Channels, Height, Width)\n",
        "\n",
        "        # Depthwise convolution\n",
        "        x = self.depthwise(x)\n",
        "\n",
        "        # Pointwise convolution (1x1 kernel)\n",
        "        x = self.pointwise(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class UNET_ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, n_time=1280):\n",
        "        super().__init__()\n",
        "        self.groupnorm_feature = nn.GroupNorm(32, in_channels)\n",
        "        self.conv_feature = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.linear_time = nn.Linear(n_time, out_channels)\n",
        "\n",
        "        self.groupnorm_merged = nn.GroupNorm(32, out_channels)\n",
        "        self.conv_merged = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "\n",
        "        if in_channels == out_channels:\n",
        "            self.residual_layer = nn.Identity()\n",
        "        else:\n",
        "            self.residual_layer = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0)\n",
        "\n",
        "    def forward(self, feature, time):\n",
        "        # feature: (Batch_Size, In_Channels, Height, Width)\n",
        "        # time: (1, 1280)\n",
        "\n",
        "        residue = feature\n",
        "\n",
        "        # (Batch_Size, In_Channels, Height, Width) -> (Batch_Size, In_Channels, Height, Width)\n",
        "        feature = self.groupnorm_feature(feature)\n",
        "\n",
        "        # (Batch_Size, In_Channels, Height, Width) -> (Batch_Size, In_Channels, Height, Width)\n",
        "        feature = F.silu(feature)\n",
        "\n",
        "        # (Batch_Size, In_Channels, Height, Width) -> (Batch_Size, Out_Channels, Height, Width)\n",
        "        feature = self.conv_feature(feature)\n",
        "\n",
        "        # (1, 1280) -> (1, 1280)\n",
        "        time = F.silu(time)\n",
        "\n",
        "        # (1, 1280) -> (1, Out_Channels)\n",
        "        time = self.linear_time(time)\n",
        "\n",
        "        # Add width and height dimension to time.\n",
        "        # (Batch_Size, Out_Channels, Height, Width) + (1, Out_Channels, 1, 1) -> (Batch_Size, Out_Channels, Height, Width)\n",
        "        merged = feature + time.unsqueeze(-1).unsqueeze(-1)\n",
        "\n",
        "        # (Batch_Size, Out_Channels, Height, Width) -> (Batch_Size, Out_Channels, Height, Width)\n",
        "        merged = self.groupnorm_merged(merged)\n",
        "\n",
        "        # (Batch_Size, Out_Channels, Height, Width) -> (Batch_Size, Out_Channels, Height, Width)\n",
        "        merged = F.silu(merged)\n",
        "\n",
        "        # (Batch_Size, Out_Channels, Height, Width) -> (Batch_Size, Out_Channels, Height, Width)\n",
        "        merged = self.conv_merged(merged)\n",
        "\n",
        "        # (Batch_Size, Out_Channels, Height, Width) + (Batch_Size, Out_Channels, Height, Width) -> (Batch_Size, Out_Channels, Height, Width)\n",
        "        return merged + self.residual_layer(residue)\n",
        "\n",
        "class UNET_Separable_ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, n_time=1280):\n",
        "        super().__init__()\n",
        "        self.groupnorm_feature = nn.GroupNorm(32, in_channels)\n",
        "        self.conv_feature = SeparableConv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.linear_time = nn.Linear(n_time, out_channels)\n",
        "\n",
        "        self.groupnorm_merged = nn.GroupNorm(32, out_channels)\n",
        "        self.conv_merged = SeparableConv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "\n",
        "        if in_channels == out_channels:\n",
        "            self.residual_layer = nn.Identity()\n",
        "        else:\n",
        "            self.residual_layer = SeparableConv2d(in_channels, out_channels, kernel_size=1, padding=0)\n",
        "\n",
        "    def forward(self, feature, time):\n",
        "        # feature: (Batch_Size, In_Channels, Height, Width)\n",
        "        # time: (1, 1280)\n",
        "\n",
        "        residue = feature\n",
        "\n",
        "        # (Batch_Size, In_Channels, Height, Width) -> (Batch_Size, In_Channels, Height, Width)\n",
        "        feature = self.groupnorm_feature(feature)\n",
        "\n",
        "        # (Batch_Size, In_Channels, Height, Width) -> (Batch_Size, In_Channels, Height, Width)\n",
        "        feature = F.silu(feature)\n",
        "\n",
        "        # (Batch_Size, In_Channels, Height, Width) -> (Batch_Size, Out_Channels, Height, Width)\n",
        "        feature = self.conv_feature(feature)\n",
        "        print(\"d\")\n",
        "\n",
        "        # (1, 1280) -> (1, 1280)\n",
        "        time = F.silu(time)\n",
        "        # assert isinstance(timesteps, torch.Tensor)\n",
        "\n",
        "\n",
        "        # (1, 1280) -> (1, Out_Channels)\n",
        "        time = self.linear_time(time)\n",
        "\n",
        "        # Add width and height dimension to time.\n",
        "        # (Batch_Size, Out_Channels, Height, Width) + (1, Out_Channels, 1, 1) -> (Batch_Size, Out_Channels, Height, Width)\n",
        "        merged = feature + time.unsqueeze(-1).unsqueeze(-1)\n",
        "\n",
        "        # (Batch_Size, Out_Channels, Height, Width) -> (Batch_Size, Out_Channels, Height, Width)\n",
        "        merged = self.groupnorm_merged(merged)\n",
        "\n",
        "        # (Batch_Size, Out_Channels, Height, Width) -> (Batch_Size, Out_Channels, Height, Width)\n",
        "        merged = F.silu(merged)\n",
        "\n",
        "        # (Batch_Size, Out_Channels, Height, Width) -> (Batch_Size, Out_Channels, Height, Width)\n",
        "        merged = self.conv_merged(merged)\n",
        "\n",
        "        # (Batch_Size, Out_Channels, Height, Width) + (Batch_Size, Out_Channels, Height, Width) -> (Batch_Size, Out_Channels, Height, Width)\n",
        "        return merged + self.residual_layer(residue)\n",
        "\n",
        "\n",
        "class UNET_AttentionBlock(nn.Module):\n",
        "    def __init__(self, n_head: int, n_embd: int, d_context=768):\n",
        "        super().__init__()\n",
        "        channels = n_head * n_embd\n",
        "\n",
        "        self.groupnorm = nn.GroupNorm(32, channels, eps=1e-6)\n",
        "        self.conv_input = nn.Conv2d(channels, channels, kernel_size=1, padding=0)\n",
        "\n",
        "        self.layernorm_1 = nn.LayerNorm(channels)\n",
        "        self.attention_1 = UnetSelfAttention(n_head, channels, in_proj_bias=False)\n",
        "        self.layernorm_2 = nn.LayerNorm(channels)\n",
        "        self.attention_2 = UnetCrossAttention(n_head, channels, d_context, in_proj_bias=False)\n",
        "        self.layernorm_3 = nn.LayerNorm(channels)\n",
        "        self.linear_geglu_1  = nn.Linear(channels, 4 * channels * 2)\n",
        "        self.linear_geglu_2 = nn.Linear(4 * channels, channels)\n",
        "\n",
        "        self.conv_output = nn.Conv2d(channels, channels, kernel_size=1, padding=0)\n",
        "\n",
        "    def forward(self, x, context):\n",
        "        # x: (Batch_Size, Features, Height, Width)\n",
        "        # context: (Batch_Size, Seq_Len, Dim)\n",
        "\n",
        "        residue_long = x\n",
        "\n",
        "        # (Batch_Size, Features, Height, Width) -> (Batch_Size, Features, Height, Width)\n",
        "        x = self.groupnorm(x)\n",
        "\n",
        "        # (Batch_Size, Features, Height, Width) -> (Batch_Size, Features, Height, Width)\n",
        "        x = self.conv_input(x)\n",
        "\n",
        "        n, c, h, w = x.shape\n",
        "\n",
        "        # (Batch_Size, Features, Height, Width) -> (Batch_Size, Features, Height * Width)\n",
        "        x = x.view((n, c, h * w))\n",
        "\n",
        "        # (Batch_Size, Features, Height * Width) -> (Batch_Size, Height * Width, Features)\n",
        "        x = x.transpose(-1, -2)\n",
        "\n",
        "        # Normalization + Self-Attention with skip connection\n",
        "\n",
        "        # (Batch_Size, Height * Width, Features)\n",
        "        residue_short = x\n",
        "\n",
        "        # (Batch_Size, Height * Width, Features) -> (Batch_Size, Height * Width, Features)\n",
        "        x = self.layernorm_1(x)\n",
        "\n",
        "        # (Batch_Size, Height * Width, Features) -> (Batch_Size, Height * Width, Features)\n",
        "        x = self.attention_1(x)\n",
        "\n",
        "        # (Batch_Size, Height * Width, Features) + (Batch_Size, Height * Width, Features) -> (Batch_Size, Height * Width, Features)\n",
        "        x += residue_short\n",
        "\n",
        "        # (Batch_Size, Height * Width, Features)\n",
        "        residue_short = x\n",
        "\n",
        "        # Normalization + Cross-Attention with skip connection\n",
        "\n",
        "        # (Batch_Size, Height * Width, Features) -> (Batch_Size, Height * Width, Features)\n",
        "        x = self.layernorm_2(x)\n",
        "\n",
        "        # (Batch_Size, Height * Width, Features) -> (Batch_Size, Height * Width, Features)\n",
        "        x = self.attention_2(x, context)\n",
        "\n",
        "        # (Batch_Size, Height * Width, Features) + (Batch_Size, Height * Width, Features) -> (Batch_Size, Height * Width, Features)\n",
        "        x += residue_short\n",
        "\n",
        "        # (Batch_Size, Height * Width, Features)\n",
        "        residue_short = x\n",
        "\n",
        "        # Normalization + FFN with GeGLU and skip connection\n",
        "\n",
        "        # (Batch_Size, Height * Width, Features) -> (Batch_Size, Height * Width, Features)\n",
        "        x = self.layernorm_3(x)\n",
        "\n",
        "        # GeGLU as implemented in the original code: https://github.com/CompVis/stable-diffusion/blob/21f890f9da3cfbeaba8e2ac3c425ee9e998d5229/ldm/modules/attention.py#L37C10-L37C10\n",
        "        # (Batch_Size, Height * Width, Features) -> two tensors of shape (Batch_Size, Height * Width, Features * 4)\n",
        "        x, gate = self.linear_geglu_1(x).chunk(2, dim=-1)\n",
        "\n",
        "        # Element-wise product: (Batch_Size, Height * Width, Features * 4) * (Batch_Size, Height * Width, Features * 4) -> (Batch_Size, Height * Width, Features * 4)\n",
        "        x = x * F.gelu(gate)\n",
        "\n",
        "        # (Batch_Size, Height * Width, Features * 4) -> (Batch_Size, Height * Width, Features)\n",
        "        x = self.linear_geglu_2(x)\n",
        "\n",
        "        # (Batch_Size, Height * Width, Features) + (Batch_Size, Height * Width, Features) -> (Batch_Size, Height * Width, Features)\n",
        "        x += residue_short\n",
        "\n",
        "        # (Batch_Size, Height * Width, Features) -> (Batch_Size, Features, Height * Width)\n",
        "        x = x.transpose(-1, -2)\n",
        "\n",
        "        # (Batch_Size, Features, Height * Width) -> (Batch_Size, Features, Height, Width)\n",
        "        x = x.view((n, c, h, w))\n",
        "\n",
        "        # Final skip connection between initial input and output of the block\n",
        "        # (Batch_Size, Features, Height, Width) + (Batch_Size, Features, Height, Width) -> (Batch_Size, Features, Height, Width)\n",
        "        return self.conv_output(x) + residue_long\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (Batch_Size, Features, Height, Width) -> (Batch_Size, Features, Height * 2, Width * 2)\n",
        "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
        "        return self.conv(x)\n",
        "\n",
        "class SwitchSequential(nn.Sequential):\n",
        "    def forward(self, x, context, time):\n",
        "        for layer in self:\n",
        "            if isinstance(layer, UNET_AttentionBlock):\n",
        "                x = layer(x, context)\n",
        "            # elif isinstance(layer, UNET_Cross_AttentionBlock):\n",
        "                # x = layer(x, context)\n",
        "            elif isinstance(layer, UNET_ResidualBlock):\n",
        "                x = layer(x, time)\n",
        "            elif isinstance(layer, UNET_Separable_ResidualBlock):\n",
        "                x = layer(x, time)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "        return x\n",
        "\n",
        "# class UNET(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         self.encoders = nn.ModuleList([\n",
        "#             # (Batch_Size, 4, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8)\n",
        "#             SwitchSequential(nn.Conv2d(4, 320, kernel_size=3, padding=1)),\n",
        "\n",
        "#             # (Batch_Size, 320, Height / 8, Width / 8) -> # (Batch_Size, 320, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8)\n",
        "#             SwitchSequential(UNET_ResidualBlock(320, 320), UNET_Cross_AttentionBlock(8, 40)),\n",
        "\n",
        "#             # (Batch_Size, 320, Height / 8, Width / 8) -> # (Batch_Size, 320, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8)\n",
        "#             SwitchSequential(UNET_ResidualBlock(320, 320), UNET_AttentionBlock(8, 40)),\n",
        "\n",
        "#             # (Batch_Size, 320, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 16, Width / 16)\n",
        "#             SwitchSequential(nn.Conv2d(320, 320, kernel_size=3, stride=2, padding=1)),\n",
        "\n",
        "#             # (Batch_Size, 320, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 16, Width / 16)\n",
        "#             SwitchSequential(UNET_ResidualBlock(320, 640), UNET_AttentionBlock(8, 80)),\n",
        "\n",
        "#             # (Batch_Size, 640, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 16, Width / 16)\n",
        "#             SwitchSequential(UNET_ResidualBlock(640, 640), UNET_AttentionBlock(8, 80)),\n",
        "\n",
        "#             # (Batch_Size, 640, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 32, Width / 32)\n",
        "#             SwitchSequential(nn.Conv2d(640, 640, kernel_size=3, stride=2, padding=1)),\n",
        "\n",
        "#             # (Batch_Size, 640, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 32, Width / 32)\n",
        "#             SwitchSequential(UNET_ResidualBlock(640, 1280), UNET_AttentionBlock(8, 160)),\n",
        "\n",
        "#             # (Batch_Size, 1280, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 32, Width / 32)\n",
        "#             SwitchSequential(UNET_ResidualBlock(1280, 1280), UNET_AttentionBlock(8, 160)),\n",
        "\n",
        "#             # (Batch_Size, 1280, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 64, Width / 64)\n",
        "#             SwitchSequential(nn.Conv2d(1280, 1280, kernel_size=3, stride=2, padding=1)),\n",
        "\n",
        "#             # (Batch_Size, 1280, Height / 64, Width / 64) -> (Batch_Size, 1280, Height / 64, Width / 64)\n",
        "#             SwitchSequential(UNET_ResidualBlock(1280, 1280)),\n",
        "\n",
        "#             # (Batch_Size, 1280, Height / 64, Width / 64) -> (Batch_Size, 1280, Height / 64, Width / 64)\n",
        "#             SwitchSequential(UNET_ResidualBlock(1280, 1280)),\n",
        "#         ])\n",
        "\n",
        "#         self.bottleneck = SwitchSequential(\n",
        "#             # (Batch_Size, 1280, Height / 64, Width / 64) -> (Batch_Size, 1280, Height / 64, Width / 64)\n",
        "#             UNET_ResidualBlock(1280, 1280),\n",
        "\n",
        "#             # (Batch_Size, 1280, Height / 64, Width / 64) -> (Batch_Size, 1280, Height / 64, Width / 64)\n",
        "#             UNET_AttentionBlock(8, 160),\n",
        "\n",
        "#             # (Batch_Size, 1280, Height / 64, Width / 64) -> (Batch_Size, 1280, Height / 64, Width / 64)\n",
        "#             UNET_ResidualBlock(1280, 1280),\n",
        "#         )\n",
        "\n",
        "#         self.decoders = nn.ModuleList([\n",
        "#             # (Batch_Size, 2560, Height / 64, Width / 64) -> (Batch_Size, 1280, Height / 64, Width / 64)\n",
        "#             SwitchSequential(UNET_ResidualBlock(2560, 1280)),\n",
        "\n",
        "#             # (Batch_Size, 2560, Height / 64, Width / 64) -> (Batch_Size, 1280, Height / 64, Width / 64)\n",
        "#             SwitchSequential(UNET_ResidualBlock(2560, 1280)),\n",
        "\n",
        "#             # (Batch_Size, 2560, Height / 64, Width / 64) -> (Batch_Size, 1280, Height / 64, Width / 64) -> (Batch_Size, 1280, Height / 32, Width / 32)\n",
        "#             SwitchSequential(UNET_ResidualBlock(2560, 1280), Upsample(1280)),\n",
        "\n",
        "#             # (Batch_Size, 2560, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 32, Width / 32)\n",
        "#             SwitchSequential(UNET_ResidualBlock(2560, 1280), UNET_AttentionBlock(8, 160)),\n",
        "\n",
        "#             # (Batch_Size, 2560, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 32, Width / 32)\n",
        "#             SwitchSequential(UNET_ResidualBlock(2560, 1280), UNET_AttentionBlock(8, 160)),\n",
        "\n",
        "#             # (Batch_Size, 1920, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 16, Width / 16)\n",
        "#             SwitchSequential(UNET_ResidualBlock(1920, 1280), UNET_AttentionBlock(8, 160), Upsample(1280)),\n",
        "\n",
        "#             # (Batch_Size, 1920, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 16, Width / 16)\n",
        "#             SwitchSequential(UNET_ResidualBlock(1920, 640), UNET_AttentionBlock(8, 80)),\n",
        "\n",
        "#             # (Batch_Size, 1280, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 16, Width / 16)\n",
        "#             SwitchSequential(UNET_ResidualBlock(1280, 640), UNET_AttentionBlock(8, 80)),\n",
        "\n",
        "#             # (Batch_Size, 960, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 8, Width / 8)\n",
        "#             SwitchSequential(UNET_ResidualBlock(960, 640), UNET_AttentionBlock(8, 80), Upsample(640)),\n",
        "\n",
        "#             # (Batch_Size, 960, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8)\n",
        "#             SwitchSequential(UNET_ResidualBlock(960, 320), UNET_AttentionBlock(8, 40)),\n",
        "\n",
        "#             # (Batch_Size, 640, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8)\n",
        "#             SwitchSequential(UNET_ResidualBlock(640, 320), UNET_AttentionBlock(8, 40)),\n",
        "\n",
        "#             # (Batch_Size, 640, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8)\n",
        "#             SwitchSequential(UNET_ResidualBlock(640, 320), UNET_AttentionBlock(8, 40)),\n",
        "#         ])\n",
        "\n",
        "#     def forward(self, x, context, time):\n",
        "#         # x: (Batch_Size, 4, Height / 8, Width / 8)\n",
        "#         # context: (Batch_Size, Seq_Len, Dim)\n",
        "#         # time: (1, 1280)\n",
        "\n",
        "#         skip_connections = []\n",
        "#         for layers in self.encoders:\n",
        "#             x = layers(x, context, time)\n",
        "#             skip_connections.append(x)\n",
        "\n",
        "#         x = self.bottleneck(x, context, time)\n",
        "\n",
        "#         for layers in self.decoders:\n",
        "#             # Since we always concat with the skip connection of the encoder, the number of features increases before being sent to the decoder's layer\n",
        "#             x = torch.cat((x, skip_connections.pop()), dim=1)\n",
        "#             x = layers(x, context, time)\n",
        "\n",
        "#         return x\n",
        "\n",
        "class UNET_OutputLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.groupnorm = nn.GroupNorm(32, in_channels)\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (Batch_Size, 320, Height / 8, Width / 8)\n",
        "\n",
        "        # (Batch_Size, 320, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8)\n",
        "        x = self.groupnorm(x)\n",
        "\n",
        "        # (Batch_Size, 320, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8)\n",
        "        x = F.silu(x)\n",
        "\n",
        "        # (Batch_Size, 320, Height / 8, Width / 8) -> (Batch_Size, 4, Height / 8, Width / 8)\n",
        "        x = self.conv(x)\n",
        "\n",
        "        # (Batch_Size, 4, Height / 8, Width / 8)\n",
        "        return x\n",
        "\n",
        "class Diffusion(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.time_embedding = TimeEmbedding(320)\n",
        "        self.unet = UNET()\n",
        "        self.final = UNET_OutputLayer(320, 4)\n",
        "\n",
        "    def forward(self, latent, context, time):\n",
        "        # latent: (Batch_Size, 4, Height / 8, Width / 8)\n",
        "        # context: (Batch_Size, Seq_Len, Dim)\n",
        "        # time: (1, 320)\n",
        "\n",
        "        # (1, 320) -> (1, 1280)\n",
        "        time = self.time_embedding(time)\n",
        "\n",
        "        # (Batch, 4, Height / 8, Width / 8) -> (Batch, 320, Height / 8, Width / 8)\n",
        "        output = self.unet(latent, context, time)\n",
        "\n",
        "        # (Batch, 320, Height / 8, Width / 8) -> (Batch, 4, Height / 8, Width / 8)\n",
        "        output = self.final(output)\n",
        "\n",
        "        # (Batch, 4, Height / 8, Width / 8)\n",
        "        return output\n",
        "class UNET(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoders = nn.ModuleList([\n",
        "\n",
        "            # CONV h -> h\n",
        "            SwitchSequential(nn.Conv2d(4, 320, kernel_size = 3, padding = 1)),\n",
        "\n",
        "            # DOWN 1 - Full Conv * 1 h -> h/2\n",
        "            SwitchSequential(UNET_ResidualBlock(320, 320)),\n",
        "            SwitchSequential(UNET_ResidualBlock(320, 320)),\n",
        "            SwitchSequential(nn.Conv2d(320, 320, 3, 2, 1)),\n",
        "\n",
        "            # # DOWN 2 - Full Conv * 1 + CA h/2 -> h/4\n",
        "            # SwitchSequential(UNET_ResidualBlock(320, 640), UNET_Cross_AttentionBlock(8, 80)), ### To do: should go deep and understand what this block does\n",
        "            # SwitchSequential(UNET_ResidualBlock(320, 640), UNET_Cross_AttentionBlock(8, 80)),\n",
        "            SwitchSequential(nn.Conv2d(320, 320, 3, 2, 1))\n",
        "        ])\n",
        "\n",
        "        self.bottleneck = nn.ModuleList([\n",
        "            # DOWN 3 - SP Conv * 2 & (SA * 3 + CA * 3) * 2 (h/4 -> h/8)\n",
        "            SwitchSequential(UNET_Separable_ResidualBlock(320, 640), UNET_AttentionBlock(8, 80)),\n",
        "            SwitchSequential(UNET_Separable_ResidualBlock(640, 640), UNET_AttentionBlock(8, 80)),\n",
        "            SwitchSequential(nn.Conv2d(640, 640, 3, 2, 1)),\n",
        "\n",
        "            # UP 3 - SP Conv * 3 & (SA * 2 + CA * 2) * 3 (h/8 -> h/4)\n",
        "            SwitchSequential(UNET_Separable_ResidualBlock(640, 640), UNET_AttentionBlock(8, 80)),\n",
        "            SwitchSequential(UNET_Separable_ResidualBlock(640, 640), UNET_AttentionBlock(8, 80)),\n",
        "            SwitchSequential(UNET_Separable_ResidualBlock(640, 640), UNET_AttentionBlock(8, 80), Upsample(640))\n",
        "        ])\n",
        "\n",
        "        self.decoders = nn.ModuleList([\n",
        "            # UP 2 - Full Conv * 2 + CA * 2 (h/4 -> h/2)\n",
        "            # SwitchSequential(UNET_ResidualBlock(960, 640), UNET_Cross_AttentionBlock(16, 80)),\n",
        "            # SwitchSequential(UNET_ResidualBlock(640, 640), UNET_Cross_AttentionBlock(16, 80)),\n",
        "            # SwitchSequential(UNET_ResidualBlock(640, 640), UNET_Cross_AttentionBlock(16, 80), Upsample(640)),\n",
        "\n",
        "            # UP 1 - Full Conv * 3\n",
        "            SwitchSequential(UNET_ResidualBlock(640, 320)),\n",
        "            SwitchSequential(UNET_ResidualBlock(320, 320)),\n",
        "            SwitchSequential(UNET_ResidualBlock(320, 320), Upsample(320)),\n",
        "            SwitchSequential(UNET_ResidualBlock(320, 320)),\n",
        "            SwitchSequential(UNET_ResidualBlock(320, 320))\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, context, time):\n",
        "      # x : batch_size, 4, h, h\n",
        "      # context : batch_size, seq_len, dim\n",
        "      # time : 1, 1280\n",
        "\n",
        "      skip_connections = []\n",
        "      #print(\"in encoder\")\n",
        "      for c,layers in enumerate(self.encoders):\n",
        "        x = layers(x, context, time)\n",
        "        #print(c, x.size())\n",
        "        skip_connections.append(x)\n",
        "\n",
        "      # print(\"skip connections....\")\n",
        "      # print(\"length: \", len(skip_connections))\n",
        "      # print(\"shape: \", skip_connections[0].size())\n",
        "\n",
        "      #print(\"===== bottleneck========\")\n",
        "      # x = self.bottleneck(x, context, time)\n",
        "      for c, layers in enumerate(self.bottleneck):\n",
        "        x = layers(x, context, time)\n",
        "        #print(c, x.size())\n",
        "        # skip_connections.append(x)\n",
        "      #print(\"done\")\n",
        "\n",
        "      #print(\"===== decoders========\")\n",
        "      for c, layers in enumerate(self.decoders):\n",
        "        #x = torch.cat((x, skip_connections.pop()), dim = 1) ### To do: Adding previous x again for residual connections\n",
        "        x = layers(x, context, time)\n",
        "        #print(c, x.size())\n",
        "\n",
        "      return x\n"
      ],
      "metadata": {
        "id": "EPJz5Ohj8XJi"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x = torch.randn(1, 4, 16, 16)  # Batch size 2, 4 input channels, 32x32 resolution\n",
        "# context = torch.randn(1, 10, 768)  # Batch size 2, sequence length 10, context feature dimension 64\n",
        "# time = torch.randn(1, 1280)  # Time embedding, shape (1, 1280)\n",
        "model = UNET().to(device)\n",
        "# print(\"d\")\n",
        "# output = model(x,context,time)\n",
        "\n",
        "z_sample = torch.randn(1,4,16,16).to(device)\n",
        "timesteps = torch.randint(0, 9, (z_sample.shape[0],)).long().to(device).to_padded_tensor\n",
        "prompt = \"This lady has no eyeglasses. This woman is in the thirties and has a smile, and no bangs.\"\n",
        "output = model(z_sample,timesteps,prompt)\n",
        "print(output)\n",
        "\n",
        "# print(output.shape)"
      ],
      "metadata": {
        "id": "wbw_-42ytLBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z_sample = torch.randn(1,4,16,16).to(device)\n",
        "timesteps = torch.randint(0, 9, (z_sample.shape[0],)).long().to(device)\n",
        "model = ClassConditionedUnet().to(device)\n",
        "prompt = \"This lady has no eyeglasses. This woman is in the thirties and has a smile, and no bangs.\"\n",
        "output = model(z_sample,timesteps,prompt)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "8btXeamftMQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qgK5B6YcQcIv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}